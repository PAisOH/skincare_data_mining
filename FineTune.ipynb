{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb56fe3-c992-4f84-88ca-42b9fdec58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e7d81-eeec-4b53-a3a4-f5cd32d32bee",
   "metadata": {},
   "source": [
    "# 0. 基本常用Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff40c38-844f-4a7d-8859-dc146e2d86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available and use it if possible\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d73bc5-5525-4a90-83cd-35a0bb4f0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "test_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be47bb0-611b-45e9-b8d7-8d58157e7780",
   "metadata": {},
   "source": [
    "# 1. LoRA(CLS任务) - 基本BERT模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b9243-25e1-45bb-ad0a-c83301358ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5bf5c2-cee5-4586-9270-37dbafa29483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2cab0d7584625a3d8694cf6bad93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe78b0eaf1946bbab688b49a6b2002a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bda4fbce7940c3a4d1343ac9db2ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cc25d5-a15b-495b-87df-a92fae24deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建较小数据集进行微调\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f2470f-5208-424d-8679-6a547fa79c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# task_Type: 指定模型将进行微调的任务类型\n",
    "# r: 表示 A 和 B 的尺寸\n",
    "# lora_alpha: 比例因子，确定“A”和“B”中的权重相对于模型原始参数的相对显著性\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8b37ce-2442-4133-a898-17bad02e7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始大模型的可训练参数数量: 108311810\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# 检查是否可以使用 MPS 设备\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased', \n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# 计算可训练参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'原始大模型的可训练参数数量: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f28c7ae-5ae4-4e39-961f-46edb9a530b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "引入LoRA后大模型的可训练参数数量为: 38402\n"
     ]
    }
   ],
   "source": [
    "# 将A、B矩阵插入大模型\n",
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "\n",
    "# 计算可训练参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'引入LoRA后大模型的可训练参数数量为: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e453ef89-585e-432a-b75b-a2a6bf0536c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/euan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Fri Apr 12 10:17:38 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# TrainingArguments可用于自定义的综合超参数，以及用于激活不同训练配置的切换选项\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_trainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/training_args.py:1340\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1340\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1357\u001b[0m ):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/training_args.py:1672\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1672\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   1673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1674\u001b[0m         )\n\u001b[1;32m   1675\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "# mac用不了Accelerate(至少我没找到解决方法)\n",
    "# 频繁报错ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
    "\n",
    "# import numpy as np\n",
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# # 将预测转换成对数\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # TrainingArguments可用于自定义的综合超参数，以及用于激活不同训练配置的切换选项\n",
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
    "#                                  num_train_epochs=25,)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbe03fd-6713-441e-954b-cca2d6d505d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.06batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 11/25:  96%|██████████████████████ | 120/125 [01:54<00:04,  1.07batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 11/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.07batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 11/25:  97%|██████████████████████▎| 121/125 [01:55<00:03,  1.07batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 11/25:  97%|███████████████████████▏| 121/125 [01:56<00:03,  1.07batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 11/25:  98%|███████████████████████▍| 122/125 [01:56<00:02,  1.06batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 11/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.06batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 11/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.07batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 11/25:  98%|███████████████████████▌| 123/125 [01:58<00:01,  1.07batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 11/25:  99%|███████████████████████▊| 124/125 [01:58<00:00,  1.07batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 11/25:  99%|██████████████████████▊| 124/125 [01:59<00:00,  1.07batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 11/25: 100%|███████████████████████| 125/125 [01:59<00:00,  1.05batch/s, Loss=0.604]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 11/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   1%|▏                         | 1/125 [00:00<00:43,  2.83batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   2%|▍                         | 2/125 [00:00<00:43,  2.85batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   2%|▌                         | 3/125 [00:01<00:42,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   3%|▊                         | 4/125 [00:01<00:42,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   4%|█                         | 5/125 [00:01<00:41,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   5%|█▏                        | 6/125 [00:02<00:41,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   6%|█▍                        | 7/125 [00:02<00:41,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   6%|█▋                        | 8/125 [00:02<00:40,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   7%|█▊                        | 9/125 [00:03<00:40,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   8%|██                       | 10/125 [00:03<00:39,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:   9%|██▏                      | 11/125 [00:03<00:39,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  10%|██▍                      | 12/125 [00:04<00:39,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  10%|██▌                      | 13/125 [00:04<00:38,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  11%|██▊                      | 14/125 [00:04<00:38,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  12%|███                      | 15/125 [00:05<00:38,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  13%|███▏                     | 16/125 [00:05<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  14%|███▍                     | 17/125 [00:05<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  14%|███▌                     | 18/125 [00:06<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  15%|███▊                     | 19/125 [00:06<00:36,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  16%|████                     | 20/125 [00:06<00:36,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  17%|████▏                    | 21/125 [00:07<00:36,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  18%|████▍                    | 22/125 [00:07<00:35,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  18%|████▌                    | 23/125 [00:07<00:35,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  20%|█████                    | 25/125 [00:08<00:34,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  21%|█████▏                   | 26/125 [00:09<00:34,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  22%|█████▌                   | 28/125 [00:09<00:33,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  23%|█████▊                   | 29/125 [00:10<00:33,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  24%|██████                   | 30/125 [00:10<00:33,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  25%|██████▏                  | 31/125 [00:10<00:32,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  26%|██████▍                  | 32/125 [00:11<00:32,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  26%|██████▌                  | 33/125 [00:11<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  27%|██████▊                  | 34/125 [00:11<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  28%|███████                  | 35/125 [00:12<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  29%|███████▏                 | 36/125 [00:12<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  30%|███████▍                 | 37/125 [00:12<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  31%|███████▊                 | 39/125 [00:13<00:29,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  32%|████████                 | 40/125 [00:13<00:29,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  34%|████████▍                | 42/125 [00:14<00:28,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  34%|████████▌                | 43/125 [00:14<00:28,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  36%|█████████                | 45/125 [00:15<00:27,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  37%|█████████▏               | 46/125 [00:15<00:27,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  38%|█████████▌               | 48/125 [00:16<00:26,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  39%|█████████▊               | 49/125 [00:17<00:26,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  40%|██████████               | 50/125 [00:17<00:26,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  41%|██████████▏              | 51/125 [00:17<00:25,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  43%|██████████▊              | 54/125 [00:18<00:24,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  44%|███████████              | 55/125 [00:19<00:24,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  46%|███████████▍             | 57/125 [00:19<00:23,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  47%|███████████▊             | 59/125 [00:20<00:23,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  48%|████████████             | 60/125 [00:20<00:22,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  50%|████████████▍            | 62/125 [00:21<00:21,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  50%|████████████▌            | 63/125 [00:21<00:21,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  52%|█████████████            | 65/125 [00:22<00:20,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  53%|█████████████▏           | 66/125 [00:22<00:20,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  54%|█████████████▌           | 68/125 [00:23<00:19,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  57%|██████████████▏          | 71/125 [00:24<00:18,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  59%|██████████████▊          | 74/125 [00:25<00:17,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  61%|███████████████▏         | 76/125 [00:26<00:17,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  62%|███████████████▍         | 77/125 [00:26<00:16,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  63%|███████████████▊         | 79/125 [00:27<00:15,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  64%|████████████████         | 80/125 [00:27<00:15,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  66%|████████████████▍        | 82/125 [00:28<00:14,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  66%|████████████████▌        | 83/125 [00:28<00:14,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  68%|█████████████████        | 85/125 [00:29<00:13,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  69%|█████████████████▏       | 86/125 [00:29<00:13,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  70%|█████████████████▌       | 88/125 [00:30<00:12,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  71%|█████████████████▊       | 89/125 [00:30<00:12,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  72%|██████████████████       | 90/125 [00:31<00:12,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  73%|██████████████████▏      | 91/125 [00:31<00:11,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  74%|██████████████████▌      | 93/125 [00:32<00:11,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  75%|██████████████████▊      | 94/125 [00:32<00:10,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  77%|███████████████████▏     | 96/125 [00:33<00:10,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  78%|███████████████████▍     | 97/125 [00:33<00:09,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  79%|███████████████████▊     | 99/125 [00:34<00:09,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  80%|███████████████████▏    | 100/125 [00:34<00:08,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  82%|███████████████████▌    | 102/125 [00:35<00:07,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  82%|███████████████████▊    | 103/125 [00:35<00:07,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  83%|███████████████████▉    | 104/125 [00:36<00:07,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  84%|████████████████████▏   | 105/125 [00:36<00:06,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  85%|████████████████████▎   | 106/125 [00:36<00:06,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  86%|████████████████████▌   | 107/125 [00:37<00:06,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  86%|████████████████████▋   | 108/125 [00:37<00:05,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  87%|████████████████████▉   | 109/125 [00:37<00:05,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  88%|█████████████████████   | 110/125 [00:38<00:05,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  89%|█████████████████████▎  | 111/125 [00:38<00:04,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  90%|█████████████████████▌  | 112/125 [00:38<00:04,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  90%|█████████████████████▋  | 113/125 [00:39<00:04,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  91%|█████████████████████▉  | 114/125 [00:39<00:03,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  92%|██████████████████████  | 115/125 [00:39<00:03,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  93%|██████████████████████▎ | 116/125 [00:40<00:03,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  94%|██████████████████████▍ | 117/125 [00:40<00:02,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  94%|██████████████████████▋ | 118/125 [00:41<00:02,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  95%|██████████████████████▊ | 119/125 [00:41<00:02,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  96%|███████████████████████ | 120/125 [00:41<00:01,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  97%|███████████████████████▏| 121/125 [00:42<00:01,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  98%|███████████████████████▍| 122/125 [00:42<00:01,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  98%|███████████████████████▌| 123/125 [00:42<00:00,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25:  99%|███████████████████████▊| 124/125 [00:43<00:00,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 11/25: 100%|████████████████████████| 125/125 [00:43<00:00,  2.88batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Eval Accuracy: 0.6920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 12/25:   0%|                                 | 0/125 [00:00<?, ?batch/s, Loss=0.707]\u001b[A\n",
      "Epoch 12/25:   1%|▏                        | 1/125 [00:00<01:52,  1.10batch/s, Loss=0.707]\u001b[A\n",
      "Epoch 12/25:   1%|▏                        | 1/125 [00:01<01:52,  1.10batch/s, Loss=0.754]\u001b[A\n",
      "Epoch 12/25:   2%|▍                        | 2/125 [00:01<01:50,  1.11batch/s, Loss=0.754]\u001b[A\n",
      "Epoch 12/25:   2%|▍                        | 2/125 [00:02<01:50,  1.11batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:   2%|▌                        | 3/125 [00:02<01:49,  1.11batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:   2%|▌                         | 3/125 [00:03<01:49,  1.11batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 12/25:   3%|▊                         | 4/125 [00:03<01:48,  1.11batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 12/25:   3%|▊                        | 4/125 [00:04<01:48,  1.11batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 12/25:   4%|█                        | 5/125 [00:04<01:47,  1.11batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 12/25:   4%|█                        | 5/125 [00:05<01:47,  1.11batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:   5%|█▏                       | 6/125 [00:05<01:47,  1.11batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:   5%|█▏                       | 6/125 [00:06<01:47,  1.11batch/s, Loss=0.759]\u001b[A\n",
      "Epoch 12/25:   6%|█▍                       | 7/125 [00:06<01:45,  1.11batch/s, Loss=0.759]\u001b[A\n",
      "Epoch 12/25:   6%|█▍                       | 7/125 [00:07<01:45,  1.11batch/s, Loss=0.665]\u001b[A\n",
      "Epoch 12/25:   6%|█▌                       | 8/125 [00:07<01:44,  1.11batch/s, Loss=0.665]\u001b[A\n",
      "Epoch 12/25:   6%|█▋                        | 8/125 [00:08<01:44,  1.11batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 12/25:   7%|█▊                        | 9/125 [00:08<01:43,  1.12batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 12/25:   7%|█▊                       | 9/125 [00:08<01:43,  1.12batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 12/25:   8%|█▉                      | 10/125 [00:08<01:43,  1.12batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 12/25:   8%|█▉                      | 10/125 [00:09<01:43,  1.12batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 12/25:   9%|██                      | 11/125 [00:09<01:42,  1.12batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 12/25:   9%|██                      | 11/125 [00:10<01:42,  1.12batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 12/25:  10%|██▎                     | 12/125 [00:10<01:41,  1.12batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 12/25:  10%|██▎                     | 12/125 [00:11<01:41,  1.12batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 12/25:  10%|██▍                     | 13/125 [00:11<01:40,  1.12batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 12/25:  10%|██▍                     | 13/125 [00:12<01:40,  1.12batch/s, Loss=0.768]\u001b[A\n",
      "Epoch 12/25:  11%|██▋                     | 14/125 [00:12<01:39,  1.12batch/s, Loss=0.768]\u001b[A\n",
      "Epoch 12/25:  11%|██▊                      | 14/125 [00:13<01:39,  1.12batch/s, Loss=0.71]\u001b[A\n",
      "Epoch 12/25:  12%|███                      | 15/125 [00:13<01:38,  1.12batch/s, Loss=0.71]\u001b[A\n",
      "Epoch 12/25:  12%|██▉                     | 15/125 [00:14<01:38,  1.12batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 12/25:  13%|███                     | 16/125 [00:14<01:37,  1.12batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 12/25:  13%|███                     | 16/125 [00:15<01:37,  1.12batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 12/25:  14%|███▎                    | 17/125 [00:15<01:36,  1.12batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 12/25:  14%|███▎                    | 17/125 [00:16<01:36,  1.12batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 12/25:  14%|███▍                    | 18/125 [00:16<01:35,  1.12batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 12/25:  14%|███▍                    | 18/125 [00:17<01:35,  1.12batch/s, Loss=0.712]\u001b[A\n",
      "Epoch 12/25:  15%|███▋                    | 19/125 [00:17<01:34,  1.12batch/s, Loss=0.712]\u001b[A\n",
      "Epoch 12/25:  15%|███▋                    | 19/125 [00:17<01:34,  1.12batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 12/25:  16%|███▊                    | 20/125 [00:17<01:34,  1.12batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 12/25:  16%|████                     | 20/125 [00:18<01:34,  1.12batch/s, Loss=0.49]\u001b[A\n",
      "Epoch 12/25:  17%|████▏                    | 21/125 [00:18<01:33,  1.12batch/s, Loss=0.49]\u001b[A\n",
      "Epoch 12/25:  17%|████                    | 21/125 [00:19<01:33,  1.12batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 12/25:  18%|████▏                   | 22/125 [00:19<01:32,  1.12batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 12/25:  18%|████▏                   | 22/125 [00:20<01:32,  1.12batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 12/25:  18%|████▍                   | 23/125 [00:20<01:31,  1.12batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 12/25:  18%|████▍                   | 23/125 [00:21<01:31,  1.12batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 12/25:  19%|████▌                   | 24/125 [00:21<01:30,  1.12batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 12/25:  19%|████▌                   | 24/125 [00:22<01:30,  1.12batch/s, Loss=0.819]\u001b[A\n",
      "Epoch 12/25:  20%|████▊                   | 25/125 [00:22<01:29,  1.12batch/s, Loss=0.819]\u001b[A\n",
      "Epoch 12/25:  20%|████▊                   | 25/125 [00:23<01:29,  1.12batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 12/25:  21%|████▉                   | 26/125 [00:23<01:28,  1.12batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 12/25:  21%|████▉                   | 26/125 [00:24<01:28,  1.12batch/s, Loss=0.656]\u001b[A\n",
      "Epoch 12/25:  22%|█████▏                  | 27/125 [00:24<01:27,  1.12batch/s, Loss=0.656]\u001b[A\n",
      "Epoch 12/25:  22%|█████▏                  | 27/125 [00:25<01:27,  1.12batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 12/25:  22%|█████▍                  | 28/125 [00:25<01:26,  1.12batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 12/25:  22%|█████▍                  | 28/125 [00:26<01:26,  1.12batch/s, Loss=0.504]\u001b[A\n",
      "Epoch 12/25:  23%|█████▌                  | 29/125 [00:26<01:25,  1.12batch/s, Loss=0.504]\u001b[A\n",
      "Epoch 12/25:  23%|█████▌                  | 29/125 [00:26<01:25,  1.12batch/s, Loss=0.683]\u001b[A\n",
      "Epoch 12/25:  24%|█████▊                  | 30/125 [00:26<01:25,  1.12batch/s, Loss=0.683]\u001b[A\n",
      "Epoch 12/25:  24%|█████▊                  | 30/125 [00:27<01:25,  1.12batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 12/25:  25%|█████▉                  | 31/125 [00:27<01:24,  1.12batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 12/25:  25%|█████▉                  | 31/125 [00:28<01:24,  1.12batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 12/25:  26%|██████▏                 | 32/125 [00:28<01:23,  1.12batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 12/25:  26%|██████▏                 | 32/125 [00:29<01:23,  1.12batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 12/25:  26%|██████▎                 | 33/125 [00:29<01:22,  1.12batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 12/25:  26%|██████▎                 | 33/125 [00:30<01:22,  1.12batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:  27%|██████▌                 | 34/125 [00:30<01:21,  1.11batch/s, Loss=0.638]\u001b[A\n",
      "Epoch 12/25:  27%|██████▌                 | 34/125 [00:31<01:21,  1.11batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 12/25:  28%|██████▋                 | 35/125 [00:31<01:20,  1.11batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 12/25:  28%|██████▋                 | 35/125 [00:32<01:20,  1.11batch/s, Loss=0.909]\u001b[A\n",
      "Epoch 12/25:  29%|██████▉                 | 36/125 [00:32<01:19,  1.11batch/s, Loss=0.909]\u001b[A\n",
      "Epoch 12/25:  29%|██████▉                 | 36/125 [00:33<01:19,  1.11batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 12/25:  30%|███████                 | 37/125 [00:33<01:18,  1.11batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 12/25:  30%|███████                 | 37/125 [00:34<01:18,  1.11batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 12/25:  30%|███████▎                | 38/125 [00:34<01:18,  1.12batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 12/25:  30%|███████▎                | 38/125 [00:34<01:18,  1.12batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 12/25:  31%|███████▍                | 39/125 [00:34<01:17,  1.11batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 12/25:  31%|███████▍                | 39/125 [00:35<01:17,  1.11batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 12/25:  32%|███████▋                | 40/125 [00:35<01:16,  1.11batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 12/25:  32%|███████▋                | 40/125 [00:36<01:16,  1.11batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 12/25:  33%|███████▊                | 41/125 [00:36<01:15,  1.12batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 12/25:  33%|███████▊                | 41/125 [00:37<01:15,  1.12batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 12/25:  34%|████████                | 42/125 [00:37<01:14,  1.12batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 12/25:  34%|████████                | 42/125 [00:38<01:14,  1.12batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 12/25:  34%|████████▎               | 43/125 [00:38<01:13,  1.11batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 12/25:  34%|████████▌                | 43/125 [00:39<01:13,  1.11batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 12/25:  35%|████████▊                | 44/125 [00:39<01:12,  1.11batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 12/25:  35%|████████▍               | 44/125 [00:40<01:12,  1.11batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 12/25:  36%|████████▋               | 45/125 [00:40<01:12,  1.10batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 12/25:  36%|████████▋               | 45/125 [00:41<01:12,  1.10batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 12/25:  37%|████████▊               | 46/125 [00:41<01:11,  1.10batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 12/25:  37%|████████▊               | 46/125 [00:42<01:11,  1.10batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 12/25:  38%|█████████               | 47/125 [00:42<01:10,  1.10batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 12/25:  38%|█████████               | 47/125 [00:43<01:10,  1.10batch/s, Loss=0.725]\u001b[A\n",
      "Epoch 12/25:  38%|█████████▏              | 48/125 [00:43<01:09,  1.11batch/s, Loss=0.725]\u001b[A\n",
      "Epoch 12/25:  38%|█████████▏              | 48/125 [00:43<01:09,  1.11batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 12/25:  39%|█████████▍              | 49/125 [00:44<01:08,  1.11batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 12/25:  39%|█████████▍              | 49/125 [00:44<01:08,  1.11batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 12/25:  40%|█████████▌              | 50/125 [00:44<01:07,  1.11batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 12/25:  40%|█████████▌              | 50/125 [00:45<01:07,  1.11batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 12/25:  41%|█████████▊              | 51/125 [00:45<01:06,  1.11batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 12/25:  41%|█████████▊              | 51/125 [00:46<01:06,  1.11batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 12/25:  42%|█████████▉              | 52/125 [00:46<01:05,  1.11batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 12/25:  42%|█████████▉              | 52/125 [00:47<01:05,  1.11batch/s, Loss=0.745]\u001b[A\n",
      "Epoch 12/25:  42%|██████████▏             | 53/125 [00:47<01:04,  1.11batch/s, Loss=0.745]\u001b[A\n",
      "Epoch 12/25:  42%|██████████▌              | 53/125 [00:48<01:04,  1.11batch/s, Loss=0.65]\u001b[A\n",
      "Epoch 12/25:  43%|██████████▊              | 54/125 [00:48<01:03,  1.11batch/s, Loss=0.65]\u001b[A\n",
      "Epoch 12/25:  43%|██████████▎             | 54/125 [00:49<01:03,  1.11batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 12/25:  44%|██████████▌             | 55/125 [00:49<01:02,  1.11batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 12/25:  44%|██████████▌             | 55/125 [00:50<01:02,  1.11batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 12/25:  45%|██████████▊             | 56/125 [00:50<01:01,  1.11batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 12/25:  45%|██████████▊             | 56/125 [00:51<01:01,  1.11batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 12/25:  46%|██████████▉             | 57/125 [00:51<01:00,  1.12batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 12/25:  46%|██████████▉             | 57/125 [00:52<01:00,  1.12batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 12/25:  46%|███████████▏            | 58/125 [00:52<01:00,  1.12batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 12/25:  46%|███████████▏            | 58/125 [00:52<01:00,  1.12batch/s, Loss=0.928]\u001b[A\n",
      "Epoch 12/25:  47%|███████████▎            | 59/125 [00:52<00:59,  1.12batch/s, Loss=0.928]\u001b[A\n",
      "Epoch 12/25:  47%|███████████▎            | 59/125 [00:53<00:59,  1.12batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 12/25:  48%|███████████▌            | 60/125 [00:53<00:58,  1.12batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 12/25:  48%|███████████▌            | 60/125 [00:54<00:58,  1.12batch/s, Loss=0.652]\u001b[A\n",
      "Epoch 12/25:  49%|███████████▋            | 61/125 [00:54<00:57,  1.11batch/s, Loss=0.652]\u001b[A\n",
      "Epoch 12/25:  49%|███████████▋            | 61/125 [00:55<00:57,  1.11batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 12/25:  50%|███████████▉            | 62/125 [00:55<00:56,  1.11batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 12/25:  50%|███████████▉            | 62/125 [00:56<00:56,  1.11batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 12/25:  50%|████████████            | 63/125 [00:56<00:55,  1.11batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 12/25:  50%|████████████            | 63/125 [00:57<00:55,  1.11batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 12/25:  51%|████████████▎           | 64/125 [00:57<00:54,  1.12batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 12/25:  51%|████████████▎           | 64/125 [00:58<00:54,  1.12batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 12/25:  52%|████████████▍           | 65/125 [00:58<00:53,  1.12batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 12/25:  52%|████████████▍           | 65/125 [00:59<00:53,  1.12batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 12/25:  53%|████████████▋           | 66/125 [00:59<00:52,  1.12batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 12/25:  53%|████████████▋           | 66/125 [01:00<00:52,  1.12batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 12/25:  54%|████████████▊           | 67/125 [01:00<00:51,  1.12batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 12/25:  54%|█████████████▍           | 67/125 [01:01<00:51,  1.12batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 12/25:  54%|█████████████▌           | 68/125 [01:01<00:51,  1.12batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 12/25:  54%|█████████████           | 68/125 [01:01<00:51,  1.12batch/s, Loss=0.611]\u001b[A\n",
      "Epoch 12/25:  55%|█████████████▏          | 69/125 [01:01<00:50,  1.12batch/s, Loss=0.611]\u001b[A\n",
      "Epoch 12/25:  55%|█████████████▏          | 69/125 [01:02<00:50,  1.12batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 12/25:  56%|█████████████▍          | 70/125 [01:02<00:49,  1.12batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 12/25:  56%|█████████████▍          | 70/125 [01:03<00:49,  1.12batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 12/25:  57%|█████████████▋          | 71/125 [01:03<00:48,  1.12batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 12/25:  57%|█████████████▋          | 71/125 [01:04<00:48,  1.12batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 12/25:  58%|█████████████▊          | 72/125 [01:04<00:47,  1.12batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 12/25:  58%|█████████████▊          | 72/125 [01:05<00:47,  1.12batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 12/25:  58%|██████████████          | 73/125 [01:05<00:46,  1.12batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 12/25:  58%|██████████████          | 73/125 [01:06<00:46,  1.12batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 12/25:  59%|██████████████▏         | 74/125 [01:06<00:45,  1.12batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 12/25:  59%|██████████████▏         | 74/125 [01:07<00:45,  1.12batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 12/25:  60%|██████████████▍         | 75/125 [01:07<00:44,  1.12batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 12/25:  60%|██████████████▍         | 75/125 [01:08<00:44,  1.12batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 12/25:  61%|██████████████▌         | 76/125 [01:08<00:43,  1.12batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 12/25:  61%|██████████████▌         | 76/125 [01:09<00:43,  1.12batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 12/25:  62%|██████████████▊         | 77/125 [01:09<00:42,  1.12batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 12/25:  62%|███████████████▍         | 77/125 [01:09<00:42,  1.12batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 12/25:  62%|███████████████▌         | 78/125 [01:09<00:42,  1.12batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 12/25:  62%|██████████████▉         | 78/125 [01:10<00:42,  1.12batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 12/25:  63%|███████████████▏        | 79/125 [01:10<00:41,  1.12batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 12/25:  63%|███████████████▏        | 79/125 [01:11<00:41,  1.12batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 12/25:  64%|███████████████▎        | 80/125 [01:11<00:40,  1.12batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 12/25:  64%|███████████████▎        | 80/125 [01:12<00:40,  1.12batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 12/25:  65%|███████████████▌        | 81/125 [01:12<00:39,  1.12batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 12/25:  65%|███████████████▌        | 81/125 [01:13<00:39,  1.12batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 12/25:  66%|███████████████▋        | 82/125 [01:13<00:38,  1.12batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 12/25:  66%|███████████████▋        | 82/125 [01:14<00:38,  1.12batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 12/25:  66%|███████████████▉        | 83/125 [01:14<00:37,  1.12batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 12/25:  66%|████████████████▌        | 83/125 [01:15<00:37,  1.12batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 12/25:  67%|████████████████▊        | 84/125 [01:15<00:36,  1.12batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 12/25:  67%|████████████████▏       | 84/125 [01:16<00:36,  1.12batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 12/25:  68%|████████████████▎       | 85/125 [01:16<00:35,  1.12batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 12/25:  68%|████████████████▎       | 85/125 [01:17<00:35,  1.12batch/s, Loss=0.575]\u001b[A\n",
      "Epoch 12/25:  69%|████████████████▌       | 86/125 [01:17<00:34,  1.12batch/s, Loss=0.575]\u001b[A\n",
      "Epoch 12/25:  69%|████████████████▌       | 86/125 [01:18<00:34,  1.12batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 12/25:  70%|████████████████▋       | 87/125 [01:18<00:34,  1.12batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 12/25:  70%|████████████████▋       | 87/125 [01:18<00:34,  1.12batch/s, Loss=0.755]\u001b[A\n",
      "Epoch 12/25:  70%|████████████████▉       | 88/125 [01:18<00:33,  1.12batch/s, Loss=0.755]\u001b[A\n",
      "Epoch 12/25:  70%|████████████████▉       | 88/125 [01:19<00:33,  1.12batch/s, Loss=0.819]\u001b[A\n",
      "Epoch 12/25:  71%|█████████████████       | 89/125 [01:19<00:32,  1.12batch/s, Loss=0.819]\u001b[A\n",
      "Epoch 12/25:  71%|█████████████████▊       | 89/125 [01:20<00:32,  1.12batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 12/25:  72%|██████████████████       | 90/125 [01:20<00:31,  1.12batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 12/25:  72%|█████████████████▎      | 90/125 [01:21<00:31,  1.12batch/s, Loss=0.628]\u001b[A\n",
      "Epoch 12/25:  73%|█████████████████▍      | 91/125 [01:21<00:30,  1.12batch/s, Loss=0.628]\u001b[A\n",
      "Epoch 12/25:  73%|█████████████████▍      | 91/125 [01:22<00:30,  1.12batch/s, Loss=0.589]\u001b[A\n",
      "Epoch 12/25:  74%|█████████████████▋      | 92/125 [01:22<00:29,  1.12batch/s, Loss=0.589]\u001b[A\n",
      "Epoch 12/25:  74%|█████████████████▋      | 92/125 [01:23<00:29,  1.12batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 12/25:  74%|█████████████████▊      | 93/125 [01:23<00:28,  1.12batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 12/25:  74%|█████████████████▊      | 93/125 [01:24<00:28,  1.12batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 12/25:  75%|██████████████████      | 94/125 [01:24<00:27,  1.12batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 12/25:  75%|██████████████████      | 94/125 [01:25<00:27,  1.12batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 12/25:  76%|██████████████████▏     | 95/125 [01:25<00:26,  1.12batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 12/25:  76%|██████████████████▏     | 95/125 [01:26<00:26,  1.12batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 12/25:  77%|██████████████████▍     | 96/125 [01:26<00:25,  1.12batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 12/25:  77%|██████████████████▍     | 96/125 [01:27<00:25,  1.12batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 12/25:  78%|██████████████████▌     | 97/125 [01:27<00:25,  1.12batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 12/25:  78%|██████████████████▌     | 97/125 [01:27<00:25,  1.12batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 12/25:  78%|██████████████████▊     | 98/125 [01:27<00:24,  1.12batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 12/25:  78%|███████████████████▌     | 98/125 [01:28<00:24,  1.12batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 12/25:  79%|███████████████████▊     | 99/125 [01:28<00:23,  1.12batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 12/25:  79%|███████████████████     | 99/125 [01:29<00:23,  1.12batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 12/25:  80%|██████████████████▍    | 100/125 [01:29<00:22,  1.12batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 12/25:  80%|██████████████████▍    | 100/125 [01:30<00:22,  1.12batch/s, Loss=0.655]\u001b[A\n",
      "Epoch 12/25:  81%|██████████████████▌    | 101/125 [01:30<00:21,  1.11batch/s, Loss=0.655]\u001b[A\n",
      "Epoch 12/25:  81%|██████████████████▌    | 101/125 [01:31<00:21,  1.11batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 12/25:  82%|██████████████████▊    | 102/125 [01:31<00:20,  1.11batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 12/25:  82%|██████████████████▊    | 102/125 [01:32<00:20,  1.11batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 12/25:  82%|██████████████████▉    | 103/125 [01:32<00:19,  1.11batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 12/25:  82%|███████████████████▊    | 103/125 [01:33<00:19,  1.11batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 12/25:  83%|███████████████████▉    | 104/125 [01:33<00:18,  1.11batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 12/25:  83%|███████████████████▏   | 104/125 [01:34<00:18,  1.11batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 12/25:  84%|███████████████████▎   | 105/125 [01:34<00:17,  1.11batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 12/25:  84%|███████████████████▎   | 105/125 [01:35<00:17,  1.11batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 12/25:  85%|███████████████████▌   | 106/125 [01:35<00:17,  1.11batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 12/25:  85%|███████████████████▌   | 106/125 [01:36<00:17,  1.11batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 12/25:  86%|███████████████████▋   | 107/125 [01:36<00:16,  1.11batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 12/25:  86%|███████████████████▋   | 107/125 [01:36<00:16,  1.11batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 12/25:  86%|███████████████████▊   | 108/125 [01:36<00:15,  1.11batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 12/25:  86%|███████████████████▊   | 108/125 [01:37<00:15,  1.11batch/s, Loss=0.726]\u001b[A\n",
      "Epoch 12/25:  87%|████████████████████   | 109/125 [01:37<00:14,  1.11batch/s, Loss=0.726]\u001b[A\n",
      "Epoch 12/25:  87%|████████████████████   | 109/125 [01:38<00:14,  1.11batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 12/25:  88%|████████████████████▏  | 110/125 [01:38<00:13,  1.11batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 12/25:  88%|████████████████████▏  | 110/125 [01:39<00:13,  1.11batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 12/25:  89%|████████████████████▍  | 111/125 [01:39<00:12,  1.11batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 12/25:  89%|████████████████████▍  | 111/125 [01:40<00:12,  1.11batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 12/25:  90%|████████████████████▌  | 112/125 [01:40<00:11,  1.11batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 12/25:  90%|█████████████████████▌  | 112/125 [01:41<00:11,  1.11batch/s, Loss=0.75]\u001b[A\n",
      "Epoch 12/25:  90%|█████████████████████▋  | 113/125 [01:41<00:10,  1.11batch/s, Loss=0.75]\u001b[A\n",
      "Epoch 12/25:  90%|████████████████████▊  | 113/125 [01:42<00:10,  1.11batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 12/25:  91%|████████████████████▉  | 114/125 [01:42<00:09,  1.11batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 12/25:  91%|████████████████████▉  | 114/125 [01:43<00:09,  1.11batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 12/25:  92%|█████████████████████▏ | 115/125 [01:43<00:08,  1.12batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 12/25:  92%|█████████████████████▏ | 115/125 [01:44<00:08,  1.12batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 12/25:  93%|█████████████████████▎ | 116/125 [01:44<00:08,  1.12batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 12/25:  93%|█████████████████████▎ | 116/125 [01:44<00:08,  1.12batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 12/25:  94%|█████████████████████▌ | 117/125 [01:44<00:07,  1.12batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 12/25:  94%|█████████████████████▌ | 117/125 [01:45<00:07,  1.12batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 12/25:  94%|█████████████████████▋ | 118/125 [01:45<00:06,  1.12batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 12/25:  94%|█████████████████████▋ | 118/125 [01:46<00:06,  1.12batch/s, Loss=0.762]\u001b[A\n",
      "Epoch 12/25:  95%|█████████████████████▉ | 119/125 [01:46<00:05,  1.12batch/s, Loss=0.762]\u001b[A\n",
      "Epoch 12/25:  95%|█████████████████████▉ | 119/125 [01:47<00:05,  1.12batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 12/25:  96%|██████████████████████ | 120/125 [01:47<00:04,  1.12batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 12/25:  96%|██████████████████████ | 120/125 [01:48<00:04,  1.12batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 12/25:  97%|██████████████████████▎| 121/125 [01:48<00:03,  1.12batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 12/25:  97%|██████████████████████▎| 121/125 [01:49<00:03,  1.12batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 12/25:  98%|██████████████████████▍| 122/125 [01:49<00:02,  1.12batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 12/25:  98%|██████████████████████▍| 122/125 [01:50<00:02,  1.12batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 12/25:  98%|██████████████████████▋| 123/125 [01:50<00:01,  1.12batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 12/25:  98%|███████████████████████▌| 123/125 [01:51<00:01,  1.12batch/s, Loss=0.77]\u001b[A\n",
      "Epoch 12/25:  99%|███████████████████████▊| 124/125 [01:51<00:00,  1.11batch/s, Loss=0.77]\u001b[A\n",
      "Epoch 12/25:  99%|██████████████████████▊| 124/125 [01:52<00:00,  1.11batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 12/25: 100%|███████████████████████| 125/125 [01:52<00:00,  1.11batch/s, Loss=0.578]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 12/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   1%|▏                         | 1/125 [00:00<00:43,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   2%|▍                         | 2/125 [00:00<00:42,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   2%|▌                         | 3/125 [00:01<00:42,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   3%|▊                         | 4/125 [00:01<00:42,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   4%|█                         | 5/125 [00:01<00:41,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   5%|█▏                        | 6/125 [00:02<00:41,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   6%|█▍                        | 7/125 [00:02<00:40,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   6%|█▋                        | 8/125 [00:02<00:40,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   7%|█▊                        | 9/125 [00:03<00:40,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   8%|██                       | 10/125 [00:03<00:39,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:   9%|██▏                      | 11/125 [00:03<00:39,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  10%|██▍                      | 12/125 [00:04<00:39,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  10%|██▌                      | 13/125 [00:04<00:38,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  11%|██▊                      | 14/125 [00:04<00:38,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  12%|███                      | 15/125 [00:05<00:38,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  13%|███▏                     | 16/125 [00:05<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  14%|███▍                     | 17/125 [00:05<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  14%|███▌                     | 18/125 [00:06<00:37,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  15%|███▊                     | 19/125 [00:06<00:36,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  16%|████                     | 20/125 [00:06<00:36,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  17%|████▏                    | 21/125 [00:07<00:36,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  18%|████▍                    | 22/125 [00:07<00:35,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  18%|████▌                    | 23/125 [00:07<00:35,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  20%|█████                    | 25/125 [00:08<00:34,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  21%|█████▏                   | 26/125 [00:09<00:34,  2.87batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  22%|█████▌                   | 28/125 [00:09<00:33,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  23%|█████▊                   | 29/125 [00:10<00:33,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  24%|██████                   | 30/125 [00:10<00:32,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  25%|██████▏                  | 31/125 [00:10<00:32,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  26%|██████▍                  | 32/125 [00:11<00:32,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  26%|██████▌                  | 33/125 [00:11<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  27%|██████▊                  | 34/125 [00:11<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  28%|███████                  | 35/125 [00:12<00:31,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  29%|███████▏                 | 36/125 [00:12<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  30%|███████▍                 | 37/125 [00:12<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  31%|███████▊                 | 39/125 [00:13<00:29,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  32%|████████                 | 40/125 [00:13<00:29,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  34%|████████▍                | 42/125 [00:14<00:28,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  34%|████████▌                | 43/125 [00:14<00:28,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  36%|█████████                | 45/125 [00:15<00:27,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  37%|█████████▏               | 46/125 [00:15<00:27,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  38%|█████████▌               | 48/125 [00:16<00:26,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  39%|█████████▊               | 49/125 [00:17<00:26,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  40%|██████████               | 50/125 [00:17<00:26,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  41%|██████████▏              | 51/125 [00:17<00:25,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  42%|██████████▌              | 53/125 [00:18<00:24,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  43%|██████████▊              | 54/125 [00:18<00:24,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  44%|███████████              | 55/125 [00:19<00:24,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  45%|███████████▏             | 56/125 [00:19<00:23,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  46%|███████████▍             | 57/125 [00:19<00:23,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  47%|███████████▊             | 59/125 [00:20<00:22,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  48%|████████████             | 60/125 [00:20<00:22,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  50%|████████████▍            | 62/125 [00:21<00:21,  2.89batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  50%|████████████▌            | 63/125 [00:21<00:21,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  52%|█████████████            | 65/125 [00:22<00:20,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  53%|█████████████▏           | 66/125 [00:22<00:20,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.88batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  54%|█████████████▌           | 68/125 [00:23<00:19,  2.86batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  55%|█████████████▊           | 69/125 [00:24<00:20,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  56%|██████████████           | 70/125 [00:24<00:20,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  57%|██████████████▏          | 71/125 [00:24<00:20,  2.65batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  58%|██████████████▍          | 72/125 [00:25<00:19,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  58%|██████████████▌          | 73/125 [00:25<00:19,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  59%|██████████████▊          | 74/125 [00:25<00:18,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  60%|███████████████          | 75/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  61%|███████████████▏         | 76/125 [00:26<00:17,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  62%|███████████████▍         | 77/125 [00:26<00:17,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  63%|███████████████▊         | 79/125 [00:27<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  64%|████████████████         | 80/125 [00:28<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  66%|████████████████▍        | 82/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  68%|█████████████████        | 85/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  70%|█████████████████▌       | 88/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  72%|██████████████████       | 90/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  73%|██████████████████▏      | 91/125 [00:31<00:12,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  74%|██████████████████▌      | 93/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  75%|██████████████████▊      | 94/125 [00:32<00:11,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  77%|███████████████████▏     | 96/125 [00:33<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  79%|███████████████████▊     | 99/125 [00:34<00:09,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  82%|███████████████████▌    | 102/125 [00:35<00:08,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  83%|███████████████████▉    | 104/125 [00:36<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  84%|████████████████████▏   | 105/125 [00:36<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  86%|████████████████████▌   | 107/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  86%|████████████████████▋   | 108/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  88%|█████████████████████   | 110/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  90%|█████████████████████▋  | 113/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  93%|██████████████████████▎ | 116/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  94%|██████████████████████▋ | 118/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  95%|██████████████████████▊ | 119/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  97%|███████████████████████▏| 121/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  98%|███████████████████████▍| 122/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25:  99%|███████████████████████▊| 124/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 12/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.84batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Eval Accuracy: 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 13/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.765]\u001b[A\n",
      "Epoch 13/25:   1%|▏                        | 1/125 [00:01<02:50,  1.37s/batch, Loss=0.765]\u001b[A\n",
      "Epoch 13/25:   1%|▏                        | 1/125 [00:02<02:50,  1.37s/batch, Loss=0.894]\u001b[A\n",
      "Epoch 13/25:   2%|▍                        | 2/125 [00:02<02:16,  1.11s/batch, Loss=0.894]\u001b[A\n",
      "Epoch 13/25:   2%|▍                         | 2/125 [00:03<02:16,  1.11s/batch, Loss=0.76]\u001b[A\n",
      "Epoch 13/25:   2%|▌                         | 3/125 [00:03<02:04,  1.02s/batch, Loss=0.76]\u001b[A\n",
      "Epoch 13/25:   2%|▌                        | 3/125 [00:04<02:04,  1.02s/batch, Loss=0.402]\u001b[A\n",
      "Epoch 13/25:   3%|▊                        | 4/125 [00:04<01:58,  1.02batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 13/25:   3%|▊                        | 4/125 [00:05<01:58,  1.02batch/s, Loss=0.563]\u001b[A\n",
      "Epoch 13/25:   4%|█                        | 5/125 [00:05<01:54,  1.04batch/s, Loss=0.563]\u001b[A\n",
      "Epoch 13/25:   4%|█                        | 5/125 [00:05<01:54,  1.04batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 13/25:   5%|█▏                       | 6/125 [00:05<01:52,  1.06batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 13/25:   5%|█▏                       | 6/125 [00:06<01:52,  1.06batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 13/25:   6%|█▍                       | 7/125 [00:06<01:50,  1.07batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 13/25:   6%|█▍                       | 7/125 [00:07<01:50,  1.07batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 13/25:   6%|█▌                       | 8/125 [00:07<01:48,  1.08batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 13/25:   6%|█▌                       | 8/125 [00:08<01:48,  1.08batch/s, Loss=0.721]\u001b[A\n",
      "Epoch 13/25:   7%|█▊                       | 9/125 [00:08<01:47,  1.08batch/s, Loss=0.721]\u001b[A\n",
      "Epoch 13/25:   7%|█▊                       | 9/125 [00:09<01:47,  1.08batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 13/25:   8%|█▉                      | 10/125 [00:09<01:46,  1.08batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 13/25:   8%|█▉                      | 10/125 [00:10<01:46,  1.08batch/s, Loss=0.425]\u001b[A\n",
      "Epoch 13/25:   9%|██                      | 11/125 [00:10<01:45,  1.08batch/s, Loss=0.425]\u001b[A\n",
      "Epoch 13/25:   9%|██                      | 11/125 [00:11<01:45,  1.08batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 13/25:  10%|██▎                     | 12/125 [00:11<01:44,  1.09batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 13/25:  10%|██▎                     | 12/125 [00:12<01:44,  1.09batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 13/25:  10%|██▍                     | 13/125 [00:12<01:43,  1.09batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 13/25:  10%|██▍                     | 13/125 [00:13<01:43,  1.09batch/s, Loss=0.698]\u001b[A\n",
      "Epoch 13/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.08batch/s, Loss=0.698]\u001b[A\n",
      "Epoch 13/25:  11%|██▋                     | 14/125 [00:14<01:42,  1.08batch/s, Loss=0.611]\u001b[A\n",
      "Epoch 13/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.611]\u001b[A\n",
      "Epoch 13/25:  12%|██▉                     | 15/125 [00:15<01:41,  1.08batch/s, Loss=0.782]\u001b[A\n",
      "Epoch 13/25:  13%|███                     | 16/125 [00:15<01:40,  1.09batch/s, Loss=0.782]\u001b[A\n",
      "Epoch 13/25:  13%|███                     | 16/125 [00:16<01:40,  1.09batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.484]\u001b[A\n",
      "Epoch 13/25:  14%|███▍                    | 18/125 [00:16<01:38,  1.09batch/s, Loss=0.484]\u001b[A\n",
      "Epoch 13/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.09batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 13/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.09batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 13/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.09batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 13/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.09batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 13/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.09batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 13/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 13/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 13/25:  18%|████▏                   | 22/125 [00:20<01:34,  1.09batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 13/25:  18%|████▏                   | 22/125 [00:21<01:34,  1.09batch/s, Loss=0.753]\u001b[A\n",
      "Epoch 13/25:  18%|████▍                   | 23/125 [00:21<01:33,  1.09batch/s, Loss=0.753]\u001b[A\n",
      "Epoch 13/25:  18%|████▍                   | 23/125 [00:22<01:33,  1.09batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 13/25:  19%|████▌                   | 24/125 [00:22<01:32,  1.09batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 13/25:  19%|████▌                   | 24/125 [00:23<01:32,  1.09batch/s, Loss=0.785]\u001b[A\n",
      "Epoch 13/25:  20%|████▊                   | 25/125 [00:23<01:31,  1.09batch/s, Loss=0.785]\u001b[A\n",
      "Epoch 13/25:  20%|████▊                   | 25/125 [00:24<01:31,  1.09batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 13/25:  21%|████▉                   | 26/125 [00:24<01:30,  1.09batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 13/25:  21%|████▉                   | 26/125 [00:25<01:30,  1.09batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 13/25:  22%|█████▏                  | 27/125 [00:25<01:29,  1.09batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 13/25:  22%|█████▏                  | 27/125 [00:26<01:29,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 13/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 13/25:  22%|█████▍                  | 28/125 [00:27<01:29,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 13/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 13/25:  23%|█████▌                  | 29/125 [00:28<01:28,  1.09batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 13/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 13/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  25%|█████▉                  | 31/125 [00:28<01:26,  1.09batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 13/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 13/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 13/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 13/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 13/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 13/25:  27%|███████                   | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.7]\u001b[A\n",
      "Epoch 13/25:  28%|███████▎                  | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.7]\u001b[A\n",
      "Epoch 13/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 13/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 13/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 13/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 13/25:  30%|███████                 | 37/125 [00:35<01:20,  1.09batch/s, Loss=0.765]\u001b[A\n",
      "Epoch 13/25:  30%|███████▎                | 38/125 [00:35<01:19,  1.09batch/s, Loss=0.765]\u001b[A\n",
      "Epoch 13/25:  30%|███████▎                | 38/125 [00:36<01:19,  1.09batch/s, Loss=0.672]\u001b[A\n",
      "Epoch 13/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.09batch/s, Loss=0.672]\u001b[A\n",
      "Epoch 13/25:  31%|███████▊                 | 39/125 [00:37<01:19,  1.09batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 13/25:  32%|████████                 | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 13/25:  32%|███████▋                | 40/125 [00:38<01:18,  1.09batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 13/25:  33%|███████▊                | 41/125 [00:38<01:18,  1.08batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 13/25:  33%|███████▊                | 41/125 [00:39<01:18,  1.08batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 13/25:  34%|████████                | 42/125 [00:39<01:17,  1.07batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 13/25:  34%|████████                | 42/125 [00:40<01:17,  1.07batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  34%|████████▎               | 43/125 [00:40<01:17,  1.06batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  34%|████████▎               | 43/125 [00:41<01:17,  1.06batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 13/25:  35%|████████▍               | 44/125 [00:41<01:16,  1.06batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 13/25:  35%|████████▍               | 44/125 [00:41<01:16,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 13/25:  36%|████████▋               | 45/125 [00:41<01:15,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 13/25:  36%|█████████                | 45/125 [00:42<01:15,  1.06batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 13/25:  37%|█████████▏               | 46/125 [00:42<01:14,  1.06batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 13/25:  37%|████████▊               | 46/125 [00:43<01:14,  1.06batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 13/25:  38%|█████████               | 47/125 [00:43<01:13,  1.06batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 13/25:  38%|█████████               | 47/125 [00:44<01:13,  1.06batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 13/25:  38%|█████████▏              | 48/125 [00:44<01:12,  1.06batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 13/25:  38%|█████████▏              | 48/125 [00:45<01:12,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  39%|█████████▍              | 49/125 [00:45<01:11,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  39%|█████████▍              | 49/125 [00:46<01:11,  1.06batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 13/25:  40%|█████████▌              | 50/125 [00:46<01:10,  1.06batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 13/25:  40%|██████████               | 50/125 [00:47<01:10,  1.06batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 13/25:  41%|██████████▏              | 51/125 [00:47<01:09,  1.06batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 13/25:  41%|█████████▊              | 51/125 [00:48<01:09,  1.06batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 13/25:  42%|█████████▉              | 52/125 [00:48<01:08,  1.06batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 13/25:  42%|█████████▉              | 52/125 [00:49<01:08,  1.06batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 13/25:  42%|██████████▏             | 53/125 [00:49<01:07,  1.06batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 13/25:  42%|██████████▏             | 53/125 [00:50<01:07,  1.06batch/s, Loss=0.575]\u001b[A\n",
      "Epoch 13/25:  43%|██████████▎             | 54/125 [00:50<01:06,  1.06batch/s, Loss=0.575]\u001b[A\n",
      "Epoch 13/25:  43%|██████████▊              | 54/125 [00:51<01:06,  1.06batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 13/25:  44%|███████████              | 55/125 [00:51<01:05,  1.06batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 13/25:  44%|██████████▌             | 55/125 [00:52<01:05,  1.06batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 13/25:  45%|██████████▊             | 56/125 [00:52<01:05,  1.06batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 13/25:  45%|██████████▊             | 56/125 [00:53<01:05,  1.06batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  46%|██████████▉             | 57/125 [00:53<01:04,  1.06batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  46%|██████████▉             | 57/125 [00:54<01:04,  1.06batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 13/25:  46%|███████████▏            | 58/125 [00:54<01:03,  1.06batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 13/25:  46%|███████████▏            | 58/125 [00:55<01:03,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  47%|███████████▎            | 59/125 [00:55<01:02,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  47%|███████████▎            | 59/125 [00:56<01:02,  1.06batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 13/25:  48%|███████████▌            | 60/125 [00:56<01:01,  1.06batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 13/25:  48%|███████████▌            | 60/125 [00:57<01:01,  1.06batch/s, Loss=0.567]\u001b[A\n",
      "Epoch 13/25:  49%|███████████▋            | 61/125 [00:57<01:00,  1.06batch/s, Loss=0.567]\u001b[A\n",
      "Epoch 13/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.06batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 13/25:  50%|███████████▉            | 62/125 [00:58<00:59,  1.05batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 13/25:  50%|███████████▉            | 62/125 [00:58<00:59,  1.05batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 13/25:  50%|████████████            | 63/125 [00:58<00:58,  1.05batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 13/25:  50%|████████████            | 63/125 [00:59<00:58,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 13/25:  51%|████████████▎           | 64/125 [00:59<00:57,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 13/25:  51%|████████████▎           | 64/125 [01:00<00:57,  1.05batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 13/25:  52%|████████████▍           | 65/125 [01:00<00:56,  1.06batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 13/25:  52%|████████████▍           | 65/125 [01:01<00:56,  1.06batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 13/25:  53%|████████████▋           | 66/125 [01:01<00:55,  1.06batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 13/25:  53%|████████████▋           | 66/125 [01:02<00:55,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  54%|████████████▊           | 67/125 [01:02<00:54,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  54%|████████████▊           | 67/125 [01:03<00:54,  1.06batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 13/25:  54%|█████████████           | 68/125 [01:03<00:53,  1.06batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 13/25:  54%|█████████████           | 68/125 [01:04<00:53,  1.06batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  55%|█████████████▏          | 69/125 [01:04<00:53,  1.06batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  55%|█████████████▏          | 69/125 [01:05<00:53,  1.06batch/s, Loss=0.757]\u001b[A\n",
      "Epoch 13/25:  56%|█████████████▍          | 70/125 [01:05<00:52,  1.06batch/s, Loss=0.757]\u001b[A\n",
      "Epoch 13/25:  56%|█████████████▍          | 70/125 [01:06<00:52,  1.06batch/s, Loss=0.463]\u001b[A\n",
      "Epoch 13/25:  57%|█████████████▋          | 71/125 [01:06<00:51,  1.06batch/s, Loss=0.463]\u001b[A\n",
      "Epoch 13/25:  57%|█████████████▋          | 71/125 [01:07<00:51,  1.06batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 13/25:  58%|█████████████▊          | 72/125 [01:07<00:50,  1.06batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 13/25:  58%|█████████████▊          | 72/125 [01:08<00:50,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  58%|██████████████          | 73/125 [01:08<00:49,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  58%|██████████████          | 73/125 [01:09<00:49,  1.06batch/s, Loss=0.642]\u001b[A\n",
      "Epoch 13/25:  59%|██████████████▏         | 74/125 [01:09<00:48,  1.06batch/s, Loss=0.642]\u001b[A\n",
      "Epoch 13/25:  59%|██████████████▏         | 74/125 [01:10<00:48,  1.06batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 13/25:  60%|██████████████▍         | 75/125 [01:10<00:47,  1.06batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 13/25:  60%|██████████████▍         | 75/125 [01:11<00:47,  1.06batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 13/25:  61%|██████████████▌         | 76/125 [01:11<00:46,  1.06batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 13/25:  61%|██████████████▌         | 76/125 [01:12<00:46,  1.06batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 13/25:  62%|██████████████▊         | 77/125 [01:12<00:45,  1.06batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 13/25:  62%|██████████████▊         | 77/125 [01:13<00:45,  1.06batch/s, Loss=0.777]\u001b[A\n",
      "Epoch 13/25:  62%|██████████████▉         | 78/125 [01:13<00:44,  1.06batch/s, Loss=0.777]\u001b[A\n",
      "Epoch 13/25:  62%|██████████████▉         | 78/125 [01:14<00:44,  1.06batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  63%|███████████████▏        | 79/125 [01:14<00:43,  1.06batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 13/25:  63%|███████████████▏        | 79/125 [01:15<00:43,  1.06batch/s, Loss=0.832]\u001b[A\n",
      "Epoch 13/25:  64%|███████████████▎        | 80/125 [01:15<00:42,  1.06batch/s, Loss=0.832]\u001b[A\n",
      "Epoch 13/25:  64%|███████████████▎        | 80/125 [01:15<00:42,  1.06batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 13/25:  65%|███████████████▌        | 81/125 [01:15<00:41,  1.06batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 13/25:  65%|███████████████▌        | 81/125 [01:16<00:41,  1.06batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 13/25:  66%|███████████████▋        | 82/125 [01:16<00:40,  1.06batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 13/25:  66%|███████████████▋        | 82/125 [01:17<00:40,  1.06batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 13/25:  66%|███████████████▉        | 83/125 [01:17<00:39,  1.06batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 13/25:  66%|███████████████▉        | 83/125 [01:18<00:39,  1.06batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 13/25:  67%|████████████████▏       | 84/125 [01:18<00:38,  1.06batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 13/25:  67%|████████████████▏       | 84/125 [01:19<00:38,  1.06batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 13/25:  68%|████████████████▎       | 85/125 [01:19<00:37,  1.06batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 13/25:  68%|█████████████████        | 85/125 [01:20<00:37,  1.06batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 13/25:  69%|█████████████████▏       | 86/125 [01:20<00:36,  1.06batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 13/25:  69%|████████████████▌       | 86/125 [01:21<00:36,  1.06batch/s, Loss=0.772]\u001b[A\n",
      "Epoch 13/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.06batch/s, Loss=0.772]\u001b[A\n",
      "Epoch 13/25:  70%|████████████████▋       | 87/125 [01:22<00:35,  1.06batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 13/25:  70%|████████████████▉       | 88/125 [01:22<00:34,  1.06batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 13/25:  70%|████████████████▉       | 88/125 [01:23<00:34,  1.06batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.06batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  71%|█████████████████       | 89/125 [01:24<00:33,  1.06batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 13/25:  72%|█████████████████▎      | 90/125 [01:24<00:33,  1.06batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 13/25:  72%|█████████████████▎      | 90/125 [01:25<00:33,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 13/25:  73%|█████████████████▍      | 91/125 [01:25<00:32,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 13/25:  73%|█████████████████▍      | 91/125 [01:26<00:32,  1.06batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 13/25:  74%|█████████████████▋      | 92/125 [01:26<00:31,  1.06batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 13/25:  74%|█████████████████▋      | 92/125 [01:27<00:31,  1.06batch/s, Loss=0.726]\u001b[A\n",
      "Epoch 13/25:  74%|█████████████████▊      | 93/125 [01:27<00:30,  1.06batch/s, Loss=0.726]\u001b[A\n",
      "Epoch 13/25:  74%|██████████████████▌      | 93/125 [01:28<00:30,  1.06batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 13/25:  75%|██████████████████▊      | 94/125 [01:28<00:29,  1.06batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 13/25:  75%|██████████████████      | 94/125 [01:29<00:29,  1.06batch/s, Loss=0.679]\u001b[A\n",
      "Epoch 13/25:  76%|██████████████████▏     | 95/125 [01:29<00:28,  1.06batch/s, Loss=0.679]\u001b[A\n",
      "Epoch 13/25:  76%|██████████████████▏     | 95/125 [01:30<00:28,  1.06batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 13/25:  77%|██████████████████▍     | 96/125 [01:30<00:27,  1.05batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 13/25:  77%|██████████████████▍     | 96/125 [01:31<00:27,  1.05batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 13/25:  78%|██████████████████▌     | 97/125 [01:31<00:26,  1.05batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 13/25:  78%|███████████████████▍     | 97/125 [01:32<00:26,  1.05batch/s, Loss=0.67]\u001b[A\n",
      "Epoch 13/25:  78%|███████████████████▌     | 98/125 [01:32<00:25,  1.05batch/s, Loss=0.67]\u001b[A\n",
      "Epoch 13/25:  78%|██████████████████▊     | 98/125 [01:33<00:25,  1.05batch/s, Loss=0.788]\u001b[A\n",
      "Epoch 13/25:  79%|███████████████████     | 99/125 [01:33<00:24,  1.06batch/s, Loss=0.788]\u001b[A\n",
      "Epoch 13/25:  79%|███████████████████     | 99/125 [01:33<00:24,  1.06batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  80%|██████████████████▍    | 100/125 [01:33<00:23,  1.06batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 13/25:  80%|██████████████████▍    | 100/125 [01:34<00:23,  1.06batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 13/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.06batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 13/25:  81%|██████████████████▌    | 101/125 [01:35<00:22,  1.06batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 13/25:  82%|██████████████████▊    | 102/125 [01:35<00:21,  1.06batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 13/25:  82%|██████████████████▊    | 102/125 [01:36<00:21,  1.06batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 13/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.06batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 13/25:  82%|██████████████████▉    | 103/125 [01:37<00:20,  1.06batch/s, Loss=0.564]\u001b[A\n",
      "Epoch 13/25:  83%|███████████████████▏   | 104/125 [01:37<00:19,  1.06batch/s, Loss=0.564]\u001b[A\n",
      "Epoch 13/25:  83%|███████████████████▉    | 104/125 [01:38<00:19,  1.06batch/s, Loss=0.93]\u001b[A\n",
      "Epoch 13/25:  84%|████████████████████▏   | 105/125 [01:38<00:18,  1.06batch/s, Loss=0.93]\u001b[A\n",
      "Epoch 13/25:  84%|███████████████████▎   | 105/125 [01:39<00:18,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  85%|███████████████████▌   | 106/125 [01:39<00:17,  1.06batch/s, Loss=0.615]\u001b[A\n",
      "Epoch 13/25:  85%|███████████████████▌   | 106/125 [01:40<00:17,  1.06batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 13/25:  86%|███████████████████▋   | 107/125 [01:40<00:17,  1.06batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 13/25:  86%|███████████████████▋   | 107/125 [01:41<00:17,  1.06batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 13/25:  86%|███████████████████▊   | 108/125 [01:41<00:16,  1.06batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 13/25:  86%|███████████████████▊   | 108/125 [01:42<00:16,  1.06batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 13/25:  87%|████████████████████   | 109/125 [01:42<00:15,  1.06batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 13/25:  87%|████████████████████   | 109/125 [01:43<00:15,  1.06batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 13/25:  88%|████████████████████▏  | 110/125 [01:43<00:14,  1.05batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 13/25:  88%|█████████████████████   | 110/125 [01:44<00:14,  1.05batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 13/25:  89%|█████████████████████▎  | 111/125 [01:44<00:13,  1.06batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 13/25:  89%|████████████████████▍  | 111/125 [01:45<00:13,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  90%|████████████████████▌  | 112/125 [01:45<00:12,  1.06batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 13/25:  90%|████████████████████▌  | 112/125 [01:46<00:12,  1.06batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 13/25:  90%|████████████████████▊  | 113/125 [01:46<00:11,  1.05batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 13/25:  90%|████████████████████▊  | 113/125 [01:47<00:11,  1.05batch/s, Loss=0.689]\u001b[A\n",
      "Epoch 13/25:  91%|████████████████████▉  | 114/125 [01:47<00:10,  1.05batch/s, Loss=0.689]\u001b[A\n",
      "Epoch 13/25:  91%|████████████████████▉  | 114/125 [01:48<00:10,  1.05batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 13/25:  92%|█████████████████████▏ | 115/125 [01:48<00:09,  1.05batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 13/25:  92%|█████████████████████▏ | 115/125 [01:49<00:09,  1.05batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 13/25:  93%|█████████████████████▎ | 116/125 [01:49<00:08,  1.05batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 13/25:  93%|█████████████████████▎ | 116/125 [01:50<00:08,  1.05batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  94%|█████████████████████▌ | 117/125 [01:50<00:07,  1.06batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 13/25:  94%|█████████████████████▌ | 117/125 [01:50<00:07,  1.06batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 13/25:  94%|█████████████████████▋ | 118/125 [01:50<00:06,  1.06batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 13/25:  94%|█████████████████████▋ | 118/125 [01:51<00:06,  1.06batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 13/25:  95%|█████████████████████▉ | 119/125 [01:51<00:05,  1.06batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 13/25:  95%|█████████████████████▉ | 119/125 [01:52<00:05,  1.06batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 13/25:  96%|██████████████████████ | 120/125 [01:52<00:04,  1.06batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 13/25:  96%|██████████████████████ | 120/125 [01:53<00:04,  1.06batch/s, Loss=0.675]\u001b[A\n",
      "Epoch 13/25:  97%|██████████████████████▎| 121/125 [01:53<00:03,  1.06batch/s, Loss=0.675]\u001b[A\n",
      "Epoch 13/25:  97%|███████████████████████▏| 121/125 [01:54<00:03,  1.06batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 13/25:  98%|███████████████████████▍| 122/125 [01:54<00:02,  1.06batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 13/25:  98%|██████████████████████▍| 122/125 [01:55<00:02,  1.06batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 13/25:  98%|██████████████████████▋| 123/125 [01:55<00:01,  1.06batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 13/25:  98%|██████████████████████▋| 123/125 [01:56<00:01,  1.06batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 13/25:  99%|██████████████████████▊| 124/125 [01:56<00:00,  1.06batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 13/25:  99%|██████████████████████▊| 124/125 [01:57<00:00,  1.06batch/s, Loss=0.564]\u001b[A\n",
      "Epoch 13/25: 100%|███████████████████████| 125/125 [01:57<00:00,  1.06batch/s, Loss=0.564]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 13/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   1%|▏                         | 1/125 [00:00<00:45,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   2%|▍                         | 2/125 [00:00<00:44,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   2%|▌                         | 3/125 [00:01<00:44,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   3%|▊                         | 4/125 [00:01<00:44,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   4%|█                         | 5/125 [00:01<00:43,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   8%|██                       | 10/125 [00:03<00:41,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  10%|██▍                      | 12/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  10%|██▌                      | 13/125 [00:04<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  12%|███                      | 15/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  13%|███▏                     | 16/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  15%|███▊                     | 19/125 [00:06<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  16%|████                     | 20/125 [00:07<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  18%|████▍                    | 22/125 [00:08<00:37,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  18%|████▌                    | 23/125 [00:08<00:37,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  20%|█████                    | 25/125 [00:09<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  24%|██████                   | 30/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  26%|██████▌                  | 33/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  28%|███████                  | 35/125 [00:12<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  30%|███████▍                 | 37/125 [00:13<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  32%|████████                 | 40/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  36%|█████████                | 45/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  38%|█████████▌               | 48/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  40%|██████████               | 50/125 [00:18<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  44%|███████████              | 55/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  47%|███████████▊             | 59/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  48%|████████████             | 60/125 [00:21<00:23,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  74%|██████████████████▍      | 92/125 [00:33<00:12,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  76%|███████████████████      | 95/125 [00:34<00:11,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  78%|███████████████████▍     | 97/125 [00:35<00:10,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  78%|███████████████████▌     | 98/125 [00:35<00:10,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  79%|███████████████████▊     | 99/125 [00:36<00:09,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  80%|███████████████████▏    | 100/125 [00:36<00:09,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  82%|███████████████████▊    | 103/125 [00:37<00:08,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  85%|████████████████████▎   | 106/125 [00:38<00:07,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  86%|████████████████████▌   | 107/125 [00:39<00:06,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  87%|████████████████████▉   | 109/125 [00:39<00:06,  2.65batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  88%|█████████████████████   | 110/125 [00:40<00:05,  2.67batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  89%|█████████████████████▎  | 111/125 [00:40<00:05,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  90%|█████████████████████▋  | 113/125 [00:41<00:04,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  91%|█████████████████████▉  | 114/125 [00:41<00:04,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  92%|██████████████████████  | 115/125 [00:42<00:03,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  94%|██████████████████████▋ | 118/125 [00:43<00:02,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  97%|███████████████████████▏| 121/125 [00:44<00:01,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25:  99%|███████████████████████▊| 124/125 [00:45<00:00,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 13/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.74batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Eval Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 14/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 14/25:   1%|▏                        | 1/125 [00:01<03:12,  1.55s/batch, Loss=0.527]\u001b[A\n",
      "Epoch 14/25:   1%|▏                         | 1/125 [00:02<03:12,  1.55s/batch, Loss=0.52]\u001b[A\n",
      "Epoch 14/25:   2%|▍                         | 2/125 [00:02<02:27,  1.20s/batch, Loss=0.52]\u001b[A\n",
      "Epoch 14/25:   2%|▍                         | 2/125 [00:03<02:27,  1.20s/batch, Loss=0.63]\u001b[A\n",
      "Epoch 14/25:   2%|▌                         | 3/125 [00:03<02:12,  1.09s/batch, Loss=0.63]\u001b[A\n",
      "Epoch 14/25:   2%|▌                        | 3/125 [00:04<02:12,  1.09s/batch, Loss=0.613]\u001b[A\n",
      "Epoch 14/25:   3%|▊                        | 4/125 [00:04<02:05,  1.03s/batch, Loss=0.613]\u001b[A\n",
      "Epoch 14/25:   3%|▊                         | 4/125 [00:05<02:05,  1.03s/batch, Loss=0.45]\u001b[A\n",
      "Epoch 14/25:   4%|█                         | 5/125 [00:05<02:00,  1.01s/batch, Loss=0.45]\u001b[A\n",
      "Epoch 14/25:   4%|█                        | 5/125 [00:06<02:00,  1.01s/batch, Loss=0.431]\u001b[A\n",
      "Epoch 14/25:   5%|█▏                       | 6/125 [00:06<01:57,  1.01batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 14/25:   5%|█▏                       | 6/125 [00:07<01:57,  1.01batch/s, Loss=0.501]\u001b[A\n",
      "Epoch 14/25:   6%|█▍                       | 7/125 [00:07<01:55,  1.02batch/s, Loss=0.501]\u001b[A\n",
      "Epoch 14/25:   6%|█▍                       | 7/125 [00:08<01:55,  1.02batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 14/25:   6%|█▌                       | 8/125 [00:08<01:53,  1.03batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 14/25:   6%|█▌                       | 8/125 [00:09<01:53,  1.03batch/s, Loss=0.517]\u001b[A\n",
      "Epoch 14/25:   7%|█▊                       | 9/125 [00:09<01:51,  1.04batch/s, Loss=0.517]\u001b[A\n",
      "Epoch 14/25:   7%|█▊                       | 9/125 [00:10<01:51,  1.04batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 14/25:   8%|█▉                      | 10/125 [00:10<01:50,  1.04batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 14/25:   8%|█▉                      | 10/125 [00:11<01:50,  1.04batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 14/25:   9%|██                      | 11/125 [00:11<01:48,  1.05batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 14/25:   9%|██                      | 11/125 [00:12<01:48,  1.05batch/s, Loss=0.702]\u001b[A\n",
      "Epoch 14/25:  10%|██▎                     | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.702]\u001b[A\n",
      "Epoch 14/25:  10%|██▎                     | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 14/25:  10%|██▍                     | 13/125 [00:12<01:46,  1.05batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 14/25:  10%|██▍                     | 13/125 [00:13<01:46,  1.05batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 14/25:  11%|██▋                     | 14/125 [00:13<01:45,  1.05batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 14/25:  11%|██▋                     | 14/125 [00:14<01:45,  1.05batch/s, Loss=0.716]\u001b[A\n",
      "Epoch 14/25:  12%|██▉                     | 15/125 [00:14<01:44,  1.05batch/s, Loss=0.716]\u001b[A\n",
      "Epoch 14/25:  12%|██▉                     | 15/125 [00:15<01:44,  1.05batch/s, Loss=0.728]\u001b[A\n",
      "Epoch 14/25:  13%|███                     | 16/125 [00:15<01:43,  1.05batch/s, Loss=0.728]\u001b[A\n",
      "Epoch 14/25:  13%|███                     | 16/125 [00:16<01:43,  1.05batch/s, Loss=0.732]\u001b[A\n",
      "Epoch 14/25:  14%|███▎                    | 17/125 [00:16<01:43,  1.05batch/s, Loss=0.732]\u001b[A\n",
      "Epoch 14/25:  14%|███▍                     | 17/125 [00:17<01:43,  1.05batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 14/25:  14%|███▌                     | 18/125 [00:17<01:42,  1.04batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 14/25:  14%|███▌                     | 18/125 [00:18<01:42,  1.04batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 14/25:  15%|███▊                     | 19/125 [00:18<01:41,  1.04batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 14/25:  15%|███▋                    | 19/125 [00:19<01:41,  1.04batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 14/25:  16%|███▊                    | 20/125 [00:19<01:40,  1.04batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 14/25:  16%|███▊                    | 20/125 [00:20<01:40,  1.04batch/s, Loss=0.783]\u001b[A\n",
      "Epoch 14/25:  17%|████                    | 21/125 [00:20<01:39,  1.04batch/s, Loss=0.783]\u001b[A\n",
      "Epoch 14/25:  17%|████                    | 21/125 [00:21<01:39,  1.04batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 14/25:  18%|████▏                   | 22/125 [00:21<01:38,  1.04batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 14/25:  18%|████▍                    | 22/125 [00:22<01:38,  1.04batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 14/25:  18%|████▌                    | 23/125 [00:22<01:37,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 14/25:  18%|████▍                   | 23/125 [00:23<01:37,  1.05batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 14/25:  19%|████▌                   | 24/125 [00:23<01:36,  1.05batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 14/25:  19%|████▌                   | 24/125 [00:24<01:36,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 14/25:  20%|████▊                   | 25/125 [00:24<01:35,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 14/25:  20%|████▊                   | 25/125 [00:25<01:35,  1.05batch/s, Loss=0.844]\u001b[A\n",
      "Epoch 14/25:  21%|████▉                   | 26/125 [00:25<01:34,  1.05batch/s, Loss=0.844]\u001b[A\n",
      "Epoch 14/25:  21%|████▉                   | 26/125 [00:26<01:34,  1.05batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 14/25:  22%|█████▏                  | 27/125 [00:26<01:33,  1.05batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 14/25:  22%|█████▏                  | 27/125 [00:27<01:33,  1.05batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 14/25:  22%|█████▍                  | 28/125 [00:27<01:32,  1.05batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 14/25:  22%|█████▍                  | 28/125 [00:28<01:32,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 14/25:  23%|█████▌                  | 29/125 [00:28<01:31,  1.04batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 14/25:  23%|█████▌                  | 29/125 [00:29<01:31,  1.04batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 14/25:  24%|█████▊                  | 30/125 [00:29<01:30,  1.05batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 14/25:  24%|█████▊                  | 30/125 [00:30<01:30,  1.05batch/s, Loss=0.494]\u001b[A\n",
      "Epoch 14/25:  25%|█████▉                  | 31/125 [00:30<01:29,  1.05batch/s, Loss=0.494]\u001b[A\n",
      "Epoch 14/25:  25%|█████▉                  | 31/125 [00:31<01:29,  1.05batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 14/25:  26%|██████▏                 | 32/125 [00:31<01:28,  1.05batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 14/25:  26%|██████▏                 | 32/125 [00:32<01:28,  1.05batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 14/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 14/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 14/25:  27%|██████▌                 | 34/125 [00:32<01:26,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 14/25:  27%|██████▌                 | 34/125 [00:33<01:26,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 14/25:  28%|██████▋                 | 35/125 [00:33<01:25,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 14/25:  28%|██████▋                 | 35/125 [00:34<01:25,  1.05batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 14/25:  29%|██████▉                 | 36/125 [00:34<01:24,  1.05batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 14/25:  29%|██████▉                 | 36/125 [00:35<01:24,  1.05batch/s, Loss=0.783]\u001b[A\n",
      "Epoch 14/25:  30%|███████                 | 37/125 [00:35<01:23,  1.05batch/s, Loss=0.783]\u001b[A\n",
      "Epoch 14/25:  30%|███████                 | 37/125 [00:36<01:23,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  30%|███████▎                | 38/125 [00:36<01:22,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  30%|███████▎                | 38/125 [00:37<01:22,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 14/25:  31%|███████▍                | 39/125 [00:37<01:21,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 14/25:  31%|███████▍                | 39/125 [00:38<01:21,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 14/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 14/25:  32%|███████▋                | 40/125 [00:39<01:20,  1.05batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 14/25:  33%|███████▊                | 41/125 [00:39<01:20,  1.05batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 14/25:  33%|███████▊                | 41/125 [00:40<01:20,  1.05batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 14/25:  34%|████████                | 42/125 [00:40<01:19,  1.05batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 14/25:  34%|████████                | 42/125 [00:41<01:19,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 14/25:  34%|████████▎               | 43/125 [00:41<01:18,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 14/25:  34%|████████▎               | 43/125 [00:42<01:18,  1.05batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 14/25:  35%|████████▍               | 44/125 [00:42<01:17,  1.05batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 14/25:  35%|████████▍               | 44/125 [00:43<01:17,  1.05batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 14/25:  36%|████████▋               | 45/125 [00:43<01:16,  1.05batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 14/25:  36%|████████▋               | 45/125 [00:44<01:16,  1.05batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 14/25:  37%|████████▊               | 46/125 [00:44<01:15,  1.05batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 14/25:  37%|█████████▏               | 46/125 [00:45<01:15,  1.05batch/s, Loss=0.54]\u001b[A\n",
      "Epoch 14/25:  38%|█████████▍               | 47/125 [00:45<01:14,  1.05batch/s, Loss=0.54]\u001b[A\n",
      "Epoch 14/25:  38%|█████████               | 47/125 [00:46<01:14,  1.05batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 14/25:  38%|█████████▏              | 48/125 [00:46<01:13,  1.05batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 14/25:  38%|█████████▏              | 48/125 [00:47<01:13,  1.05batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 14/25:  39%|█████████▍              | 49/125 [00:47<01:12,  1.05batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 14/25:  39%|█████████▊               | 49/125 [00:48<01:12,  1.05batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 14/25:  40%|██████████               | 50/125 [00:48<01:11,  1.05batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 14/25:  40%|█████████▌              | 50/125 [00:49<01:11,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 14/25:  41%|█████████▊              | 51/125 [00:49<01:10,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 14/25:  41%|██████████▏              | 51/125 [00:50<01:10,  1.05batch/s, Loss=0.41]\u001b[A\n",
      "Epoch 14/25:  42%|██████████▍              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.41]\u001b[A\n",
      "Epoch 14/25:  42%|█████████▉              | 52/125 [00:51<01:09,  1.05batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 14/25:  42%|██████████▏             | 53/125 [00:51<01:09,  1.04batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 14/25:  42%|██████████▌              | 53/125 [00:52<01:09,  1.04batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 14/25:  43%|██████████▊              | 54/125 [00:52<01:08,  1.04batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 14/25:  43%|██████████▎             | 54/125 [00:53<01:08,  1.04batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 14/25:  44%|██████████▌             | 55/125 [00:53<01:07,  1.04batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 14/25:  44%|██████████▌             | 55/125 [00:53<01:07,  1.04batch/s, Loss=0.773]\u001b[A\n",
      "Epoch 14/25:  45%|██████████▊             | 56/125 [00:53<01:06,  1.04batch/s, Loss=0.773]\u001b[A\n",
      "Epoch 14/25:  45%|██████████▊             | 56/125 [00:54<01:06,  1.04batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 14/25:  46%|██████████▉             | 57/125 [00:54<01:05,  1.04batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 14/25:  46%|██████████▉             | 57/125 [00:55<01:05,  1.04batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 14/25:  46%|███████████▏            | 58/125 [00:55<01:04,  1.04batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 14/25:  46%|███████████▏            | 58/125 [00:56<01:04,  1.04batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 14/25:  47%|███████████▎            | 59/125 [00:56<01:03,  1.04batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 14/25:  47%|███████████▎            | 59/125 [00:57<01:03,  1.04batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 14/25:  48%|███████████▌            | 60/125 [00:57<01:02,  1.05batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 14/25:  48%|███████████▌            | 60/125 [00:58<01:02,  1.05batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 14/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.05batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 14/25:  49%|████████████▏            | 61/125 [00:59<01:00,  1.05batch/s, Loss=0.74]\u001b[A\n",
      "Epoch 14/25:  50%|████████████▍            | 62/125 [00:59<01:00,  1.05batch/s, Loss=0.74]\u001b[A\n",
      "Epoch 14/25:  50%|███████████▉            | 62/125 [01:00<01:00,  1.05batch/s, Loss=0.609]\u001b[A\n",
      "Epoch 14/25:  50%|████████████            | 63/125 [01:00<00:59,  1.05batch/s, Loss=0.609]\u001b[A\n",
      "Epoch 14/25:  50%|████████████            | 63/125 [01:01<00:59,  1.05batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 14/25:  51%|████████████▎           | 64/125 [01:01<00:58,  1.05batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 14/25:  51%|████████████▎           | 64/125 [01:02<00:58,  1.05batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 14/25:  52%|████████████▍           | 65/125 [01:02<00:57,  1.04batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 14/25:  52%|████████████▍           | 65/125 [01:03<00:57,  1.04batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 14/25:  53%|████████████▋           | 66/125 [01:03<00:56,  1.04batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 14/25:  53%|████████████▋           | 66/125 [01:04<00:56,  1.04batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 14/25:  54%|████████████▊           | 67/125 [01:04<00:55,  1.05batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 14/25:  54%|█████████████▍           | 67/125 [01:05<00:55,  1.05batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 14/25:  54%|█████████████▌           | 68/125 [01:05<00:54,  1.04batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 14/25:  54%|█████████████           | 68/125 [01:06<00:54,  1.04batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  55%|█████████████▏          | 69/125 [01:06<00:53,  1.04batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  55%|█████████████▏          | 69/125 [01:07<00:53,  1.04batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 14/25:  56%|█████████████▍          | 70/125 [01:07<00:52,  1.04batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 14/25:  56%|█████████████▍          | 70/125 [01:08<00:52,  1.04batch/s, Loss=0.471]\u001b[A\n",
      "Epoch 14/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.04batch/s, Loss=0.471]\u001b[A\n",
      "Epoch 14/25:  57%|█████████████▋          | 71/125 [01:09<00:51,  1.04batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 14/25:  58%|█████████████▊          | 72/125 [01:09<00:50,  1.04batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 14/25:  58%|█████████████▊          | 72/125 [01:10<00:50,  1.04batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 14/25:  58%|██████████████          | 73/125 [01:10<00:49,  1.04batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 14/25:  58%|██████████████          | 73/125 [01:11<00:49,  1.04batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 14/25:  59%|██████████████▏         | 74/125 [01:11<00:49,  1.03batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 14/25:  59%|██████████████▏         | 74/125 [01:12<00:49,  1.03batch/s, Loss=0.739]\u001b[A\n",
      "Epoch 14/25:  60%|██████████████▍         | 75/125 [01:12<00:48,  1.03batch/s, Loss=0.739]\u001b[A\n",
      "Epoch 14/25:  60%|██████████████▍         | 75/125 [01:13<00:48,  1.03batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 14/25:  61%|██████████████▌         | 76/125 [01:13<00:47,  1.03batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 14/25:  61%|██████████████▌         | 76/125 [01:14<00:47,  1.03batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 14/25:  62%|██████████████▊         | 77/125 [01:14<00:46,  1.03batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 14/25:  62%|██████████████▊         | 77/125 [01:15<00:46,  1.03batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 14/25:  62%|██████████████▉         | 78/125 [01:15<00:45,  1.04batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 14/25:  62%|██████████████▉         | 78/125 [01:16<00:45,  1.04batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 14/25:  63%|███████████████▏        | 79/125 [01:16<00:44,  1.04batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 14/25:  63%|███████████████▏        | 79/125 [01:17<00:44,  1.04batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 14/25:  64%|███████████████▎        | 80/125 [01:17<00:43,  1.04batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 14/25:  64%|███████████████▎        | 80/125 [01:18<00:43,  1.04batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 14/25:  65%|███████████████▌        | 81/125 [01:18<00:42,  1.04batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 14/25:  65%|███████████████▌        | 81/125 [01:18<00:42,  1.04batch/s, Loss=0.677]\u001b[A\n",
      "Epoch 14/25:  66%|███████████████▋        | 82/125 [01:18<00:41,  1.04batch/s, Loss=0.677]\u001b[A\n",
      "Epoch 14/25:  66%|███████████████▋        | 82/125 [01:19<00:41,  1.04batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 14/25:  66%|███████████████▉        | 83/125 [01:19<00:40,  1.04batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 14/25:  66%|████████████████▌        | 83/125 [01:20<00:40,  1.04batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 14/25:  67%|████████████████▊        | 84/125 [01:20<00:39,  1.04batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 14/25:  67%|████████████████▏       | 84/125 [01:21<00:39,  1.04batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 14/25:  68%|████████████████▎       | 85/125 [01:21<00:38,  1.04batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 14/25:  68%|████████████████▎       | 85/125 [01:22<00:38,  1.04batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 14/25:  69%|████████████████▌       | 86/125 [01:22<00:37,  1.05batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 14/25:  69%|████████████████▌       | 86/125 [01:23<00:37,  1.05batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 14/25:  70%|████████████████▋       | 87/125 [01:23<00:36,  1.05batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 14/25:  70%|████████████████▋       | 87/125 [01:24<00:36,  1.05batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 14/25:  70%|████████████████▉       | 88/125 [01:24<00:35,  1.05batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 14/25:  70%|████████████████▉       | 88/125 [01:25<00:35,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 14/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 14/25:  71%|█████████████████       | 89/125 [01:26<00:34,  1.05batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 14/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.04batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 14/25:  72%|█████████████████▎      | 90/125 [01:27<00:33,  1.04batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 14/25:  73%|█████████████████▍      | 91/125 [01:27<00:32,  1.04batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 14/25:  73%|█████████████████▍      | 91/125 [01:28<00:32,  1.04batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 14/25:  74%|█████████████████▋      | 92/125 [01:28<00:31,  1.04batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 14/25:  74%|██████████████████▍      | 92/125 [01:29<00:31,  1.04batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 14/25:  74%|██████████████████▌      | 93/125 [01:29<00:30,  1.04batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 14/25:  74%|█████████████████▊      | 93/125 [01:30<00:30,  1.04batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 14/25:  75%|██████████████████      | 94/125 [01:30<00:29,  1.04batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 14/25:  75%|██████████████████      | 94/125 [01:31<00:29,  1.04batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 14/25:  76%|██████████████████▏     | 95/125 [01:31<00:28,  1.04batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 14/25:  76%|██████████████████▏     | 95/125 [01:32<00:28,  1.04batch/s, Loss=0.563]\u001b[A\n",
      "Epoch 14/25:  77%|██████████████████▍     | 96/125 [01:32<00:27,  1.04batch/s, Loss=0.563]\u001b[A\n",
      "Epoch 14/25:  77%|██████████████████▍     | 96/125 [01:33<00:27,  1.04batch/s, Loss=0.719]\u001b[A\n",
      "Epoch 14/25:  78%|██████████████████▌     | 97/125 [01:33<00:26,  1.05batch/s, Loss=0.719]\u001b[A\n",
      "Epoch 14/25:  78%|██████████████████▌     | 97/125 [01:34<00:26,  1.05batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 14/25:  78%|██████████████████▊     | 98/125 [01:34<00:25,  1.04batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 14/25:  78%|██████████████████▊     | 98/125 [01:35<00:25,  1.04batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 14/25:  79%|███████████████████     | 99/125 [01:35<00:24,  1.04batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 14/25:  79%|███████████████████     | 99/125 [01:36<00:24,  1.04batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 14/25:  80%|██████████████████▍    | 100/125 [01:36<00:23,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 14/25:  80%|███████████████████▏    | 100/125 [01:37<00:23,  1.05batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 14/25:  81%|███████████████████▍    | 101/125 [01:37<00:22,  1.05batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 14/25:  81%|██████████████████▌    | 101/125 [01:38<00:22,  1.05batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 14/25:  82%|██████████████████▊    | 102/125 [01:38<00:21,  1.05batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 14/25:  82%|██████████████████▊    | 102/125 [01:39<00:21,  1.05batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 14/25:  82%|██████████████████▉    | 103/125 [01:39<00:20,  1.05batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 14/25:  82%|███████████████████▊    | 103/125 [01:40<00:20,  1.05batch/s, Loss=0.77]\u001b[A\n",
      "Epoch 14/25:  83%|███████████████████▉    | 104/125 [01:40<00:19,  1.05batch/s, Loss=0.77]\u001b[A\n",
      "Epoch 14/25:  83%|███████████████████▏   | 104/125 [01:40<00:19,  1.05batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 14/25:  84%|███████████████████▎   | 105/125 [01:40<00:18,  1.05batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 14/25:  84%|███████████████████▎   | 105/125 [01:41<00:18,  1.05batch/s, Loss=0.404]\u001b[A\n",
      "Epoch 14/25:  85%|███████████████████▌   | 106/125 [01:41<00:18,  1.06batch/s, Loss=0.404]\u001b[A\n",
      "Epoch 14/25:  85%|███████████████████▌   | 106/125 [01:42<00:18,  1.06batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 14/25:  86%|███████████████████▋   | 107/125 [01:42<00:17,  1.05batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 14/25:  86%|████████████████████▌   | 107/125 [01:43<00:17,  1.05batch/s, Loss=0.54]\u001b[A\n",
      "Epoch 14/25:  86%|████████████████████▋   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.54]\u001b[A\n",
      "Epoch 14/25:  86%|███████████████████▊   | 108/125 [01:44<00:16,  1.05batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 14/25:  87%|████████████████████   | 109/125 [01:44<00:15,  1.05batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 14/25:  87%|████████████████████   | 109/125 [01:45<00:15,  1.05batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 14/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.05batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 14/25:  88%|████████████████████▏  | 110/125 [01:46<00:14,  1.05batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 14/25:  89%|████████████████████▍  | 111/125 [01:46<00:13,  1.05batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 14/25:  89%|████████████████████▍  | 111/125 [01:47<00:13,  1.05batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 14/25:  90%|████████████████████▌  | 112/125 [01:47<00:12,  1.05batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 14/25:  90%|████████████████████▌  | 112/125 [01:48<00:12,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 14/25:  90%|████████████████████▊  | 113/125 [01:48<00:11,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 14/25:  90%|████████████████████▊  | 113/125 [01:49<00:11,  1.05batch/s, Loss=0.671]\u001b[A\n",
      "Epoch 14/25:  91%|████████████████████▉  | 114/125 [01:49<00:10,  1.05batch/s, Loss=0.671]\u001b[A\n",
      "Epoch 14/25:  91%|████████████████████▉  | 114/125 [01:50<00:10,  1.05batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 14/25:  92%|█████████████████████▏ | 115/125 [01:50<00:09,  1.05batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 14/25:  92%|█████████████████████▏ | 115/125 [01:51<00:09,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 14/25:  93%|█████████████████████▎ | 116/125 [01:51<00:08,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 14/25:  93%|█████████████████████▎ | 116/125 [01:52<00:08,  1.05batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 14/25:  94%|█████████████████████▌ | 117/125 [01:52<00:07,  1.05batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 14/25:  94%|█████████████████████▌ | 117/125 [01:53<00:07,  1.05batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 14/25:  94%|█████████████████████▋ | 118/125 [01:53<00:06,  1.05batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 14/25:  94%|█████████████████████▋ | 118/125 [01:54<00:06,  1.05batch/s, Loss=0.689]\u001b[A\n",
      "Epoch 14/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.05batch/s, Loss=0.689]\u001b[A\n",
      "Epoch 14/25:  95%|█████████████████████▉ | 119/125 [01:55<00:05,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 14/25:  96%|██████████████████████ | 120/125 [01:56<00:04,  1.05batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 14/25:  97%|██████████████████████▎| 121/125 [01:56<00:03,  1.05batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 14/25:  97%|██████████████████████▎| 121/125 [01:57<00:03,  1.05batch/s, Loss=0.532]\u001b[A\n",
      "Epoch 14/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.05batch/s, Loss=0.532]\u001b[A\n",
      "Epoch 14/25:  98%|██████████████████████▍| 122/125 [01:58<00:02,  1.05batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 14/25:  98%|██████████████████████▋| 123/125 [01:58<00:01,  1.05batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 14/25:  98%|██████████████████████▋| 123/125 [01:59<00:01,  1.05batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 14/25:  99%|██████████████████████▊| 124/125 [01:59<00:00,  1.04batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 14/25:  99%|███████████████████████▊| 124/125 [02:00<00:00,  1.04batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 14/25: 100%|████████████████████████| 125/125 [02:00<00:00,  1.04batch/s, Loss=0.46]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 14/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   1%|▏                         | 1/125 [00:00<00:45,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   2%|▍                         | 2/125 [00:00<00:45,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   2%|▌                         | 3/125 [00:01<00:44,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   3%|▊                         | 4/125 [00:01<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   4%|█                         | 5/125 [00:01<00:43,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   8%|██                       | 10/125 [00:03<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  10%|██▍                      | 12/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  10%|██▌                      | 13/125 [00:04<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  12%|███                      | 15/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  13%|███▏                     | 16/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  14%|███▌                     | 18/125 [00:06<00:39,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  15%|███▊                     | 19/125 [00:06<00:38,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  16%|████                     | 20/125 [00:07<00:38,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  18%|████▍                    | 22/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  18%|████▌                    | 23/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  20%|█████                    | 25/125 [00:09<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  21%|█████▏                   | 26/125 [00:09<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  23%|█████▊                   | 29/125 [00:10<00:35,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  24%|██████                   | 30/125 [00:10<00:34,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  26%|██████▍                  | 32/125 [00:11<00:34,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  26%|██████▌                  | 33/125 [00:12<00:33,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  28%|███████                  | 35/125 [00:12<00:32,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  30%|███████▍                 | 37/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  32%|████████                 | 40/125 [00:14<00:31,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  36%|█████████                | 45/125 [00:16<00:29,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  38%|█████████▌               | 48/125 [00:17<00:28,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  40%|██████████               | 50/125 [00:18<00:27,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  41%|██████████▏              | 51/125 [00:18<00:27,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  44%|███████████              | 55/125 [00:20<00:25,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  47%|███████████▊             | 59/125 [00:21<00:24,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  48%|████████████             | 60/125 [00:21<00:23,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  50%|████████████▍            | 62/125 [00:22<00:23,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  50%|████████████▌            | 63/125 [00:23<00:22,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  59%|██████████████▊          | 74/125 [00:27<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  68%|█████████████████        | 85/125 [00:31<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  74%|██████████████████▍      | 92/125 [00:33<00:12,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  76%|███████████████████      | 95/125 [00:34<00:10,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  77%|███████████████████▏     | 96/125 [00:35<00:10,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  78%|███████████████████▍     | 97/125 [00:35<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  78%|███████████████████▌     | 98/125 [00:35<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  79%|███████████████████▊     | 99/125 [00:36<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  80%|███████████████████▏    | 100/125 [00:36<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  82%|███████████████████▊    | 103/125 [00:37<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  85%|████████████████████▎   | 106/125 [00:38<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  86%|████████████████████▌   | 107/125 [00:39<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  87%|████████████████████▉   | 109/125 [00:39<00:05,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  88%|█████████████████████   | 110/125 [00:40<00:05,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  89%|█████████████████████▎  | 111/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  90%|█████████████████████▋  | 113/125 [00:41<00:04,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  91%|█████████████████████▉  | 114/125 [00:41<00:04,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  92%|██████████████████████  | 115/125 [00:41<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  94%|██████████████████████▋ | 118/125 [00:43<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  97%|███████████████████████▏| 121/125 [00:44<00:01,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25:  99%|███████████████████████▊| 124/125 [00:45<00:00,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 14/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.74batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Eval Accuracy: 0.7380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 15/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 15/25:   1%|▏                        | 1/125 [00:01<03:22,  1.63s/batch, Loss=0.576]\u001b[A\n",
      "Epoch 15/25:   1%|▏                        | 1/125 [00:02<03:22,  1.63s/batch, Loss=0.635]\u001b[A\n",
      "Epoch 15/25:   2%|▍                        | 2/125 [00:02<02:32,  1.24s/batch, Loss=0.635]\u001b[A\n",
      "Epoch 15/25:   2%|▍                        | 2/125 [00:03<02:32,  1.24s/batch, Loss=0.515]\u001b[A\n",
      "Epoch 15/25:   2%|▌                        | 3/125 [00:03<02:14,  1.10s/batch, Loss=0.515]\u001b[A\n",
      "Epoch 15/25:   2%|▌                        | 3/125 [00:04<02:14,  1.10s/batch, Loss=0.517]\u001b[A\n",
      "Epoch 15/25:   3%|▊                        | 4/125 [00:04<02:06,  1.04s/batch, Loss=0.517]\u001b[A\n",
      "Epoch 15/25:   3%|▊                        | 4/125 [00:05<02:06,  1.04s/batch, Loss=0.607]\u001b[A\n",
      "Epoch 15/25:   4%|█                        | 5/125 [00:05<02:01,  1.01s/batch, Loss=0.607]\u001b[A\n",
      "Epoch 15/25:   4%|█                        | 5/125 [00:06<02:01,  1.01s/batch, Loss=0.563]\u001b[A\n",
      "Epoch 15/25:   5%|█▏                       | 6/125 [00:06<01:57,  1.01batch/s, Loss=0.563]\u001b[A\n",
      "Epoch 15/25:   5%|█▏                       | 6/125 [00:07<01:57,  1.01batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 15/25:   6%|█▍                       | 7/125 [00:07<01:55,  1.03batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 15/25:   6%|█▍                       | 7/125 [00:08<01:55,  1.03batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:   6%|█▌                       | 8/125 [00:08<01:53,  1.03batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:   6%|█▌                       | 8/125 [00:09<01:53,  1.03batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 15/25:   7%|█▊                       | 9/125 [00:09<01:51,  1.04batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 15/25:   7%|█▊                       | 9/125 [00:10<01:51,  1.04batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 15/25:   8%|█▉                      | 10/125 [00:10<01:50,  1.04batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 15/25:   8%|█▉                      | 10/125 [00:11<01:50,  1.04batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 15/25:   9%|██                      | 11/125 [00:11<01:48,  1.05batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 15/25:   9%|██                      | 11/125 [00:12<01:48,  1.05batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 15/25:  10%|██▎                     | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 15/25:  10%|██▎                     | 12/125 [00:13<01:47,  1.05batch/s, Loss=0.766]\u001b[A\n",
      "Epoch 15/25:  10%|██▍                     | 13/125 [00:13<01:46,  1.05batch/s, Loss=0.766]\u001b[A\n",
      "Epoch 15/25:  10%|██▍                     | 13/125 [00:13<01:46,  1.05batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 15/25:  11%|██▋                     | 14/125 [00:13<01:45,  1.05batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 15/25:  11%|██▋                     | 14/125 [00:14<01:45,  1.05batch/s, Loss=0.755]\u001b[A\n",
      "Epoch 15/25:  12%|██▉                     | 15/125 [00:14<01:44,  1.05batch/s, Loss=0.755]\u001b[A\n",
      "Epoch 15/25:  12%|██▉                     | 15/125 [00:15<01:44,  1.05batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 15/25:  13%|███                     | 16/125 [00:15<01:43,  1.06batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 15/25:  13%|███                     | 16/125 [00:16<01:43,  1.06batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 15/25:  14%|███▎                    | 17/125 [00:16<01:42,  1.05batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 15/25:  14%|███▎                    | 17/125 [00:17<01:42,  1.05batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 15/25:  14%|███▍                    | 18/125 [00:17<01:41,  1.05batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 15/25:  14%|███▍                    | 18/125 [00:18<01:41,  1.05batch/s, Loss=0.728]\u001b[A\n",
      "Epoch 15/25:  15%|███▋                    | 19/125 [00:18<01:40,  1.05batch/s, Loss=0.728]\u001b[A\n",
      "Epoch 15/25:  15%|███▋                    | 19/125 [00:19<01:40,  1.05batch/s, Loss=0.747]\u001b[A\n",
      "Epoch 15/25:  16%|███▊                    | 20/125 [00:19<01:39,  1.05batch/s, Loss=0.747]\u001b[A\n",
      "Epoch 15/25:  16%|███▊                    | 20/125 [00:20<01:39,  1.05batch/s, Loss=0.704]\u001b[A\n",
      "Epoch 15/25:  17%|████                    | 21/125 [00:20<01:38,  1.05batch/s, Loss=0.704]\u001b[A\n",
      "Epoch 15/25:  17%|████                    | 21/125 [00:21<01:38,  1.05batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 15/25:  18%|████▏                   | 22/125 [00:21<01:37,  1.05batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 15/25:  18%|████▏                   | 22/125 [00:22<01:37,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 15/25:  18%|████▍                   | 23/125 [00:22<01:37,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 15/25:  18%|████▍                   | 23/125 [00:23<01:37,  1.05batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 15/25:  19%|████▌                   | 24/125 [00:23<01:36,  1.05batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 15/25:  19%|████▌                   | 24/125 [00:24<01:36,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 15/25:  20%|████▊                   | 25/125 [00:24<01:35,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 15/25:  20%|████▊                   | 25/125 [00:25<01:35,  1.05batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 15/25:  21%|████▉                   | 26/125 [00:25<01:34,  1.05batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 15/25:  21%|█████▏                   | 26/125 [00:26<01:34,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 15/25:  22%|█████▍                   | 27/125 [00:26<01:33,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 15/25:  22%|█████▏                  | 27/125 [00:27<01:33,  1.05batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 15/25:  22%|█████▍                  | 28/125 [00:27<01:32,  1.05batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 15/25:  22%|█████▍                  | 28/125 [00:28<01:32,  1.05batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 15/25:  23%|█████▌                  | 29/125 [00:28<01:31,  1.05batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 15/25:  23%|█████▌                  | 29/125 [00:29<01:31,  1.05batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 15/25:  24%|█████▊                  | 30/125 [00:29<01:30,  1.05batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 15/25:  24%|█████▊                  | 30/125 [00:30<01:30,  1.05batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 15/25:  25%|█████▉                  | 31/125 [00:30<01:29,  1.05batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 15/25:  25%|█████▉                  | 31/125 [00:31<01:29,  1.05batch/s, Loss=0.708]\u001b[A\n",
      "Epoch 15/25:  26%|██████▏                 | 32/125 [00:31<01:28,  1.05batch/s, Loss=0.708]\u001b[A\n",
      "Epoch 15/25:  26%|██████▏                 | 32/125 [00:32<01:28,  1.05batch/s, Loss=0.736]\u001b[A\n",
      "Epoch 15/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.736]\u001b[A\n",
      "Epoch 15/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.569]\u001b[A\n",
      "Epoch 15/25:  27%|██████▌                 | 34/125 [00:32<01:26,  1.05batch/s, Loss=0.569]\u001b[A\n",
      "Epoch 15/25:  27%|██████▌                 | 34/125 [00:33<01:26,  1.05batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 15/25:  28%|██████▋                 | 35/125 [00:33<01:25,  1.05batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 15/25:  28%|██████▋                 | 35/125 [00:34<01:25,  1.05batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 15/25:  29%|██████▉                 | 36/125 [00:34<01:24,  1.06batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 15/25:  29%|██████▉                 | 36/125 [00:35<01:24,  1.06batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 15/25:  30%|███████                 | 37/125 [00:35<01:23,  1.06batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 15/25:  30%|███████                 | 37/125 [00:36<01:23,  1.06batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 15/25:  30%|███████▎                | 38/125 [00:36<01:22,  1.05batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 15/25:  30%|███████▎                | 38/125 [00:37<01:22,  1.05batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 15/25:  31%|███████▍                | 39/125 [00:37<01:21,  1.06batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 15/25:  31%|███████▍                | 39/125 [00:38<01:21,  1.06batch/s, Loss=0.427]\u001b[A\n",
      "Epoch 15/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.05batch/s, Loss=0.427]\u001b[A\n",
      "Epoch 15/25:  32%|████████                 | 40/125 [00:39<01:20,  1.05batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 15/25:  33%|████████▏                | 41/125 [00:39<01:19,  1.05batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 15/25:  33%|███████▊                | 41/125 [00:40<01:19,  1.05batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 15/25:  34%|████████                | 42/125 [00:40<01:18,  1.06batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 15/25:  34%|████████                | 42/125 [00:41<01:18,  1.06batch/s, Loss=0.731]\u001b[A\n",
      "Epoch 15/25:  34%|████████▎               | 43/125 [00:41<01:17,  1.06batch/s, Loss=0.731]\u001b[A\n",
      "Epoch 15/25:  34%|████████▎               | 43/125 [00:42<01:17,  1.06batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 15/25:  35%|████████▍               | 44/125 [00:42<01:16,  1.05batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 15/25:  35%|████████▍               | 44/125 [00:43<01:16,  1.05batch/s, Loss=0.478]\u001b[A\n",
      "Epoch 15/25:  36%|████████▋               | 45/125 [00:43<01:15,  1.05batch/s, Loss=0.478]\u001b[A\n",
      "Epoch 15/25:  36%|████████▋               | 45/125 [00:44<01:15,  1.05batch/s, Loss=0.429]\u001b[A\n",
      "Epoch 15/25:  37%|████████▊               | 46/125 [00:44<01:14,  1.05batch/s, Loss=0.429]\u001b[A\n",
      "Epoch 15/25:  37%|████████▊               | 46/125 [00:45<01:14,  1.05batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 15/25:  38%|█████████               | 47/125 [00:45<01:13,  1.06batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 15/25:  38%|█████████               | 47/125 [00:46<01:13,  1.06batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 15/25:  38%|█████████▏              | 48/125 [00:46<01:12,  1.06batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 15/25:  38%|█████████▏              | 48/125 [00:47<01:12,  1.06batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 15/25:  39%|█████████▍              | 49/125 [00:47<01:12,  1.05batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 15/25:  39%|█████████▍              | 49/125 [00:48<01:12,  1.05batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 15/25:  40%|█████████▌              | 50/125 [00:48<01:11,  1.06batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 15/25:  40%|██████████               | 50/125 [00:49<01:11,  1.06batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 15/25:  41%|██████████▏              | 51/125 [00:49<01:10,  1.06batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 15/25:  41%|█████████▊              | 51/125 [00:50<01:10,  1.06batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 15/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 15/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.868]\u001b[A\n",
      "Epoch 15/25:  42%|██████████▏             | 53/125 [00:50<01:08,  1.05batch/s, Loss=0.868]\u001b[A\n",
      "Epoch 15/25:  42%|██████████▏             | 53/125 [00:51<01:08,  1.05batch/s, Loss=0.683]\u001b[A\n",
      "Epoch 15/25:  43%|██████████▎             | 54/125 [00:51<01:07,  1.05batch/s, Loss=0.683]\u001b[A\n",
      "Epoch 15/25:  43%|██████████▎             | 54/125 [00:52<01:07,  1.05batch/s, Loss=0.693]\u001b[A\n",
      "Epoch 15/25:  44%|██████████▌             | 55/125 [00:52<01:06,  1.05batch/s, Loss=0.693]\u001b[A\n",
      "Epoch 15/25:  44%|██████████▌             | 55/125 [00:53<01:06,  1.05batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 15/25:  45%|██████████▊             | 56/125 [00:53<01:05,  1.05batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 15/25:  45%|██████████▊             | 56/125 [00:54<01:05,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:  46%|██████████▉             | 57/125 [00:54<01:04,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:  46%|██████████▉             | 57/125 [00:55<01:04,  1.05batch/s, Loss=0.902]\u001b[A\n",
      "Epoch 15/25:  46%|███████████▏            | 58/125 [00:55<01:03,  1.05batch/s, Loss=0.902]\u001b[A\n",
      "Epoch 15/25:  46%|███████████▏            | 58/125 [00:56<01:03,  1.05batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 15/25:  47%|███████████▎            | 59/125 [00:56<01:02,  1.05batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 15/25:  47%|███████████▊             | 59/125 [00:57<01:02,  1.05batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 15/25:  48%|████████████             | 60/125 [00:57<01:01,  1.05batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 15/25:  48%|███████████▌            | 60/125 [00:58<01:01,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 15/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 15/25:  49%|███████████▋            | 61/125 [00:59<01:00,  1.05batch/s, Loss=0.463]\u001b[A\n",
      "Epoch 15/25:  50%|███████████▉            | 62/125 [00:59<00:59,  1.05batch/s, Loss=0.463]\u001b[A\n",
      "Epoch 15/25:  50%|███████████▉            | 62/125 [01:00<00:59,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 15/25:  50%|████████████            | 63/125 [01:00<00:58,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 15/25:  50%|████████████            | 63/125 [01:01<00:58,  1.05batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 15/25:  51%|████████████▎           | 64/125 [01:01<00:57,  1.05batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 15/25:  51%|████████████▎           | 64/125 [01:02<00:57,  1.05batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 15/25:  52%|████████████▍           | 65/125 [01:02<00:56,  1.05batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 15/25:  52%|████████████▍           | 65/125 [01:03<00:56,  1.05batch/s, Loss=0.812]\u001b[A\n",
      "Epoch 15/25:  53%|████████████▋           | 66/125 [01:03<00:55,  1.05batch/s, Loss=0.812]\u001b[A\n",
      "Epoch 15/25:  53%|████████████▋           | 66/125 [01:04<00:55,  1.05batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 15/25:  54%|████████████▊           | 67/125 [01:04<00:55,  1.05batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 15/25:  54%|████████████▊           | 67/125 [01:05<00:55,  1.05batch/s, Loss=0.376]\u001b[A\n",
      "Epoch 15/25:  54%|█████████████           | 68/125 [01:05<00:54,  1.05batch/s, Loss=0.376]\u001b[A\n",
      "Epoch 15/25:  54%|█████████████           | 68/125 [01:06<00:54,  1.05batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 15/25:  55%|█████████████▏          | 69/125 [01:06<00:53,  1.05batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 15/25:  55%|█████████████▏          | 69/125 [01:07<00:53,  1.05batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 15/25:  56%|█████████████▍          | 70/125 [01:07<00:52,  1.05batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 15/25:  56%|█████████████▍          | 70/125 [01:08<00:52,  1.05batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 15/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.05batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 15/25:  57%|█████████████▋          | 71/125 [01:09<00:51,  1.05batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 15/25:  58%|█████████████▊          | 72/125 [01:09<00:50,  1.05batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 15/25:  58%|█████████████▊          | 72/125 [01:10<00:50,  1.05batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 15/25:  58%|██████████████          | 73/125 [01:10<00:49,  1.05batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 15/25:  58%|██████████████          | 73/125 [01:10<00:49,  1.05batch/s, Loss=0.466]\u001b[A\n",
      "Epoch 15/25:  59%|██████████████▏         | 74/125 [01:10<00:48,  1.05batch/s, Loss=0.466]\u001b[A\n",
      "Epoch 15/25:  59%|██████████████▏         | 74/125 [01:11<00:48,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 15/25:  60%|██████████████▍         | 75/125 [01:11<00:47,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 15/25:  60%|██████████████▍         | 75/125 [01:12<00:47,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 15/25:  61%|██████████████▌         | 76/125 [01:12<00:46,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 15/25:  61%|██████████████▌         | 76/125 [01:13<00:46,  1.05batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 15/25:  62%|██████████████▊         | 77/125 [01:13<00:45,  1.05batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 15/25:  62%|███████████████▍         | 77/125 [01:14<00:45,  1.05batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 15/25:  62%|███████████████▌         | 78/125 [01:14<00:44,  1.05batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 15/25:  62%|██████████████▉         | 78/125 [01:15<00:44,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:  63%|███████████████▏        | 79/125 [01:15<00:43,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 15/25:  63%|███████████████▏        | 79/125 [01:16<00:43,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 15/25:  64%|███████████████▎        | 80/125 [01:16<00:42,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 15/25:  64%|███████████████▎        | 80/125 [01:17<00:42,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 15/25:  65%|███████████████▌        | 81/125 [01:17<00:41,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 15/25:  65%|███████████████▌        | 81/125 [01:18<00:41,  1.05batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 15/25:  66%|███████████████▋        | 82/125 [01:18<00:40,  1.05batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 15/25:  66%|███████████████▋        | 82/125 [01:19<00:40,  1.05batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 15/25:  66%|███████████████▉        | 83/125 [01:19<00:39,  1.05batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 15/25:  66%|███████████████▉        | 83/125 [01:20<00:39,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 15/25:  67%|████████████████▏       | 84/125 [01:20<00:38,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 15/25:  67%|████████████████▊        | 84/125 [01:21<00:38,  1.05batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 15/25:  68%|█████████████████        | 85/125 [01:21<00:37,  1.05batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 15/25:  68%|█████████████████        | 85/125 [01:22<00:37,  1.05batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 15/25:  69%|█████████████████▏       | 86/125 [01:22<00:37,  1.05batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 15/25:  69%|████████████████▌       | 86/125 [01:23<00:37,  1.05batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 15/25:  70%|████████████████▋       | 87/125 [01:23<00:36,  1.05batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 15/25:  70%|████████████████▋       | 87/125 [01:24<00:36,  1.05batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 15/25:  70%|████████████████▉       | 88/125 [01:24<00:35,  1.05batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 15/25:  70%|████████████████▉       | 88/125 [01:25<00:35,  1.05batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 15/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.05batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 15/25:  71%|█████████████████       | 89/125 [01:26<00:34,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 15/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.05batch/s, Loss=0.616]\u001b[A\n",
      "Epoch 15/25:  72%|█████████████████▎      | 90/125 [01:27<00:33,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 15/25:  73%|█████████████████▍      | 91/125 [01:27<00:32,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 15/25:  73%|██████████████████▏      | 91/125 [01:28<00:32,  1.05batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 15/25:  74%|██████████████████▍      | 92/125 [01:28<00:31,  1.05batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 15/25:  74%|█████████████████▋      | 92/125 [01:29<00:31,  1.05batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 15/25:  74%|█████████████████▊      | 93/125 [01:29<00:30,  1.05batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 15/25:  74%|█████████████████▊      | 93/125 [01:30<00:30,  1.05batch/s, Loss=0.744]\u001b[A\n",
      "Epoch 15/25:  75%|██████████████████      | 94/125 [01:30<00:29,  1.05batch/s, Loss=0.744]\u001b[A\n",
      "Epoch 15/25:  75%|██████████████████      | 94/125 [01:30<00:29,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 15/25:  76%|██████████████████▏     | 95/125 [01:30<00:28,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 15/25:  76%|██████████████████▏     | 95/125 [01:31<00:28,  1.05batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 15/25:  77%|██████████████████▍     | 96/125 [01:31<00:27,  1.05batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 15/25:  77%|██████████████████▍     | 96/125 [01:32<00:27,  1.05batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 15/25:  78%|██████████████████▌     | 97/125 [01:32<00:26,  1.05batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 15/25:  78%|██████████████████▌     | 97/125 [01:33<00:26,  1.05batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 15/25:  78%|██████████████████▊     | 98/125 [01:33<00:25,  1.04batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 15/25:  78%|██████████████████▊     | 98/125 [01:34<00:25,  1.04batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 15/25:  79%|███████████████████     | 99/125 [01:34<00:24,  1.04batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 15/25:  79%|███████████████████     | 99/125 [01:35<00:24,  1.04batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 15/25:  80%|██████████████████▍    | 100/125 [01:35<00:24,  1.04batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 15/25:  80%|██████████████████▍    | 100/125 [01:36<00:24,  1.04batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 15/25:  81%|██████████████████▌    | 101/125 [01:36<00:22,  1.04batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 15/25:  81%|██████████████████▌    | 101/125 [01:37<00:22,  1.04batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 15/25:  82%|██████████████████▊    | 102/125 [01:37<00:22,  1.04batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 15/25:  82%|██████████████████▊    | 102/125 [01:38<00:22,  1.04batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 15/25:  82%|██████████████████▉    | 103/125 [01:38<00:21,  1.04batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 15/25:  82%|██████████████████▉    | 103/125 [01:39<00:21,  1.04batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 15/25:  83%|███████████████████▏   | 104/125 [01:39<00:20,  1.05batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 15/25:  83%|███████████████████▏   | 104/125 [01:40<00:20,  1.05batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 15/25:  84%|███████████████████▎   | 105/125 [01:40<00:19,  1.05batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 15/25:  84%|███████████████████▎   | 105/125 [01:41<00:19,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 15/25:  85%|███████████████████▌   | 106/125 [01:41<00:18,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 15/25:  85%|███████████████████▌   | 106/125 [01:42<00:18,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 15/25:  86%|███████████████████▋   | 107/125 [01:42<00:17,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 15/25:  86%|███████████████████▋   | 107/125 [01:43<00:17,  1.05batch/s, Loss=0.807]\u001b[A\n",
      "Epoch 15/25:  86%|███████████████████▊   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.807]\u001b[A\n",
      "Epoch 15/25:  86%|████████████████████▋   | 108/125 [01:44<00:16,  1.05batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 15/25:  87%|████████████████████▉   | 109/125 [01:44<00:15,  1.05batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 15/25:  87%|████████████████████   | 109/125 [01:45<00:15,  1.05batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 15/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.05batch/s, Loss=0.601]\u001b[A\n",
      "Epoch 15/25:  88%|████████████████████▏  | 110/125 [01:46<00:14,  1.05batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 15/25:  89%|████████████████████▍  | 111/125 [01:46<00:13,  1.05batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 15/25:  89%|████████████████████▍  | 111/125 [01:47<00:13,  1.05batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 15/25:  90%|████████████████████▌  | 112/125 [01:47<00:12,  1.05batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 15/25:  90%|████████████████████▌  | 112/125 [01:48<00:12,  1.05batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 15/25:  90%|████████████████████▊  | 113/125 [01:48<00:11,  1.05batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 15/25:  90%|█████████████████████▋  | 113/125 [01:49<00:11,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 15/25:  91%|█████████████████████▉  | 114/125 [01:49<00:10,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 15/25:  91%|████████████████████▉  | 114/125 [01:50<00:10,  1.05batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 15/25:  92%|█████████████████████▏ | 115/125 [01:50<00:09,  1.04batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 15/25:  92%|██████████████████████  | 115/125 [01:51<00:09,  1.04batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 15/25:  93%|██████████████████████▎ | 116/125 [01:51<00:08,  1.05batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 15/25:  93%|█████████████████████▎ | 116/125 [01:51<00:08,  1.05batch/s, Loss=0.753]\u001b[A\n",
      "Epoch 15/25:  94%|█████████████████████▌ | 117/125 [01:51<00:07,  1.05batch/s, Loss=0.753]\u001b[A\n",
      "Epoch 15/25:  94%|█████████████████████▌ | 117/125 [01:52<00:07,  1.05batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 15/25:  94%|█████████████████████▋ | 118/125 [01:52<00:06,  1.05batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 15/25:  94%|█████████████████████▋ | 118/125 [01:53<00:06,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 15/25:  95%|█████████████████████▉ | 119/125 [01:53<00:05,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 15/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 15/25:  96%|██████████████████████ | 120/125 [01:54<00:04,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 15/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.05batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 15/25:  97%|██████████████████████▎| 121/125 [01:55<00:03,  1.05batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 15/25:  97%|██████████████████████▎| 121/125 [01:56<00:03,  1.05batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 15/25:  98%|██████████████████████▍| 122/125 [01:56<00:02,  1.05batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 15/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.05batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 15/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.05batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 15/25:  98%|██████████████████████▋| 123/125 [01:58<00:01,  1.05batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 15/25:  99%|██████████████████████▊| 124/125 [01:58<00:00,  1.05batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 15/25:  99%|██████████████████████▊| 124/125 [01:59<00:00,  1.05batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 15/25: 100%|███████████████████████| 125/125 [01:59<00:00,  1.05batch/s, Loss=0.515]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 15/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   1%|▏                         | 1/125 [00:00<00:45,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   2%|▍                         | 2/125 [00:00<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   2%|▌                         | 3/125 [00:01<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   3%|▊                         | 4/125 [00:01<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   4%|█                         | 5/125 [00:01<00:43,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   8%|██                       | 10/125 [00:03<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  10%|██▍                      | 12/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  10%|██▌                      | 13/125 [00:04<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  12%|███                      | 15/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  13%|███▏                     | 16/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  15%|███▊                     | 19/125 [00:06<00:38,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  16%|████                     | 20/125 [00:07<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  18%|████▍                    | 22/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  18%|████▌                    | 23/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  20%|█████                    | 25/125 [00:09<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  21%|█████▏                   | 26/125 [00:09<00:36,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  24%|██████                   | 30/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  26%|██████▌                  | 33/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  28%|███████                  | 35/125 [00:12<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  30%|███████▍                 | 37/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  32%|████████                 | 40/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  36%|█████████                | 45/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  38%|█████████▌               | 48/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  40%|██████████               | 50/125 [00:18<00:27,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  44%|███████████              | 55/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  47%|███████████▊             | 59/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  48%|████████████             | 60/125 [00:21<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  74%|██████████████████▍      | 92/125 [00:33<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  76%|███████████████████      | 95/125 [00:34<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  78%|███████████████████▍     | 97/125 [00:35<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  78%|███████████████████▌     | 98/125 [00:35<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  79%|███████████████████▊     | 99/125 [00:36<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  80%|███████████████████▏    | 100/125 [00:36<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  82%|███████████████████▊    | 103/125 [00:37<00:08,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  85%|████████████████████▎   | 106/125 [00:38<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  87%|████████████████████▉   | 109/125 [00:39<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  88%|█████████████████████   | 110/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  89%|█████████████████████▎  | 111/125 [00:40<00:05,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  90%|█████████████████████▋  | 113/125 [00:41<00:04,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  91%|█████████████████████▉  | 114/125 [00:41<00:04,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  92%|██████████████████████  | 115/125 [00:41<00:03,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  97%|███████████████████████▏| 121/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25:  99%|███████████████████████▊| 124/125 [00:45<00:00,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 15/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.75batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Eval Accuracy: 0.7380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 16/25:   0%|                                  | 0/125 [00:01<?, ?batch/s, Loss=0.49]\u001b[A\n",
      "Epoch 16/25:   1%|▏                         | 1/125 [00:01<02:09,  1.04s/batch, Loss=0.49]\u001b[A\n",
      "Epoch 16/25:   1%|▏                        | 1/125 [00:01<02:09,  1.04s/batch, Loss=0.579]\u001b[A\n",
      "Epoch 16/25:   2%|▍                        | 2/125 [00:01<02:01,  1.01batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 16/25:   2%|▍                        | 2/125 [00:02<02:01,  1.01batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 16/25:   2%|▌                        | 3/125 [00:02<01:58,  1.03batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 16/25:   2%|▌                         | 3/125 [00:03<01:58,  1.03batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 16/25:   3%|▊                         | 4/125 [00:03<01:56,  1.04batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 16/25:   3%|▊                        | 4/125 [00:04<01:56,  1.04batch/s, Loss=0.437]\u001b[A\n",
      "Epoch 16/25:   4%|█                        | 5/125 [00:04<01:54,  1.05batch/s, Loss=0.437]\u001b[A\n",
      "Epoch 16/25:   4%|█                        | 5/125 [00:05<01:54,  1.05batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 16/25:   5%|█▏                       | 6/125 [00:05<01:53,  1.05batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 16/25:   5%|█▏                        | 6/125 [00:06<01:53,  1.05batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 16/25:   6%|█▍                        | 7/125 [00:06<01:51,  1.05batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 16/25:   6%|█▍                       | 7/125 [00:07<01:51,  1.05batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 16/25:   6%|█▌                       | 8/125 [00:07<01:50,  1.06batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 16/25:   6%|█▌                       | 8/125 [00:08<01:50,  1.06batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 16/25:   7%|█▊                       | 9/125 [00:08<01:49,  1.06batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 16/25:   7%|█▊                       | 9/125 [00:09<01:49,  1.06batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 16/25:   8%|█▉                      | 10/125 [00:09<01:48,  1.06batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 16/25:   8%|█▉                      | 10/125 [00:10<01:48,  1.06batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 16/25:   9%|██                      | 11/125 [00:10<01:47,  1.06batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 16/25:   9%|██                      | 11/125 [00:11<01:47,  1.06batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 16/25:  10%|██▎                     | 12/125 [00:11<01:46,  1.06batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 16/25:  10%|██▎                     | 12/125 [00:12<01:46,  1.06batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 16/25:  10%|██▍                     | 13/125 [00:12<01:45,  1.06batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 16/25:  10%|██▌                      | 13/125 [00:13<01:45,  1.06batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 16/25:  11%|██▊                      | 14/125 [00:13<01:44,  1.06batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 16/25:  11%|██▋                     | 14/125 [00:14<01:44,  1.06batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 16/25:  12%|██▉                     | 15/125 [00:14<01:44,  1.06batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 16/25:  12%|██▉                     | 15/125 [00:15<01:44,  1.06batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 16/25:  13%|███                     | 16/125 [00:15<01:43,  1.06batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 16/25:  13%|███                     | 16/125 [00:16<01:43,  1.06batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 16/25:  14%|███▎                    | 17/125 [00:16<01:42,  1.05batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 16/25:  14%|███▎                    | 17/125 [00:17<01:42,  1.05batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 16/25:  14%|███▍                    | 18/125 [00:17<01:41,  1.05batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 16/25:  14%|███▍                    | 18/125 [00:18<01:41,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 16/25:  15%|███▋                    | 19/125 [00:18<01:40,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 16/25:  15%|███▋                    | 19/125 [00:19<01:40,  1.05batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 16/25:  16%|███▊                    | 20/125 [00:19<01:39,  1.05batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 16/25:  16%|████                     | 20/125 [00:19<01:39,  1.05batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 16/25:  17%|████▏                    | 21/125 [00:19<01:38,  1.05batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 16/25:  17%|████                    | 21/125 [00:20<01:38,  1.05batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 16/25:  18%|████▏                   | 22/125 [00:20<01:37,  1.05batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 16/25:  18%|████▍                    | 22/125 [00:21<01:37,  1.05batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 16/25:  18%|████▌                    | 23/125 [00:21<01:36,  1.05batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 16/25:  18%|████▍                   | 23/125 [00:22<01:36,  1.05batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 16/25:  19%|████▌                   | 24/125 [00:22<01:35,  1.05batch/s, Loss=0.622]\u001b[A\n",
      "Epoch 16/25:  19%|████▌                   | 24/125 [00:23<01:35,  1.05batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 16/25:  20%|████▊                   | 25/125 [00:23<01:35,  1.05batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 16/25:  20%|████▊                   | 25/125 [00:24<01:35,  1.05batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 16/25:  21%|████▉                   | 26/125 [00:24<01:34,  1.05batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 16/25:  21%|████▉                   | 26/125 [00:25<01:34,  1.05batch/s, Loss=0.421]\u001b[A\n",
      "Epoch 16/25:  22%|█████▏                  | 27/125 [00:25<01:33,  1.05batch/s, Loss=0.421]\u001b[A\n",
      "Epoch 16/25:  22%|█████▏                  | 27/125 [00:26<01:33,  1.05batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 16/25:  22%|█████▍                  | 28/125 [00:26<01:33,  1.04batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 16/25:  22%|█████▍                  | 28/125 [00:27<01:33,  1.04batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 16/25:  23%|█████▌                  | 29/125 [00:27<01:31,  1.05batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 16/25:  23%|█████▌                  | 29/125 [00:28<01:31,  1.05batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 16/25:  24%|█████▊                  | 30/125 [00:28<01:30,  1.05batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 16/25:  24%|█████▊                  | 30/125 [00:29<01:30,  1.05batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 16/25:  25%|█████▉                  | 31/125 [00:29<01:29,  1.05batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 16/25:  25%|█████▉                  | 31/125 [00:30<01:29,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 16/25:  26%|██████▏                 | 32/125 [00:30<01:28,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 16/25:  26%|██████▏                 | 32/125 [00:31<01:28,  1.05batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 16/25:  26%|██████▎                 | 33/125 [00:31<01:27,  1.05batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 16/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.896]\u001b[A\n",
      "Epoch 16/25:  27%|██████▌                 | 34/125 [00:32<01:26,  1.05batch/s, Loss=0.896]\u001b[A\n",
      "Epoch 16/25:  27%|██████▌                 | 34/125 [00:33<01:26,  1.05batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 16/25:  28%|██████▋                 | 35/125 [00:33<01:25,  1.05batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 16/25:  28%|██████▋                 | 35/125 [00:34<01:25,  1.05batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 16/25:  29%|██████▉                 | 36/125 [00:34<01:24,  1.06batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 16/25:  29%|██████▉                 | 36/125 [00:35<01:24,  1.06batch/s, Loss=0.404]\u001b[A\n",
      "Epoch 16/25:  30%|███████                 | 37/125 [00:35<01:23,  1.06batch/s, Loss=0.404]\u001b[A\n",
      "Epoch 16/25:  30%|███████                 | 37/125 [00:36<01:23,  1.06batch/s, Loss=0.834]\u001b[A\n",
      "Epoch 16/25:  30%|███████▎                | 38/125 [00:36<01:22,  1.05batch/s, Loss=0.834]\u001b[A\n",
      "Epoch 16/25:  30%|███████▎                | 38/125 [00:37<01:22,  1.05batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 16/25:  31%|███████▍                | 39/125 [00:37<01:21,  1.06batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 16/25:  31%|███████▍                | 39/125 [00:38<01:21,  1.06batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 16/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.06batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 16/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.06batch/s, Loss=0.364]\u001b[A\n",
      "Epoch 16/25:  33%|███████▊                | 41/125 [00:38<01:19,  1.06batch/s, Loss=0.364]\u001b[A\n",
      "Epoch 16/25:  33%|███████▊                | 41/125 [00:39<01:19,  1.06batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 16/25:  34%|████████                | 42/125 [00:39<01:18,  1.05batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 16/25:  34%|████████▋                 | 42/125 [00:40<01:18,  1.05batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 16/25:  34%|████████▉                 | 43/125 [00:40<01:17,  1.05batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 16/25:  34%|████████▎               | 43/125 [00:41<01:17,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 16/25:  35%|████████▍               | 44/125 [00:41<01:16,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 16/25:  35%|████████▍               | 44/125 [00:42<01:16,  1.05batch/s, Loss=0.363]\u001b[A\n",
      "Epoch 16/25:  36%|████████▋               | 45/125 [00:42<01:15,  1.06batch/s, Loss=0.363]\u001b[A\n",
      "Epoch 16/25:  36%|████████▋               | 45/125 [00:43<01:15,  1.06batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 16/25:  37%|████████▊               | 46/125 [00:43<01:14,  1.05batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 16/25:  37%|████████▊               | 46/125 [00:44<01:14,  1.05batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 16/25:  38%|█████████               | 47/125 [00:44<01:13,  1.05batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 16/25:  38%|█████████               | 47/125 [00:45<01:13,  1.05batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 16/25:  38%|█████████▏              | 48/125 [00:45<01:12,  1.05batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 16/25:  38%|█████████▏              | 48/125 [00:46<01:12,  1.05batch/s, Loss=0.471]\u001b[A\n",
      "Epoch 16/25:  39%|█████████▍              | 49/125 [00:46<01:12,  1.05batch/s, Loss=0.471]\u001b[A\n",
      "Epoch 16/25:  39%|█████████▍              | 49/125 [00:47<01:12,  1.05batch/s, Loss=0.848]\u001b[A\n",
      "Epoch 16/25:  40%|█████████▌              | 50/125 [00:47<01:11,  1.05batch/s, Loss=0.848]\u001b[A\n",
      "Epoch 16/25:  40%|█████████▌              | 50/125 [00:48<01:11,  1.05batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 16/25:  41%|█████████▊              | 51/125 [00:48<01:10,  1.05batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 16/25:  41%|█████████▊              | 51/125 [00:49<01:10,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  42%|█████████▉              | 52/125 [00:49<01:09,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 16/25:  42%|██████████▏             | 53/125 [00:50<01:08,  1.05batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 16/25:  42%|██████████▏             | 53/125 [00:51<01:08,  1.05batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 16/25:  43%|██████████▎             | 54/125 [00:51<01:07,  1.05batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 16/25:  43%|██████████▎             | 54/125 [00:52<01:07,  1.05batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 16/25:  44%|██████████▌             | 55/125 [00:52<01:06,  1.05batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 16/25:  44%|██████████▌             | 55/125 [00:53<01:06,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 16/25:  45%|██████████▊             | 56/125 [00:53<01:05,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 16/25:  45%|███████████▏             | 56/125 [00:54<01:05,  1.05batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 16/25:  46%|███████████▍             | 57/125 [00:54<01:05,  1.04batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 16/25:  46%|██████████▉             | 57/125 [00:55<01:05,  1.04batch/s, Loss=0.721]\u001b[A\n",
      "Epoch 16/25:  46%|███████████▏            | 58/125 [00:55<01:04,  1.04batch/s, Loss=0.721]\u001b[A\n",
      "Epoch 16/25:  46%|███████████▏            | 58/125 [00:56<01:04,  1.04batch/s, Loss=0.723]\u001b[A\n",
      "Epoch 16/25:  47%|███████████▎            | 59/125 [00:56<01:03,  1.04batch/s, Loss=0.723]\u001b[A\n",
      "Epoch 16/25:  47%|███████████▎            | 59/125 [00:57<01:03,  1.04batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 16/25:  48%|███████████▌            | 60/125 [00:57<01:02,  1.05batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 16/25:  48%|███████████▌            | 60/125 [00:58<01:02,  1.05batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 16/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.05batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 16/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.05batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 16/25:  50%|███████████▉            | 62/125 [00:58<00:59,  1.05batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 16/25:  50%|███████████▉            | 62/125 [00:59<00:59,  1.05batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 16/25:  50%|████████████            | 63/125 [00:59<00:59,  1.05batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 16/25:  50%|████████████            | 63/125 [01:00<00:59,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  51%|████████████▎           | 64/125 [01:00<00:58,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  51%|████████████▎           | 64/125 [01:01<00:58,  1.05batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 16/25:  52%|████████████▍           | 65/125 [01:01<00:57,  1.05batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 16/25:  52%|████████████▍           | 65/125 [01:02<00:57,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 16/25:  53%|████████████▋           | 66/125 [01:02<00:56,  1.05batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 16/25:  53%|████████████▋           | 66/125 [01:03<00:56,  1.05batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 16/25:  54%|████████████▊           | 67/125 [01:03<00:55,  1.05batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 16/25:  54%|████████████▊           | 67/125 [01:04<00:55,  1.05batch/s, Loss=0.425]\u001b[A\n",
      "Epoch 16/25:  54%|█████████████           | 68/125 [01:04<00:54,  1.05batch/s, Loss=0.425]\u001b[A\n",
      "Epoch 16/25:  54%|█████████████           | 68/125 [01:05<00:54,  1.05batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 16/25:  55%|█████████████▏          | 69/125 [01:05<00:53,  1.04batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 16/25:  55%|█████████████▏          | 69/125 [01:06<00:53,  1.04batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 16/25:  56%|█████████████▍          | 70/125 [01:06<00:52,  1.05batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 16/25:  56%|█████████████▍          | 70/125 [01:07<00:52,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 16/25:  57%|█████████████▋          | 71/125 [01:07<00:51,  1.05batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 16/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.05batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 16/25:  58%|█████████████▊          | 72/125 [01:08<00:50,  1.05batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 16/25:  58%|██████████████▍          | 72/125 [01:09<00:50,  1.05batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 16/25:  58%|██████████████▌          | 73/125 [01:09<00:49,  1.05batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 16/25:  58%|██████████████▌          | 73/125 [01:10<00:49,  1.05batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 16/25:  59%|██████████████▊          | 74/125 [01:10<00:48,  1.05batch/s, Loss=0.68]\u001b[A\n",
      "Epoch 16/25:  59%|██████████████▏         | 74/125 [01:11<00:48,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 16/25:  60%|██████████████▍         | 75/125 [01:11<00:47,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 16/25:  60%|███████████████          | 75/125 [01:12<00:47,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 16/25:  61%|███████████████▏         | 76/125 [01:12<00:46,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 16/25:  61%|███████████████▊          | 76/125 [01:13<00:46,  1.05batch/s, Loss=0.6]\u001b[A\n",
      "Epoch 16/25:  62%|████████████████          | 77/125 [01:13<00:45,  1.05batch/s, Loss=0.6]\u001b[A\n",
      "Epoch 16/25:  62%|██████████████▊         | 77/125 [01:14<00:45,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 16/25:  62%|██████████████▉         | 78/125 [01:14<00:44,  1.05batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 16/25:  62%|██████████████▉         | 78/125 [01:15<00:44,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 16/25:  63%|███████████████▏        | 79/125 [01:15<00:43,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 16/25:  63%|███████████████▏        | 79/125 [01:16<00:43,  1.05batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 16/25:  64%|███████████████▎        | 80/125 [01:16<00:42,  1.05batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 16/25:  64%|████████████████         | 80/125 [01:17<00:42,  1.05batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 16/25:  65%|████████████████▏        | 81/125 [01:17<00:42,  1.04batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 16/25:  65%|████████████████▏        | 81/125 [01:18<00:42,  1.04batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 16/25:  66%|████████████████▍        | 82/125 [01:18<00:41,  1.04batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 16/25:  66%|███████████████▋        | 82/125 [01:19<00:41,  1.04batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 16/25:  66%|███████████████▉        | 83/125 [01:19<00:40,  1.05batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 16/25:  66%|███████████████▉        | 83/125 [01:19<00:40,  1.05batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 16/25:  67%|████████████████▏       | 84/125 [01:19<00:39,  1.05batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 16/25:  67%|████████████████▏       | 84/125 [01:20<00:39,  1.05batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 16/25:  68%|████████████████▎       | 85/125 [01:20<00:38,  1.05batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 16/25:  68%|█████████████████        | 85/125 [01:21<00:38,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 16/25:  69%|█████████████████▏       | 86/125 [01:21<00:37,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 16/25:  69%|████████████████▌       | 86/125 [01:22<00:37,  1.05batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 16/25:  70%|████████████████▋       | 87/125 [01:22<00:36,  1.05batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 16/25:  70%|████████████████▋       | 87/125 [01:23<00:36,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 16/25:  70%|████████████████▉       | 88/125 [01:23<00:35,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 16/25:  70%|████████████████▉       | 88/125 [01:24<00:35,  1.05batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 16/25:  71%|█████████████████       | 89/125 [01:24<00:34,  1.05batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 16/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.05batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 16/25:  72%|█████████████████▎      | 90/125 [01:25<00:33,  1.05batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 16/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.05batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 16/25:  73%|█████████████████▍      | 91/125 [01:26<00:32,  1.05batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 16/25:  73%|█████████████████▍      | 91/125 [01:27<00:32,  1.05batch/s, Loss=0.656]\u001b[A\n",
      "Epoch 16/25:  74%|█████████████████▋      | 92/125 [01:27<00:31,  1.05batch/s, Loss=0.656]\u001b[A\n",
      "Epoch 16/25:  74%|█████████████████▋      | 92/125 [01:28<00:31,  1.05batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 16/25:  74%|█████████████████▊      | 93/125 [01:28<00:30,  1.05batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 16/25:  74%|█████████████████▊      | 93/125 [01:29<00:30,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  75%|██████████████████      | 94/125 [01:29<00:29,  1.05batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 16/25:  75%|██████████████████▊      | 94/125 [01:30<00:29,  1.05batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 16/25:  76%|███████████████████      | 95/125 [01:30<00:28,  1.05batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 16/25:  76%|██████████████████▏     | 95/125 [01:31<00:28,  1.05batch/s, Loss=0.785]\u001b[A\n",
      "Epoch 16/25:  77%|██████████████████▍     | 96/125 [01:31<00:27,  1.05batch/s, Loss=0.785]\u001b[A\n",
      "Epoch 16/25:  77%|██████████████████▍     | 96/125 [01:32<00:27,  1.05batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 16/25:  78%|██████████████████▌     | 97/125 [01:32<00:26,  1.05batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 16/25:  78%|██████████████████▌     | 97/125 [01:33<00:26,  1.05batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 16/25:  78%|██████████████████▊     | 98/125 [01:33<00:25,  1.05batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 16/25:  78%|██████████████████▊     | 98/125 [01:34<00:25,  1.05batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 16/25:  79%|███████████████████     | 99/125 [01:34<00:24,  1.05batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 16/25:  79%|███████████████████     | 99/125 [01:35<00:24,  1.05batch/s, Loss=0.504]\u001b[A\n",
      "Epoch 16/25:  80%|██████████████████▍    | 100/125 [01:35<00:23,  1.05batch/s, Loss=0.504]\u001b[A\n",
      "Epoch 16/25:  80%|███████████████████▏    | 100/125 [01:36<00:23,  1.05batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 16/25:  81%|███████████████████▍    | 101/125 [01:36<00:22,  1.05batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 16/25:  81%|██████████████████▌    | 101/125 [01:37<00:22,  1.05batch/s, Loss=0.778]\u001b[A\n",
      "Epoch 16/25:  82%|██████████████████▊    | 102/125 [01:37<00:21,  1.05batch/s, Loss=0.778]\u001b[A\n",
      "Epoch 16/25:  82%|██████████████████▊    | 102/125 [01:38<00:21,  1.05batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 16/25:  82%|██████████████████▉    | 103/125 [01:38<00:21,  1.05batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 16/25:  82%|██████████████████▉    | 103/125 [01:39<00:21,  1.05batch/s, Loss=0.517]\u001b[A\n",
      "Epoch 16/25:  83%|███████████████████▏   | 104/125 [01:39<00:20,  1.05batch/s, Loss=0.517]\u001b[A\n",
      "Epoch 16/25:  83%|███████████████████▏   | 104/125 [01:39<00:20,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 16/25:  84%|███████████████████▎   | 105/125 [01:39<00:19,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 16/25:  84%|███████████████████▎   | 105/125 [01:40<00:19,  1.05batch/s, Loss=0.469]\u001b[A\n",
      "Epoch 16/25:  85%|███████████████████▌   | 106/125 [01:40<00:18,  1.05batch/s, Loss=0.469]\u001b[A\n",
      "Epoch 16/25:  85%|███████████████████▌   | 106/125 [01:41<00:18,  1.05batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  86%|███████████████████▋   | 107/125 [01:41<00:17,  1.05batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  86%|███████████████████▋   | 107/125 [01:42<00:17,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 16/25:  86%|███████████████████▊   | 108/125 [01:42<00:16,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 16/25:  86%|███████████████████▊   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 16/25:  87%|████████████████████   | 109/125 [01:43<00:15,  1.05batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 16/25:  87%|████████████████████   | 109/125 [01:44<00:15,  1.05batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 16/25:  88%|████████████████████▏  | 110/125 [01:44<00:14,  1.04batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 16/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.04batch/s, Loss=0.889]\u001b[A\n",
      "Epoch 16/25:  89%|████████████████████▍  | 111/125 [01:45<00:13,  1.05batch/s, Loss=0.889]\u001b[A\n",
      "Epoch 16/25:  89%|████████████████████▍  | 111/125 [01:46<00:13,  1.05batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 16/25:  90%|████████████████████▌  | 112/125 [01:46<00:12,  1.05batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 16/25:  90%|█████████████████████▌  | 112/125 [01:47<00:12,  1.05batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 16/25:  90%|█████████████████████▋  | 113/125 [01:47<00:11,  1.05batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 16/25:  90%|████████████████████▊  | 113/125 [01:48<00:11,  1.05batch/s, Loss=0.389]\u001b[A\n",
      "Epoch 16/25:  91%|████████████████████▉  | 114/125 [01:48<00:10,  1.06batch/s, Loss=0.389]\u001b[A\n",
      "Epoch 16/25:  91%|████████████████████▉  | 114/125 [01:49<00:10,  1.06batch/s, Loss=0.502]\u001b[A\n",
      "Epoch 16/25:  92%|█████████████████████▏ | 115/125 [01:49<00:09,  1.06batch/s, Loss=0.502]\u001b[A\n",
      "Epoch 16/25:  92%|█████████████████████▏ | 115/125 [01:50<00:09,  1.06batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 16/25:  93%|█████████████████████▎ | 116/125 [01:50<00:08,  1.06batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 16/25:  93%|██████████████████████▎ | 116/125 [01:51<00:08,  1.06batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 16/25:  94%|██████████████████████▍ | 117/125 [01:51<00:07,  1.06batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 16/25:  94%|█████████████████████▌ | 117/125 [01:52<00:07,  1.06batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 16/25:  94%|█████████████████████▋ | 118/125 [01:52<00:06,  1.06batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 16/25:  94%|█████████████████████▋ | 118/125 [01:53<00:06,  1.06batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  95%|█████████████████████▉ | 119/125 [01:53<00:05,  1.06batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.06batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 16/25:  96%|██████████████████████ | 120/125 [01:54<00:04,  1.06batch/s, Loss=0.556]\u001b[A\n",
      "Epoch 16/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.06batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  97%|██████████████████████▎| 121/125 [01:55<00:03,  1.06batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 16/25:  97%|██████████████████████▎| 121/125 [01:56<00:03,  1.06batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 16/25:  98%|██████████████████████▍| 122/125 [01:56<00:02,  1.06batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 16/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.06batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 16/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.06batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 16/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.06batch/s, Loss=0.558]\u001b[A\n",
      "Epoch 16/25:  99%|██████████████████████▊| 124/125 [01:57<00:00,  1.06batch/s, Loss=0.558]\u001b[A\n",
      "Epoch 16/25:  99%|██████████████████████▊| 124/125 [01:58<00:00,  1.06batch/s, Loss=0.848]\u001b[A\n",
      "Epoch 16/25: 100%|███████████████████████| 125/125 [01:58<00:00,  1.05batch/s, Loss=0.848]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 16/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   1%|▏                         | 1/125 [00:00<00:45,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   2%|▍                         | 2/125 [00:00<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   2%|▌                         | 3/125 [00:01<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   3%|▊                         | 4/125 [00:01<00:44,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   4%|█                         | 5/125 [00:01<00:43,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   8%|██                       | 10/125 [00:03<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  10%|██▌                      | 13/125 [00:04<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  12%|███                      | 15/125 [00:05<00:40,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  13%|███▏                     | 16/125 [00:05<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  15%|███▊                     | 19/125 [00:06<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  16%|████                     | 20/125 [00:07<00:38,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  18%|████▍                    | 22/125 [00:07<00:37,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  18%|████▌                    | 23/125 [00:08<00:37,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  20%|█████                    | 25/125 [00:09<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  24%|██████                   | 30/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  26%|██████▌                  | 33/125 [00:11<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  28%|███████                  | 35/125 [00:12<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  30%|███████▍                 | 37/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  32%|████████                 | 40/125 [00:14<00:30,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  36%|█████████                | 45/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  38%|█████████▌               | 48/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  40%|██████████               | 50/125 [00:18<00:27,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  41%|██████████▏              | 51/125 [00:18<00:27,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  44%|███████████              | 55/125 [00:20<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  47%|███████████▊             | 59/125 [00:21<00:24,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  48%|████████████             | 60/125 [00:21<00:23,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  74%|██████████████████▍      | 92/125 [00:33<00:12,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  76%|███████████████████      | 95/125 [00:34<00:10,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  78%|███████████████████▍     | 97/125 [00:35<00:10,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  78%|███████████████████▌     | 98/125 [00:35<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  79%|███████████████████▊     | 99/125 [00:36<00:09,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  80%|███████████████████▏    | 100/125 [00:36<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  82%|███████████████████▊    | 103/125 [00:37<00:08,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  85%|████████████████████▎   | 106/125 [00:38<00:06,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  87%|████████████████████▉   | 109/125 [00:39<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  88%|█████████████████████   | 110/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  89%|█████████████████████▎  | 111/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  90%|█████████████████████▋  | 113/125 [00:41<00:04,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  91%|█████████████████████▉  | 114/125 [00:41<00:04,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  92%|██████████████████████  | 115/125 [00:41<00:03,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  94%|██████████████████████▋ | 118/125 [00:43<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  97%|███████████████████████▏| 121/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25:  99%|███████████████████████▊| 124/125 [00:45<00:00,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 16/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.74batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Eval Accuracy: 0.7520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 17/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 17/25:   1%|▏                        | 1/125 [00:01<03:13,  1.56s/batch, Loss=0.536]\u001b[A\n",
      "Epoch 17/25:   1%|▏                        | 1/125 [00:02<03:13,  1.56s/batch, Loss=0.328]\u001b[A\n",
      "Epoch 17/25:   2%|▍                        | 2/125 [00:02<02:27,  1.20s/batch, Loss=0.328]\u001b[A\n",
      "Epoch 17/25:   2%|▍                        | 2/125 [00:03<02:27,  1.20s/batch, Loss=0.481]\u001b[A\n",
      "Epoch 17/25:   2%|▌                        | 3/125 [00:03<02:12,  1.09s/batch, Loss=0.481]\u001b[A\n",
      "Epoch 17/25:   2%|▌                        | 3/125 [00:04<02:12,  1.09s/batch, Loss=0.525]\u001b[A\n",
      "Epoch 17/25:   3%|▊                        | 4/125 [00:04<02:05,  1.03s/batch, Loss=0.525]\u001b[A\n",
      "Epoch 17/25:   3%|▊                        | 4/125 [00:05<02:05,  1.03s/batch, Loss=0.433]\u001b[A\n",
      "Epoch 17/25:   4%|█                        | 5/125 [00:05<02:00,  1.00s/batch, Loss=0.433]\u001b[A\n",
      "Epoch 17/25:   4%|█                        | 5/125 [00:06<02:00,  1.00s/batch, Loss=0.347]\u001b[A\n",
      "Epoch 17/25:   5%|█▏                       | 6/125 [00:06<01:57,  1.01batch/s, Loss=0.347]\u001b[A\n",
      "Epoch 17/25:   5%|█▏                       | 6/125 [00:07<01:57,  1.01batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 17/25:   6%|█▍                       | 7/125 [00:07<01:55,  1.03batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 17/25:   6%|█▍                       | 7/125 [00:08<01:55,  1.03batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 17/25:   6%|█▌                       | 8/125 [00:08<01:53,  1.03batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 17/25:   6%|█▌                       | 8/125 [00:09<01:53,  1.03batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 17/25:   7%|█▊                       | 9/125 [00:09<01:51,  1.04batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 17/25:   7%|█▊                        | 9/125 [00:10<01:51,  1.04batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 17/25:   8%|██                       | 10/125 [00:10<01:50,  1.04batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 17/25:   8%|██                       | 10/125 [00:11<01:50,  1.04batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 17/25:   9%|██▏                      | 11/125 [00:11<01:49,  1.04batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 17/25:   9%|██▏                      | 11/125 [00:12<01:49,  1.04batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 17/25:  10%|██▍                      | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 17/25:  10%|██▍                      | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 17/25:  10%|██▌                      | 13/125 [00:12<01:46,  1.05batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 17/25:  10%|██▍                     | 13/125 [00:13<01:46,  1.05batch/s, Loss=0.893]\u001b[A\n",
      "Epoch 17/25:  11%|██▋                     | 14/125 [00:13<01:45,  1.05batch/s, Loss=0.893]\u001b[A\n",
      "Epoch 17/25:  11%|██▋                     | 14/125 [00:14<01:45,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 17/25:  12%|██▉                     | 15/125 [00:14<01:44,  1.05batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 17/25:  12%|██▉                     | 15/125 [00:15<01:44,  1.05batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 17/25:  13%|███                     | 16/125 [00:15<01:43,  1.05batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 17/25:  13%|███                     | 16/125 [00:16<01:43,  1.05batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 17/25:  14%|███▎                    | 17/125 [00:16<01:42,  1.05batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 17/25:  14%|███▎                    | 17/125 [00:17<01:42,  1.05batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 17/25:  14%|███▍                    | 18/125 [00:17<01:41,  1.05batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 17/25:  14%|███▍                    | 18/125 [00:18<01:41,  1.05batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 17/25:  15%|███▋                    | 19/125 [00:18<01:40,  1.06batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 17/25:  15%|███▋                    | 19/125 [00:19<01:40,  1.06batch/s, Loss=0.405]\u001b[A\n",
      "Epoch 17/25:  16%|███▊                    | 20/125 [00:19<01:39,  1.06batch/s, Loss=0.405]\u001b[A\n",
      "Epoch 17/25:  16%|████                     | 20/125 [00:20<01:39,  1.06batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 17/25:  17%|████▏                    | 21/125 [00:20<01:38,  1.05batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 17/25:  17%|████                    | 21/125 [00:21<01:38,  1.05batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 17/25:  18%|████▏                   | 22/125 [00:21<01:37,  1.05batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 17/25:  18%|████▍                    | 22/125 [00:22<01:37,  1.05batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 17/25:  18%|████▌                    | 23/125 [00:22<01:36,  1.06batch/s, Loss=0.52]\u001b[A\n",
      "Epoch 17/25:  18%|████▍                   | 23/125 [00:23<01:36,  1.06batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 17/25:  19%|████▌                   | 24/125 [00:23<01:35,  1.06batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 17/25:  19%|████▌                   | 24/125 [00:24<01:35,  1.06batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  20%|████▊                   | 25/125 [00:24<01:34,  1.06batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  20%|████▊                   | 25/125 [00:25<01:34,  1.06batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 17/25:  21%|████▉                   | 26/125 [00:25<01:33,  1.06batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 17/25:  21%|████▉                   | 26/125 [00:26<01:33,  1.06batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 17/25:  22%|█████▏                  | 27/125 [00:26<01:32,  1.06batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 17/25:  22%|█████▏                  | 27/125 [00:27<01:32,  1.06batch/s, Loss=0.764]\u001b[A\n",
      "Epoch 17/25:  22%|█████▍                  | 28/125 [00:27<01:31,  1.06batch/s, Loss=0.764]\u001b[A\n",
      "Epoch 17/25:  22%|█████▍                  | 28/125 [00:28<01:31,  1.06batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 17/25:  23%|█████▌                  | 29/125 [00:28<01:30,  1.06batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 17/25:  23%|█████▊                   | 29/125 [00:29<01:30,  1.06batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 17/25:  24%|██████                   | 30/125 [00:29<01:29,  1.06batch/s, Loss=0.62]\u001b[A\n",
      "Epoch 17/25:  24%|█████▊                  | 30/125 [00:30<01:29,  1.06batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 17/25:  25%|█████▉                  | 31/125 [00:30<01:28,  1.06batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 17/25:  25%|█████▉                  | 31/125 [00:30<01:28,  1.06batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 17/25:  26%|██████▏                 | 32/125 [00:30<01:28,  1.05batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 17/25:  26%|██████▏                 | 32/125 [00:31<01:28,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 17/25:  26%|██████▎                 | 33/125 [00:31<01:27,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 17/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 17/25:  27%|██████▌                 | 34/125 [00:32<01:26,  1.05batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 17/25:  27%|██████▌                 | 34/125 [00:33<01:26,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 17/25:  28%|██████▋                 | 35/125 [00:33<01:25,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 17/25:  28%|██████▋                 | 35/125 [00:34<01:25,  1.05batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 17/25:  29%|██████▉                 | 36/125 [00:34<01:24,  1.06batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 17/25:  29%|██████▉                 | 36/125 [00:35<01:24,  1.06batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 17/25:  30%|███████                 | 37/125 [00:35<01:23,  1.06batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 17/25:  30%|███████▍                 | 37/125 [00:36<01:23,  1.06batch/s, Loss=0.79]\u001b[A\n",
      "Epoch 17/25:  30%|███████▌                 | 38/125 [00:36<01:22,  1.06batch/s, Loss=0.79]\u001b[A\n",
      "Epoch 17/25:  30%|███████▎                | 38/125 [00:37<01:22,  1.06batch/s, Loss=0.792]\u001b[A\n",
      "Epoch 17/25:  31%|███████▍                | 39/125 [00:37<01:21,  1.06batch/s, Loss=0.792]\u001b[A\n",
      "Epoch 17/25:  31%|███████▍                | 39/125 [00:38<01:21,  1.06batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 17/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.06batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 17/25:  32%|███████▋                | 40/125 [00:39<01:20,  1.06batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 17/25:  33%|███████▊                | 41/125 [00:39<01:19,  1.06batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 17/25:  33%|███████▊                | 41/125 [00:40<01:19,  1.06batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 17/25:  34%|████████                | 42/125 [00:40<01:18,  1.05batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 17/25:  34%|████████▍                | 42/125 [00:41<01:18,  1.05batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 17/25:  34%|████████▌                | 43/125 [00:41<01:18,  1.05batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 17/25:  34%|████████▎               | 43/125 [00:42<01:18,  1.05batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 17/25:  35%|████████▍               | 44/125 [00:42<01:17,  1.05batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 17/25:  35%|████████▍               | 44/125 [00:43<01:17,  1.05batch/s, Loss=0.886]\u001b[A\n",
      "Epoch 17/25:  36%|████████▋               | 45/125 [00:43<01:16,  1.05batch/s, Loss=0.886]\u001b[A\n",
      "Epoch 17/25:  36%|████████▋               | 45/125 [00:44<01:16,  1.05batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 17/25:  37%|████████▊               | 46/125 [00:44<01:15,  1.05batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 17/25:  37%|████████▊               | 46/125 [00:45<01:15,  1.05batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 17/25:  38%|█████████               | 47/125 [00:45<01:14,  1.05batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 17/25:  38%|█████████               | 47/125 [00:46<01:14,  1.05batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 17/25:  38%|█████████▏              | 48/125 [00:46<01:13,  1.05batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 17/25:  38%|█████████▏              | 48/125 [00:47<01:13,  1.05batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 17/25:  39%|█████████▍              | 49/125 [00:47<01:12,  1.05batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 17/25:  39%|█████████▍              | 49/125 [00:48<01:12,  1.05batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 17/25:  40%|█████████▌              | 50/125 [00:48<01:11,  1.05batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 17/25:  40%|█████████▌              | 50/125 [00:49<01:11,  1.05batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 17/25:  41%|█████████▊              | 51/125 [00:49<01:10,  1.05batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 17/25:  41%|█████████▊              | 51/125 [00:50<01:10,  1.05batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 17/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 17/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.05batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 17/25:  42%|██████████▏             | 53/125 [00:50<01:08,  1.05batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 17/25:  42%|██████████▏             | 53/125 [00:51<01:08,  1.05batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 17/25:  43%|██████████▎             | 54/125 [00:51<01:07,  1.05batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 17/25:  43%|██████████▎             | 54/125 [00:52<01:07,  1.05batch/s, Loss=0.613]\u001b[A\n",
      "Epoch 17/25:  44%|██████████▌             | 55/125 [00:52<01:06,  1.05batch/s, Loss=0.613]\u001b[A\n",
      "Epoch 17/25:  44%|██████████▌             | 55/125 [00:53<01:06,  1.05batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 17/25:  45%|██████████▊             | 56/125 [00:53<01:05,  1.05batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 17/25:  45%|██████████▊             | 56/125 [00:54<01:05,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  46%|██████████▉             | 57/125 [00:54<01:04,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  46%|██████████▉             | 57/125 [00:55<01:04,  1.05batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 17/25:  46%|███████████▏            | 58/125 [00:55<01:03,  1.05batch/s, Loss=0.472]\u001b[A\n",
      "Epoch 17/25:  46%|███████████▏            | 58/125 [00:56<01:03,  1.05batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 17/25:  47%|███████████▎            | 59/125 [00:56<01:02,  1.06batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 17/25:  47%|███████████▎            | 59/125 [00:57<01:02,  1.06batch/s, Loss=0.648]\u001b[A\n",
      "Epoch 17/25:  48%|███████████▌            | 60/125 [00:57<01:01,  1.05batch/s, Loss=0.648]\u001b[A\n",
      "Epoch 17/25:  48%|███████████▌            | 60/125 [00:58<01:01,  1.05batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 17/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.05batch/s, Loss=0.591]\u001b[A\n",
      "Epoch 17/25:  49%|███████████▋            | 61/125 [00:59<01:00,  1.05batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 17/25:  50%|███████████▉            | 62/125 [00:59<00:59,  1.05batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 17/25:  50%|███████████▉            | 62/125 [01:00<00:59,  1.05batch/s, Loss=0.678]\u001b[A\n",
      "Epoch 17/25:  50%|████████████            | 63/125 [01:00<00:58,  1.05batch/s, Loss=0.678]\u001b[A\n",
      "Epoch 17/25:  50%|████████████            | 63/125 [01:01<00:58,  1.05batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 17/25:  51%|████████████▎           | 64/125 [01:01<00:57,  1.06batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 17/25:  51%|████████████▎           | 64/125 [01:02<00:57,  1.06batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 17/25:  52%|████████████▍           | 65/125 [01:02<00:56,  1.06batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 17/25:  52%|████████████▍           | 65/125 [01:03<00:56,  1.06batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 17/25:  53%|████████████▋           | 66/125 [01:03<00:55,  1.06batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 17/25:  53%|████████████▋           | 66/125 [01:04<00:55,  1.06batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 17/25:  54%|████████████▊           | 67/125 [01:04<00:54,  1.06batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 17/25:  54%|████████████▊           | 67/125 [01:05<00:54,  1.06batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 17/25:  54%|█████████████           | 68/125 [01:05<00:54,  1.06batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 17/25:  54%|█████████████           | 68/125 [01:06<00:54,  1.06batch/s, Loss=0.352]\u001b[A\n",
      "Epoch 17/25:  55%|█████████████▏          | 69/125 [01:06<00:53,  1.05batch/s, Loss=0.352]\u001b[A\n",
      "Epoch 17/25:  55%|█████████████▏          | 69/125 [01:07<00:53,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 17/25:  56%|█████████████▍          | 70/125 [01:07<00:52,  1.05batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 17/25:  56%|█████████████▍          | 70/125 [01:08<00:52,  1.05batch/s, Loss=0.427]\u001b[A\n",
      "Epoch 17/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.05batch/s, Loss=0.427]\u001b[A\n",
      "Epoch 17/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.05batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 17/25:  58%|█████████████▊          | 72/125 [01:08<00:50,  1.06batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 17/25:  58%|█████████████▊          | 72/125 [01:09<00:50,  1.06batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 17/25:  58%|██████████████          | 73/125 [01:09<00:49,  1.05batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 17/25:  58%|██████████████          | 73/125 [01:10<00:49,  1.05batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 17/25:  59%|██████████████▏         | 74/125 [01:10<00:48,  1.05batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 17/25:  59%|██████████████▏         | 74/125 [01:11<00:48,  1.05batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 17/25:  60%|██████████████▍         | 75/125 [01:11<00:47,  1.05batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 17/25:  60%|██████████████▍         | 75/125 [01:12<00:47,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 17/25:  61%|██████████████▌         | 76/125 [01:12<00:46,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 17/25:  61%|██████████████▌         | 76/125 [01:13<00:46,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 17/25:  62%|██████████████▊         | 77/125 [01:13<00:45,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 17/25:  62%|██████████████▊         | 77/125 [01:14<00:45,  1.05batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 17/25:  62%|██████████████▉         | 78/125 [01:14<00:44,  1.05batch/s, Loss=0.686]\u001b[A\n",
      "Epoch 17/25:  62%|██████████████▉         | 78/125 [01:15<00:44,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 17/25:  63%|███████████████▏        | 79/125 [01:15<00:43,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 17/25:  63%|███████████████▏        | 79/125 [01:16<00:43,  1.05batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 17/25:  64%|███████████████▎        | 80/125 [01:16<00:42,  1.06batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 17/25:  64%|███████████████▎        | 80/125 [01:17<00:42,  1.06batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 17/25:  65%|███████████████▌        | 81/125 [01:17<00:41,  1.06batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 17/25:  65%|███████████████▌        | 81/125 [01:18<00:41,  1.06batch/s, Loss=0.373]\u001b[A\n",
      "Epoch 17/25:  66%|███████████████▋        | 82/125 [01:18<00:40,  1.06batch/s, Loss=0.373]\u001b[A\n",
      "Epoch 17/25:  66%|███████████████▋        | 82/125 [01:19<00:40,  1.06batch/s, Loss=0.675]\u001b[A\n",
      "Epoch 17/25:  66%|███████████████▉        | 83/125 [01:19<00:39,  1.06batch/s, Loss=0.675]\u001b[A\n",
      "Epoch 17/25:  66%|███████████████▉        | 83/125 [01:20<00:39,  1.06batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 17/25:  67%|████████████████▏       | 84/125 [01:20<00:38,  1.06batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 17/25:  67%|████████████████▏       | 84/125 [01:21<00:38,  1.06batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 17/25:  68%|████████████████▎       | 85/125 [01:21<00:37,  1.06batch/s, Loss=0.549]\u001b[A\n",
      "Epoch 17/25:  68%|████████████████▎       | 85/125 [01:22<00:37,  1.06batch/s, Loss=0.252]\u001b[A\n",
      "Epoch 17/25:  69%|████████████████▌       | 86/125 [01:22<00:36,  1.06batch/s, Loss=0.252]\u001b[A\n",
      "Epoch 17/25:  69%|████████████████▌       | 86/125 [01:23<00:36,  1.06batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 17/25:  70%|████████████████▋       | 87/125 [01:23<00:35,  1.06batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 17/25:  70%|████████████████▋       | 87/125 [01:24<00:35,  1.06batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 17/25:  70%|████████████████▉       | 88/125 [01:24<00:35,  1.06batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 17/25:  70%|████████████████▉       | 88/125 [01:25<00:35,  1.06batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 17/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.06batch/s, Loss=0.562]\u001b[A\n",
      "Epoch 17/25:  71%|█████████████████       | 89/125 [01:26<00:34,  1.06batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 17/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.06batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 17/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.06batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 17/25:  73%|█████████████████▍      | 91/125 [01:26<00:32,  1.06batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 17/25:  73%|█████████████████▍      | 91/125 [01:27<00:32,  1.06batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 17/25:  74%|█████████████████▋      | 92/125 [01:27<00:31,  1.05batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 17/25:  74%|██████████████████▍      | 92/125 [01:28<00:31,  1.05batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 17/25:  74%|██████████████████▌      | 93/125 [01:28<00:30,  1.05batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 17/25:  74%|█████████████████▊      | 93/125 [01:29<00:30,  1.05batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 17/25:  75%|██████████████████      | 94/125 [01:29<00:29,  1.06batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 17/25:  75%|██████████████████      | 94/125 [01:30<00:29,  1.06batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 17/25:  76%|██████████████████▏     | 95/125 [01:30<00:28,  1.06batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 17/25:  76%|██████████████████▏     | 95/125 [01:31<00:28,  1.06batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 17/25:  77%|██████████████████▍     | 96/125 [01:31<00:27,  1.05batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 17/25:  77%|███████████████████▏     | 96/125 [01:32<00:27,  1.05batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 17/25:  78%|███████████████████▍     | 97/125 [01:32<00:26,  1.05batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 17/25:  78%|██████████████████▌     | 97/125 [01:33<00:26,  1.05batch/s, Loss=0.646]\u001b[A\n",
      "Epoch 17/25:  78%|██████████████████▊     | 98/125 [01:33<00:25,  1.05batch/s, Loss=0.646]\u001b[A\n",
      "Epoch 17/25:  78%|██████████████████▊     | 98/125 [01:34<00:25,  1.05batch/s, Loss=0.366]\u001b[A\n",
      "Epoch 17/25:  79%|███████████████████     | 99/125 [01:34<00:24,  1.05batch/s, Loss=0.366]\u001b[A\n",
      "Epoch 17/25:  79%|███████████████████     | 99/125 [01:35<00:24,  1.05batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 17/25:  80%|██████████████████▍    | 100/125 [01:35<00:23,  1.05batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 17/25:  80%|██████████████████▍    | 100/125 [01:36<00:23,  1.05batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 17/25:  81%|██████████████████▌    | 101/125 [01:36<00:22,  1.05batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 17/25:  81%|██████████████████▌    | 101/125 [01:37<00:22,  1.05batch/s, Loss=0.789]\u001b[A\n",
      "Epoch 17/25:  82%|██████████████████▊    | 102/125 [01:37<00:21,  1.05batch/s, Loss=0.789]\u001b[A\n",
      "Epoch 17/25:  82%|██████████████████▊    | 102/125 [01:38<00:21,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 17/25:  82%|██████████████████▉    | 103/125 [01:38<00:20,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 17/25:  82%|██████████████████▉    | 103/125 [01:39<00:20,  1.05batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 17/25:  83%|███████████████████▏   | 104/125 [01:39<00:19,  1.05batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 17/25:  83%|███████████████████▏   | 104/125 [01:40<00:19,  1.05batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 17/25:  84%|███████████████████▎   | 105/125 [01:40<00:19,  1.05batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 17/25:  84%|███████████████████▎   | 105/125 [01:41<00:19,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 17/25:  85%|███████████████████▌   | 106/125 [01:41<00:18,  1.05batch/s, Loss=0.545]\u001b[A\n",
      "Epoch 17/25:  85%|███████████████████▌   | 106/125 [01:42<00:18,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  86%|███████████████████▋   | 107/125 [01:42<00:17,  1.05batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 17/25:  86%|█████████████████████▍   | 107/125 [01:43<00:17,  1.05batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 17/25:  86%|█████████████████████▌   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 17/25:  86%|███████████████████▊   | 108/125 [01:44<00:16,  1.05batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 17/25:  87%|████████████████████   | 109/125 [01:44<00:15,  1.05batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 17/25:  87%|████████████████████   | 109/125 [01:45<00:15,  1.05batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 17/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.05batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 17/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 17/25:  89%|████████████████████▍  | 111/125 [01:45<00:13,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 17/25:  89%|█████████████████████▎  | 111/125 [01:46<00:13,  1.05batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 17/25:  90%|█████████████████████▌  | 112/125 [01:46<00:12,  1.05batch/s, Loss=0.64]\u001b[A\n",
      "Epoch 17/25:  90%|████████████████████▌  | 112/125 [01:47<00:12,  1.05batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 17/25:  90%|████████████████████▊  | 113/125 [01:47<00:11,  1.05batch/s, Loss=0.699]\u001b[A\n",
      "Epoch 17/25:  90%|████████████████████▊  | 113/125 [01:48<00:11,  1.05batch/s, Loss=0.672]\u001b[A\n",
      "Epoch 17/25:  91%|████████████████████▉  | 114/125 [01:48<00:10,  1.05batch/s, Loss=0.672]\u001b[A\n",
      "Epoch 17/25:  91%|████████████████████▉  | 114/125 [01:49<00:10,  1.05batch/s, Loss=0.351]\u001b[A\n",
      "Epoch 17/25:  92%|█████████████████████▏ | 115/125 [01:49<00:09,  1.05batch/s, Loss=0.351]\u001b[A\n",
      "Epoch 17/25:  92%|██████████████████████  | 115/125 [01:50<00:09,  1.05batch/s, Loss=0.72]\u001b[A\n",
      "Epoch 17/25:  93%|██████████████████████▎ | 116/125 [01:50<00:08,  1.05batch/s, Loss=0.72]\u001b[A\n",
      "Epoch 17/25:  93%|█████████████████████▎ | 116/125 [01:51<00:08,  1.05batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 17/25:  94%|█████████████████████▌ | 117/125 [01:51<00:07,  1.05batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 17/25:  94%|██████████████████████▍ | 117/125 [01:52<00:07,  1.05batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 17/25:  94%|██████████████████████▋ | 118/125 [01:52<00:06,  1.05batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 17/25:  94%|█████████████████████▋ | 118/125 [01:53<00:06,  1.05batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 17/25:  95%|█████████████████████▉ | 119/125 [01:53<00:05,  1.05batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 17/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.05batch/s, Loss=0.284]\u001b[A\n",
      "Epoch 17/25:  96%|██████████████████████ | 120/125 [01:54<00:04,  1.05batch/s, Loss=0.284]\u001b[A\n",
      "Epoch 17/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.05batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 17/25:  97%|██████████████████████▎| 121/125 [01:55<00:03,  1.05batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 17/25:  97%|██████████████████████▎| 121/125 [01:56<00:03,  1.05batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 17/25:  98%|██████████████████████▍| 122/125 [01:56<00:02,  1.05batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 17/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.05batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 17/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.05batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 17/25:  98%|██████████████████████▋| 123/125 [01:58<00:01,  1.05batch/s, Loss=0.414]\u001b[A\n",
      "Epoch 17/25:  99%|██████████████████████▊| 124/125 [01:58<00:00,  1.06batch/s, Loss=0.414]\u001b[A\n",
      "Epoch 17/25:  99%|██████████████████████▊| 124/125 [01:59<00:00,  1.06batch/s, Loss=0.805]\u001b[A\n",
      "Epoch 17/25: 100%|███████████████████████| 125/125 [01:59<00:00,  1.05batch/s, Loss=0.805]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 17/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   1%|▏                         | 1/125 [00:00<00:46,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   2%|▍                         | 2/125 [00:00<00:45,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   2%|▌                         | 3/125 [00:01<00:44,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   3%|▊                         | 4/125 [00:01<00:44,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   4%|█                         | 5/125 [00:01<00:44,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   5%|█▏                        | 6/125 [00:02<00:44,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   6%|█▍                        | 7/125 [00:02<00:43,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   6%|█▋                        | 8/125 [00:02<00:43,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   8%|██                       | 10/125 [00:03<00:42,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  10%|██▍                      | 12/125 [00:04<00:41,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  10%|██▌                      | 13/125 [00:04<00:41,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  12%|███                      | 15/125 [00:05<00:40,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  13%|███▏                     | 16/125 [00:05<00:40,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  14%|███▌                     | 18/125 [00:06<00:39,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  15%|███▊                     | 19/125 [00:07<00:39,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  16%|████                     | 20/125 [00:07<00:39,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  17%|████▏                    | 21/125 [00:07<00:38,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  18%|████▍                    | 22/125 [00:08<00:38,  2.67batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  18%|████▌                    | 23/125 [00:08<00:38,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  19%|████▊                    | 24/125 [00:08<00:37,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  20%|█████                    | 25/125 [00:09<00:37,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  21%|█████▏                   | 26/125 [00:09<00:36,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  22%|█████▍                   | 27/125 [00:09<00:36,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  23%|█████▊                   | 29/125 [00:10<00:35,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  24%|██████                   | 30/125 [00:11<00:34,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  26%|██████▍                  | 32/125 [00:11<00:34,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  26%|██████▌                  | 33/125 [00:12<00:33,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  28%|███████                  | 35/125 [00:12<00:32,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  30%|███████▍                 | 37/125 [00:13<00:32,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  30%|███████▌                 | 38/125 [00:14<00:31,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  32%|████████                 | 40/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  33%|████████▏                | 41/125 [00:15<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  36%|█████████                | 45/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  38%|█████████▌               | 48/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  39%|█████████▊               | 49/125 [00:18<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  40%|██████████               | 50/125 [00:18<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  42%|██████████▍              | 52/125 [00:19<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  44%|███████████              | 55/125 [00:20<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  47%|███████████▊             | 59/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  48%|████████████             | 60/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  50%|████████████▌            | 63/125 [00:23<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  57%|██████████████▏          | 71/125 [00:26<00:19,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  59%|██████████████▊          | 74/125 [00:27<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  66%|████████████████▍        | 82/125 [00:30<00:15,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  68%|█████████████████        | 85/125 [00:31<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  74%|██████████████████▍      | 92/125 [00:33<00:12,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  74%|██████████████████▌      | 93/125 [00:34<00:11,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  76%|███████████████████      | 95/125 [00:34<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  77%|███████████████████▏     | 96/125 [00:35<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  78%|███████████████████▍     | 97/125 [00:35<00:10,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  78%|███████████████████▌     | 98/125 [00:35<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  79%|███████████████████▊     | 99/125 [00:36<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  80%|███████████████████▏    | 100/125 [00:36<00:09,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  82%|███████████████████▊    | 103/125 [00:37<00:08,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  83%|███████████████████▉    | 104/125 [00:38<00:07,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  85%|████████████████████▎   | 106/125 [00:38<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  86%|████████████████████▌   | 107/125 [00:39<00:06,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  87%|████████████████████▉   | 109/125 [00:39<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  88%|█████████████████████   | 110/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  89%|█████████████████████▎  | 111/125 [00:40<00:05,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  90%|█████████████████████▋  | 113/125 [00:41<00:04,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  91%|█████████████████████▉  | 114/125 [00:41<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  92%|██████████████████████  | 115/125 [00:42<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  94%|██████████████████████▋ | 118/125 [00:43<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  97%|███████████████████████▏| 121/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25:  99%|███████████████████████▊| 124/125 [00:45<00:00,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 17/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.74batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Eval Accuracy: 0.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 18/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 18/25:   1%|▏                        | 1/125 [00:01<03:08,  1.52s/batch, Loss=0.566]\u001b[A\n",
      "Epoch 18/25:   1%|▏                        | 1/125 [00:02<03:08,  1.52s/batch, Loss=0.483]\u001b[A\n",
      "Epoch 18/25:   2%|▍                        | 2/125 [00:02<02:25,  1.18s/batch, Loss=0.483]\u001b[A\n",
      "Epoch 18/25:   2%|▍                        | 2/125 [00:03<02:25,  1.18s/batch, Loss=0.963]\u001b[A\n",
      "Epoch 18/25:   2%|▌                        | 3/125 [00:03<02:11,  1.08s/batch, Loss=0.963]\u001b[A\n",
      "Epoch 18/25:   2%|▌                        | 3/125 [00:04<02:11,  1.08s/batch, Loss=0.674]\u001b[A\n",
      "Epoch 18/25:   3%|▊                        | 4/125 [00:04<02:04,  1.03s/batch, Loss=0.674]\u001b[A\n",
      "Epoch 18/25:   3%|▊                        | 4/125 [00:05<02:04,  1.03s/batch, Loss=0.678]\u001b[A\n",
      "Epoch 18/25:   4%|█                        | 5/125 [00:05<01:59,  1.00batch/s, Loss=0.678]\u001b[A\n",
      "Epoch 18/25:   4%|█                        | 5/125 [00:06<01:59,  1.00batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 18/25:   5%|█▏                       | 6/125 [00:06<01:56,  1.02batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 18/25:   5%|█▏                       | 6/125 [00:07<01:56,  1.02batch/s, Loss=0.373]\u001b[A\n",
      "Epoch 18/25:   6%|█▍                       | 7/125 [00:07<01:54,  1.03batch/s, Loss=0.373]\u001b[A\n",
      "Epoch 18/25:   6%|█▍                       | 7/125 [00:08<01:54,  1.03batch/s, Loss=0.724]\u001b[A\n",
      "Epoch 18/25:   6%|█▌                       | 8/125 [00:08<01:52,  1.04batch/s, Loss=0.724]\u001b[A\n",
      "Epoch 18/25:   6%|█▌                       | 8/125 [00:09<01:52,  1.04batch/s, Loss=0.341]\u001b[A\n",
      "Epoch 18/25:   7%|█▊                       | 9/125 [00:09<01:50,  1.05batch/s, Loss=0.341]\u001b[A\n",
      "Epoch 18/25:   7%|█▊                        | 9/125 [00:10<01:50,  1.05batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 18/25:   8%|██                       | 10/125 [00:10<01:49,  1.05batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 18/25:   8%|█▉                      | 10/125 [00:10<01:49,  1.05batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 18/25:   9%|██                      | 11/125 [00:10<01:48,  1.05batch/s, Loss=0.593]\u001b[A\n",
      "Epoch 18/25:   9%|██                      | 11/125 [00:11<01:48,  1.05batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 18/25:  10%|██▎                     | 12/125 [00:11<01:47,  1.05batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 18/25:  10%|██▎                     | 12/125 [00:12<01:47,  1.05batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 18/25:  10%|██▍                     | 13/125 [00:12<01:46,  1.05batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 18/25:  10%|██▌                      | 13/125 [00:13<01:46,  1.05batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 18/25:  11%|██▊                      | 14/125 [00:13<01:45,  1.06batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 18/25:  11%|██▋                     | 14/125 [00:14<01:45,  1.06batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 18/25:  12%|██▉                     | 15/125 [00:14<01:44,  1.06batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 18/25:  12%|██▉                     | 15/125 [00:15<01:44,  1.06batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 18/25:  13%|███                     | 16/125 [00:15<01:43,  1.06batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 18/25:  13%|███▏                     | 16/125 [00:16<01:43,  1.06batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 18/25:  14%|███▍                     | 17/125 [00:16<01:42,  1.06batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 18/25:  14%|███▎                    | 17/125 [00:17<01:42,  1.06batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 18/25:  14%|███▍                    | 18/125 [00:17<01:41,  1.06batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 18/25:  14%|███▍                    | 18/125 [00:18<01:41,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 18/25:  15%|███▋                    | 19/125 [00:18<01:40,  1.06batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 18/25:  15%|███▊                     | 19/125 [00:19<01:40,  1.06batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 18/25:  16%|████                     | 20/125 [00:19<01:39,  1.06batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 18/25:  16%|████                     | 20/125 [00:20<01:39,  1.06batch/s, Loss=0.67]\u001b[A\n",
      "Epoch 18/25:  17%|████▏                    | 21/125 [00:20<01:38,  1.06batch/s, Loss=0.67]\u001b[A\n",
      "Epoch 18/25:  17%|████                    | 21/125 [00:21<01:38,  1.06batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 18/25:  18%|████▏                   | 22/125 [00:21<01:37,  1.06batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 18/25:  18%|████▏                   | 22/125 [00:22<01:37,  1.06batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 18/25:  18%|████▍                   | 23/125 [00:22<01:36,  1.06batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 18/25:  18%|████▍                   | 23/125 [00:23<01:36,  1.06batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 18/25:  19%|████▌                   | 24/125 [00:23<01:35,  1.06batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 18/25:  19%|████▌                   | 24/125 [00:24<01:35,  1.06batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 18/25:  20%|████▊                   | 25/125 [00:24<01:34,  1.06batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 18/25:  20%|████▊                   | 25/125 [00:25<01:34,  1.06batch/s, Loss=0.784]\u001b[A\n",
      "Epoch 18/25:  21%|████▉                   | 26/125 [00:25<01:33,  1.06batch/s, Loss=0.784]\u001b[A\n",
      "Epoch 18/25:  21%|████▉                   | 26/125 [00:26<01:33,  1.06batch/s, Loss=0.847]\u001b[A\n",
      "Epoch 18/25:  22%|█████▏                  | 27/125 [00:26<01:32,  1.06batch/s, Loss=0.847]\u001b[A\n",
      "Epoch 18/25:  22%|█████▏                  | 27/125 [00:27<01:32,  1.06batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 18/25:  22%|█████▍                  | 28/125 [00:27<01:32,  1.05batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 18/25:  22%|█████▍                  | 28/125 [00:28<01:32,  1.05batch/s, Loss=0.814]\u001b[A\n",
      "Epoch 18/25:  23%|█████▌                  | 29/125 [00:28<01:31,  1.05batch/s, Loss=0.814]\u001b[A\n",
      "Epoch 18/25:  23%|█████▊                   | 29/125 [00:28<01:31,  1.05batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 18/25:  24%|██████                   | 30/125 [00:28<01:30,  1.05batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 18/25:  24%|█████▊                  | 30/125 [00:29<01:30,  1.05batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 18/25:  25%|█████▉                  | 31/125 [00:29<01:29,  1.05batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 18/25:  25%|█████▉                  | 31/125 [00:30<01:29,  1.05batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 18/25:  26%|██████▏                 | 32/125 [00:30<01:28,  1.05batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 18/25:  26%|██████▏                 | 32/125 [00:31<01:28,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 18/25:  26%|██████▎                 | 33/125 [00:31<01:27,  1.05batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 18/25:  26%|██████▎                 | 33/125 [00:32<01:27,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 18/25:  27%|██████▌                 | 34/125 [00:32<01:26,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 18/25:  27%|██████▌                 | 34/125 [00:33<01:26,  1.05batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 18/25:  28%|██████▋                 | 35/125 [00:33<01:25,  1.05batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 18/25:  28%|██████▋                 | 35/125 [00:34<01:25,  1.05batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 18/25:  29%|██████▉                 | 36/125 [00:34<01:24,  1.05batch/s, Loss=0.505]\u001b[A\n",
      "Epoch 18/25:  29%|██████▉                 | 36/125 [00:35<01:24,  1.05batch/s, Loss=0.483]\u001b[A\n",
      "Epoch 18/25:  30%|███████                 | 37/125 [00:35<01:23,  1.05batch/s, Loss=0.483]\u001b[A\n",
      "Epoch 18/25:  30%|███████                 | 37/125 [00:36<01:23,  1.05batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 18/25:  30%|███████▎                | 38/125 [00:36<01:22,  1.05batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 18/25:  30%|███████▎                | 38/125 [00:37<01:22,  1.05batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 18/25:  31%|███████▍                | 39/125 [00:37<01:21,  1.05batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 18/25:  31%|███████▍                | 39/125 [00:38<01:21,  1.05batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 18/25:  32%|███████▋                | 40/125 [00:38<01:20,  1.05batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 18/25:  32%|███████▋                | 40/125 [00:39<01:20,  1.05batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 18/25:  33%|███████▊                | 41/125 [00:39<01:19,  1.05batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 18/25:  33%|███████▊                | 41/125 [00:40<01:19,  1.05batch/s, Loss=0.896]\u001b[A\n",
      "Epoch 18/25:  34%|████████                | 42/125 [00:40<01:18,  1.05batch/s, Loss=0.896]\u001b[A\n",
      "Epoch 18/25:  34%|████████                | 42/125 [00:41<01:18,  1.05batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 18/25:  34%|████████▎               | 43/125 [00:41<01:17,  1.05batch/s, Loss=0.666]\u001b[A\n",
      "Epoch 18/25:  34%|████████▌                | 43/125 [00:42<01:17,  1.05batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 18/25:  35%|████████▊                | 44/125 [00:42<01:16,  1.06batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 18/25:  35%|████████▍               | 44/125 [00:43<01:16,  1.06batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 18/25:  36%|████████▋               | 45/125 [00:43<01:16,  1.05batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 18/25:  36%|█████████                | 45/125 [00:44<01:16,  1.05batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 18/25:  37%|█████████▏               | 46/125 [00:44<01:14,  1.05batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 18/25:  37%|████████▊               | 46/125 [00:45<01:14,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 18/25:  38%|█████████               | 47/125 [00:45<01:13,  1.05batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 18/25:  38%|█████████               | 47/125 [00:46<01:13,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  38%|█████████▏              | 48/125 [00:46<01:12,  1.06batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  38%|█████████▏              | 48/125 [00:46<01:12,  1.06batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 18/25:  39%|█████████▍              | 49/125 [00:46<01:11,  1.06batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 18/25:  39%|█████████▍              | 49/125 [00:47<01:11,  1.06batch/s, Loss=0.737]\u001b[A\n",
      "Epoch 18/25:  40%|█████████▌              | 50/125 [00:47<01:11,  1.06batch/s, Loss=0.737]\u001b[A\n",
      "Epoch 18/25:  40%|██████████               | 50/125 [00:48<01:11,  1.06batch/s, Loss=0.65]\u001b[A\n",
      "Epoch 18/25:  41%|██████████▏              | 51/125 [00:48<01:10,  1.06batch/s, Loss=0.65]\u001b[A\n",
      "Epoch 18/25:  41%|█████████▊              | 51/125 [00:49<01:10,  1.06batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 18/25:  42%|█████████▉              | 52/125 [00:49<01:09,  1.06batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 18/25:  42%|█████████▉              | 52/125 [00:50<01:09,  1.06batch/s, Loss=0.717]\u001b[A\n",
      "Epoch 18/25:  42%|██████████▏             | 53/125 [00:50<01:08,  1.06batch/s, Loss=0.717]\u001b[A\n",
      "Epoch 18/25:  42%|██████████▏             | 53/125 [00:51<01:08,  1.06batch/s, Loss=0.655]\u001b[A\n",
      "Epoch 18/25:  43%|██████████▎             | 54/125 [00:51<01:07,  1.06batch/s, Loss=0.655]\u001b[A\n",
      "Epoch 18/25:  43%|██████████▎             | 54/125 [00:52<01:07,  1.06batch/s, Loss=0.569]\u001b[A\n",
      "Epoch 18/25:  44%|██████████▌             | 55/125 [00:52<01:06,  1.06batch/s, Loss=0.569]\u001b[A\n",
      "Epoch 18/25:  44%|██████████▌             | 55/125 [00:53<01:06,  1.06batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 18/25:  45%|██████████▊             | 56/125 [00:53<01:05,  1.06batch/s, Loss=0.508]\u001b[A\n",
      "Epoch 18/25:  45%|██████████▊             | 56/125 [00:54<01:05,  1.06batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 18/25:  46%|██████████▉             | 57/125 [00:54<01:04,  1.06batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 18/25:  46%|██████████▉             | 57/125 [00:55<01:04,  1.06batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 18/25:  46%|███████████▏            | 58/125 [00:55<01:03,  1.05batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 18/25:  46%|███████████▏            | 58/125 [00:56<01:03,  1.05batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 18/25:  47%|███████████▎            | 59/125 [00:56<01:02,  1.05batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 18/25:  47%|███████████▎            | 59/125 [00:57<01:02,  1.05batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 18/25:  48%|███████████▌            | 60/125 [00:57<01:01,  1.06batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 18/25:  48%|███████████▌            | 60/125 [00:58<01:01,  1.06batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 18/25:  49%|███████████▋            | 61/125 [00:58<01:00,  1.06batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 18/25:  49%|███████████▋            | 61/125 [00:59<01:00,  1.06batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 18/25:  50%|███████████▉            | 62/125 [00:59<00:59,  1.05batch/s, Loss=0.685]\u001b[A\n",
      "Epoch 18/25:  50%|███████████▉            | 62/125 [01:00<00:59,  1.05batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 18/25:  50%|████████████            | 63/125 [01:00<00:58,  1.06batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 18/25:  50%|████████████            | 63/125 [01:01<00:58,  1.06batch/s, Loss=0.357]\u001b[A\n",
      "Epoch 18/25:  51%|████████████▎           | 64/125 [01:01<00:57,  1.06batch/s, Loss=0.357]\u001b[A\n",
      "Epoch 18/25:  51%|████████████▎           | 64/125 [01:02<00:57,  1.06batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 18/25:  52%|████████████▍           | 65/125 [01:02<00:56,  1.06batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 18/25:  52%|████████████▍           | 65/125 [01:03<00:56,  1.06batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  53%|████████████▋           | 66/125 [01:03<00:55,  1.05batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  53%|████████████▋           | 66/125 [01:04<00:55,  1.05batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 18/25:  54%|████████████▊           | 67/125 [01:04<00:55,  1.05batch/s, Loss=0.669]\u001b[A\n",
      "Epoch 18/25:  54%|████████████▊           | 67/125 [01:05<00:55,  1.05batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 18/25:  54%|█████████████           | 68/125 [01:05<00:54,  1.05batch/s, Loss=0.803]\u001b[A\n",
      "Epoch 18/25:  54%|█████████████           | 68/125 [01:05<00:54,  1.05batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 18/25:  55%|█████████████▏          | 69/125 [01:05<00:53,  1.04batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 18/25:  55%|█████████████▏          | 69/125 [01:06<00:53,  1.04batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 18/25:  56%|█████████████▍          | 70/125 [01:06<00:52,  1.04batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 18/25:  56%|█████████████▍          | 70/125 [01:07<00:52,  1.04batch/s, Loss=0.445]\u001b[A\n",
      "Epoch 18/25:  57%|█████████████▋          | 71/125 [01:07<00:51,  1.04batch/s, Loss=0.445]\u001b[A\n",
      "Epoch 18/25:  57%|█████████████▋          | 71/125 [01:08<00:51,  1.04batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 18/25:  58%|█████████████▊          | 72/125 [01:08<00:50,  1.04batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 18/25:  58%|█████████████▊          | 72/125 [01:09<00:50,  1.04batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 18/25:  58%|██████████████          | 73/125 [01:09<00:49,  1.05batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 18/25:  58%|██████████████          | 73/125 [01:10<00:49,  1.05batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 18/25:  59%|██████████████▏         | 74/125 [01:10<00:48,  1.05batch/s, Loss=0.573]\u001b[A\n",
      "Epoch 18/25:  59%|██████████████▏         | 74/125 [01:11<00:48,  1.05batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 18/25:  60%|██████████████▍         | 75/125 [01:11<00:47,  1.05batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 18/25:  60%|██████████████▍         | 75/125 [01:12<00:47,  1.05batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 18/25:  61%|██████████████▌         | 76/125 [01:12<00:46,  1.05batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 18/25:  61%|███████████████▏         | 76/125 [01:13<00:46,  1.05batch/s, Loss=0.21]\u001b[A\n",
      "Epoch 18/25:  62%|███████████████▍         | 77/125 [01:13<00:45,  1.05batch/s, Loss=0.21]\u001b[A\n",
      "Epoch 18/25:  62%|██████████████▊         | 77/125 [01:14<00:45,  1.05batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 18/25:  62%|██████████████▉         | 78/125 [01:14<00:44,  1.05batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 18/25:  62%|██████████████▉         | 78/125 [01:15<00:44,  1.05batch/s, Loss=0.481]\u001b[A\n",
      "Epoch 18/25:  63%|███████████████▏        | 79/125 [01:15<00:43,  1.05batch/s, Loss=0.481]\u001b[A\n",
      "Epoch 18/25:  63%|███████████████▏        | 79/125 [01:16<00:43,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 18/25:  64%|███████████████▎        | 80/125 [01:16<00:42,  1.05batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 18/25:  64%|███████████████▎        | 80/125 [01:17<00:42,  1.05batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 18/25:  65%|███████████████▌        | 81/125 [01:17<00:41,  1.05batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 18/25:  65%|███████████████▌        | 81/125 [01:18<00:41,  1.05batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 18/25:  66%|███████████████▋        | 82/125 [01:18<00:40,  1.05batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 18/25:  66%|███████████████▋        | 82/125 [01:19<00:40,  1.05batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 18/25:  66%|███████████████▉        | 83/125 [01:19<00:39,  1.05batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 18/25:  66%|███████████████▉        | 83/125 [01:20<00:39,  1.05batch/s, Loss=0.257]\u001b[A\n",
      "Epoch 18/25:  67%|████████████████▏       | 84/125 [01:20<00:39,  1.05batch/s, Loss=0.257]\u001b[A\n",
      "Epoch 18/25:  67%|████████████████▏       | 84/125 [01:21<00:39,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 18/25:  68%|████████████████▎       | 85/125 [01:21<00:38,  1.05batch/s, Loss=0.577]\u001b[A\n",
      "Epoch 18/25:  68%|█████████████████        | 85/125 [01:22<00:38,  1.05batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 18/25:  69%|█████████████████▏       | 86/125 [01:22<00:37,  1.05batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 18/25:  69%|████████████████▌       | 86/125 [01:23<00:37,  1.05batch/s, Loss=0.674]\u001b[A\n",
      "Epoch 18/25:  70%|████████████████▋       | 87/125 [01:23<00:36,  1.05batch/s, Loss=0.674]\u001b[A\n",
      "Epoch 18/25:  70%|████████████████▋       | 87/125 [01:24<00:36,  1.05batch/s, Loss=0.246]\u001b[A\n",
      "Epoch 18/25:  70%|████████████████▉       | 88/125 [01:24<00:35,  1.05batch/s, Loss=0.246]\u001b[A\n",
      "Epoch 18/25:  70%|████████████████▉       | 88/125 [01:25<00:35,  1.05batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 18/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.05batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 18/25:  71%|█████████████████       | 89/125 [01:25<00:34,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 18/25:  72%|█████████████████▎      | 90/125 [01:25<00:33,  1.05batch/s, Loss=0.605]\u001b[A\n",
      "Epoch 18/25:  72%|█████████████████▎      | 90/125 [01:26<00:33,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 18/25:  73%|█████████████████▍      | 91/125 [01:26<00:32,  1.05batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 18/25:  73%|█████████████████▍      | 91/125 [01:27<00:32,  1.05batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 18/25:  74%|█████████████████▋      | 92/125 [01:27<00:31,  1.05batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 18/25:  74%|█████████████████▋      | 92/125 [01:28<00:31,  1.05batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 18/25:  74%|█████████████████▊      | 93/125 [01:28<00:30,  1.05batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 18/25:  74%|█████████████████▊      | 93/125 [01:29<00:30,  1.05batch/s, Loss=0.312]\u001b[A\n",
      "Epoch 18/25:  75%|██████████████████      | 94/125 [01:29<00:29,  1.05batch/s, Loss=0.312]\u001b[A\n",
      "Epoch 18/25:  75%|██████████████████      | 94/125 [01:30<00:29,  1.05batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 18/25:  76%|██████████████████▏     | 95/125 [01:30<00:28,  1.05batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 18/25:  76%|██████████████████▏     | 95/125 [01:31<00:28,  1.05batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 18/25:  77%|██████████████████▍     | 96/125 [01:31<00:27,  1.05batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 18/25:  77%|██████████████████▍     | 96/125 [01:32<00:27,  1.05batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 18/25:  78%|██████████████████▌     | 97/125 [01:32<00:26,  1.06batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 18/25:  78%|██████████████████▌     | 97/125 [01:33<00:26,  1.06batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  78%|██████████████████▊     | 98/125 [01:33<00:25,  1.06batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 18/25:  78%|██████████████████▊     | 98/125 [01:34<00:25,  1.06batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 18/25:  79%|███████████████████     | 99/125 [01:34<00:24,  1.06batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 18/25:  79%|███████████████████     | 99/125 [01:35<00:24,  1.06batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 18/25:  80%|██████████████████▍    | 100/125 [01:35<00:23,  1.06batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 18/25:  80%|██████████████████▍    | 100/125 [01:36<00:23,  1.06batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 18/25:  81%|██████████████████▌    | 101/125 [01:36<00:22,  1.06batch/s, Loss=0.618]\u001b[A\n",
      "Epoch 18/25:  81%|██████████████████▌    | 101/125 [01:37<00:22,  1.06batch/s, Loss=0.533]\u001b[A\n",
      "Epoch 18/25:  82%|██████████████████▊    | 102/125 [01:37<00:21,  1.06batch/s, Loss=0.533]\u001b[A\n",
      "Epoch 18/25:  82%|██████████████████▊    | 102/125 [01:38<00:21,  1.06batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 18/25:  82%|██████████████████▉    | 103/125 [01:38<00:20,  1.06batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 18/25:  82%|██████████████████▉    | 103/125 [01:39<00:20,  1.06batch/s, Loss=0.305]\u001b[A\n",
      "Epoch 18/25:  83%|███████████████████▏   | 104/125 [01:39<00:19,  1.06batch/s, Loss=0.305]\u001b[A\n",
      "Epoch 18/25:  83%|███████████████████▏   | 104/125 [01:40<00:19,  1.06batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 18/25:  84%|███████████████████▎   | 105/125 [01:40<00:18,  1.05batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 18/25:  84%|███████████████████▎   | 105/125 [01:41<00:18,  1.05batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 18/25:  85%|███████████████████▌   | 106/125 [01:41<00:18,  1.05batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 18/25:  85%|███████████████████▌   | 106/125 [01:42<00:18,  1.05batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 18/25:  86%|███████████████████▋   | 107/125 [01:42<00:17,  1.06batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 18/25:  86%|███████████████████▋   | 107/125 [01:43<00:17,  1.06batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 18/25:  86%|███████████████████▊   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 18/25:  86%|███████████████████▊   | 108/125 [01:43<00:16,  1.05batch/s, Loss=0.854]\u001b[A\n",
      "Epoch 18/25:  87%|████████████████████   | 109/125 [01:43<00:15,  1.05batch/s, Loss=0.854]\u001b[A\n",
      "Epoch 18/25:  87%|████████████████████   | 109/125 [01:44<00:15,  1.05batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 18/25:  88%|████████████████████▏  | 110/125 [01:44<00:14,  1.05batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 18/25:  88%|████████████████████▏  | 110/125 [01:45<00:14,  1.05batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 18/25:  89%|████████████████████▍  | 111/125 [01:45<00:13,  1.05batch/s, Loss=0.585]\u001b[A\n",
      "Epoch 18/25:  89%|████████████████████▍  | 111/125 [01:46<00:13,  1.05batch/s, Loss=0.817]\u001b[A\n",
      "Epoch 18/25:  90%|████████████████████▌  | 112/125 [01:46<00:12,  1.05batch/s, Loss=0.817]\u001b[A\n",
      "Epoch 18/25:  90%|████████████████████▌  | 112/125 [01:47<00:12,  1.05batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 18/25:  90%|████████████████████▊  | 113/125 [01:47<00:11,  1.04batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 18/25:  90%|████████████████████▊  | 113/125 [01:48<00:11,  1.04batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 18/25:  91%|████████████████████▉  | 114/125 [01:48<00:10,  1.05batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 18/25:  91%|████████████████████▉  | 114/125 [01:49<00:10,  1.05batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 18/25:  92%|█████████████████████▏ | 115/125 [01:49<00:09,  1.05batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 18/25:  92%|█████████████████████▏ | 115/125 [01:50<00:09,  1.05batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 18/25:  93%|█████████████████████▎ | 116/125 [01:50<00:08,  1.05batch/s, Loss=0.528]\u001b[A\n",
      "Epoch 18/25:  93%|█████████████████████▎ | 116/125 [01:51<00:08,  1.05batch/s, Loss=0.313]\u001b[A\n",
      "Epoch 18/25:  94%|█████████████████████▌ | 117/125 [01:51<00:07,  1.05batch/s, Loss=0.313]\u001b[A\n",
      "Epoch 18/25:  94%|█████████████████████▌ | 117/125 [01:52<00:07,  1.05batch/s, Loss=0.799]\u001b[A\n",
      "Epoch 18/25:  94%|█████████████████████▋ | 118/125 [01:52<00:06,  1.05batch/s, Loss=0.799]\u001b[A\n",
      "Epoch 18/25:  94%|█████████████████████▋ | 118/125 [01:53<00:06,  1.05batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 18/25:  95%|█████████████████████▉ | 119/125 [01:53<00:05,  1.05batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 18/25:  95%|█████████████████████▉ | 119/125 [01:54<00:05,  1.05batch/s, Loss=0.376]\u001b[A\n",
      "Epoch 18/25:  96%|██████████████████████ | 120/125 [01:54<00:04,  1.05batch/s, Loss=0.376]\u001b[A\n",
      "Epoch 18/25:  96%|██████████████████████ | 120/125 [01:55<00:04,  1.05batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 18/25:  97%|██████████████████████▎| 121/125 [01:55<00:03,  1.05batch/s, Loss=0.603]\u001b[A\n",
      "Epoch 18/25:  97%|██████████████████████▎| 121/125 [01:56<00:03,  1.05batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 18/25:  98%|██████████████████████▍| 122/125 [01:56<00:02,  1.05batch/s, Loss=0.629]\u001b[A\n",
      "Epoch 18/25:  98%|██████████████████████▍| 122/125 [01:57<00:02,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 18/25:  98%|██████████████████████▋| 123/125 [01:57<00:01,  1.05batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 18/25:  98%|██████████████████████▋| 123/125 [01:58<00:01,  1.05batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 18/25:  99%|██████████████████████▊| 124/125 [01:58<00:00,  1.05batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 18/25:  99%|██████████████████████▊| 124/125 [01:59<00:00,  1.05batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 18/25: 100%|███████████████████████| 125/125 [01:59<00:00,  1.05batch/s, Loss=0.442]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 0.0654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 18/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   1%|▏                         | 1/125 [00:00<00:46,  2.64batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   2%|▍                         | 2/125 [00:00<00:46,  2.67batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   2%|▌                         | 3/125 [00:01<00:45,  2.68batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   3%|▊                         | 4/125 [00:01<00:44,  2.69batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   4%|█                         | 5/125 [00:01<00:44,  2.71batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   6%|█▍                        | 7/125 [00:02<00:43,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   7%|█▊                        | 9/125 [00:03<00:42,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   8%|██                       | 10/125 [00:03<00:41,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:   9%|██▏                      | 11/125 [00:04<00:41,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  10%|██▍                      | 12/125 [00:04<00:41,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  10%|██▌                      | 13/125 [00:04<00:41,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  11%|██▊                      | 14/125 [00:05<00:40,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  12%|███                      | 15/125 [00:05<00:40,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  13%|███▏                     | 16/125 [00:05<00:39,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  14%|███▍                     | 17/125 [00:06<00:39,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  14%|███▌                     | 18/125 [00:06<00:39,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  15%|███▊                     | 19/125 [00:06<00:38,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  16%|████                     | 20/125 [00:07<00:38,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  18%|████▍                    | 22/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  18%|████▌                    | 23/125 [00:08<00:37,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  20%|█████                    | 25/125 [00:09<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  21%|█████▏                   | 26/125 [00:09<00:36,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  22%|█████▌                   | 28/125 [00:10<00:35,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  24%|██████                   | 30/125 [00:10<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  25%|██████▏                  | 31/125 [00:11<00:34,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  26%|██████▌                  | 33/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  27%|██████▊                  | 34/125 [00:12<00:33,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  28%|███████                  | 35/125 [00:12<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  29%|███████▏                 | 36/125 [00:13<00:32,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  31%|███████▊                 | 39/125 [00:14<00:31,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  32%|████████                 | 40/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  34%|████████▍                | 42/125 [00:15<00:30,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  35%|████████▊                | 44/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  36%|█████████                | 45/125 [00:16<00:29,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  38%|█████████▍               | 47/125 [00:17<00:28,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  40%|██████████               | 50/125 [00:18<00:27,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  42%|██████████▌              | 53/125 [00:19<00:26,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  44%|███████████              | 55/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  45%|███████████▏             | 56/125 [00:20<00:25,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  46%|███████████▌             | 58/125 [00:21<00:24,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  48%|████████████             | 60/125 [00:21<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  49%|████████████▏            | 61/125 [00:22<00:23,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  51%|████████████▊            | 64/125 [00:23<00:22,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  53%|█████████████▏           | 66/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  54%|█████████████▍           | 67/125 [00:24<00:21,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  55%|█████████████▊           | 69/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  56%|██████████████           | 70/125 [00:25<00:20,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  58%|██████████████▍          | 72/125 [00:26<00:19,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  60%|███████████████          | 75/125 [00:27<00:18,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  62%|███████████████▍         | 77/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  62%|███████████████▌         | 78/125 [00:28<00:17,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  64%|████████████████         | 80/125 [00:29<00:16,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  65%|████████████████▏        | 81/125 [00:29<00:16,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  66%|████████████████▌        | 83/125 [00:30<00:15,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  67%|████████████████▊        | 84/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  69%|█████████████████▏       | 86/125 [00:31<00:14,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  70%|█████████████████▌       | 88/125 [00:32<00:13,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  71%|█████████████████▊       | 89/125 [00:32<00:13,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  73%|██████████████████▏      | 91/125 [00:33<00:12,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  74%|██████████████████▍      | 92/125 [00:33<00:11,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  75%|██████████████████▊      | 94/125 [00:34<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  76%|███████████████████      | 95/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  78%|███████████████████▍     | 97/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  78%|███████████████████▌     | 98/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  80%|███████████████████▏    | 100/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  82%|███████████████████▌    | 102/125 [00:37<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  82%|███████████████████▊    | 103/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  84%|████████████████████▏   | 105/125 [00:38<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  85%|████████████████████▎   | 106/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  86%|████████████████████▋   | 108/125 [00:39<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  87%|████████████████████▉   | 109/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  89%|█████████████████████▎  | 111/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  90%|█████████████████████▌  | 112/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  91%|█████████████████████▉  | 114/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  92%|██████████████████████  | 115/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  93%|██████████████████████▎ | 116/125 [00:42<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  94%|██████████████████████▍ | 117/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  95%|██████████████████████▊ | 119/125 [00:43<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  96%|███████████████████████ | 120/125 [00:43<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  98%|███████████████████████▍| 122/125 [00:44<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  98%|███████████████████████▌| 123/125 [00:44<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 18/25: 100%|████████████████████████| 125/125 [00:45<00:00,  2.76batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Eval Accuracy: 0.7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 19/25:   0%|                                  | 0/125 [00:00<?, ?batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 19/25:   1%|▏                         | 1/125 [00:00<01:58,  1.05batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 19/25:   1%|▏                        | 1/125 [00:01<01:58,  1.05batch/s, Loss=0.344]\u001b[A\n",
      "Epoch 19/25:   2%|▍                        | 2/125 [00:01<01:55,  1.07batch/s, Loss=0.344]\u001b[A\n",
      "Epoch 19/25:   2%|▍                        | 2/125 [00:02<01:55,  1.07batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 19/25:   2%|▌                        | 3/125 [00:02<01:53,  1.08batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 19/25:   2%|▌                        | 3/125 [00:03<01:53,  1.08batch/s, Loss=0.233]\u001b[A\n",
      "Epoch 19/25:   3%|▊                        | 4/125 [00:03<01:51,  1.08batch/s, Loss=0.233]\u001b[A\n",
      "Epoch 19/25:   3%|▊                        | 4/125 [00:04<01:51,  1.08batch/s, Loss=0.278]\u001b[A\n",
      "Epoch 19/25:   4%|█                        | 5/125 [00:04<01:50,  1.08batch/s, Loss=0.278]\u001b[A\n",
      "Epoch 19/25:   4%|█                        | 5/125 [00:05<01:50,  1.08batch/s, Loss=0.634]\u001b[A\n",
      "Epoch 19/25:   5%|█▏                       | 6/125 [00:05<01:49,  1.08batch/s, Loss=0.634]\u001b[A\n",
      "Epoch 19/25:   5%|█▏                       | 6/125 [00:06<01:49,  1.08batch/s, Loss=0.259]\u001b[A\n",
      "Epoch 19/25:   6%|█▍                       | 7/125 [00:06<01:48,  1.09batch/s, Loss=0.259]\u001b[A\n",
      "Epoch 19/25:   6%|█▍                       | 7/125 [00:07<01:48,  1.09batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 19/25:   6%|█▌                       | 8/125 [00:07<01:47,  1.09batch/s, Loss=0.574]\u001b[A\n",
      "Epoch 19/25:   6%|█▌                       | 8/125 [00:08<01:47,  1.09batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 19/25:   7%|█▊                       | 9/125 [00:08<01:46,  1.09batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 19/25:   7%|█▊                        | 9/125 [00:09<01:46,  1.09batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 19/25:   8%|██                       | 10/125 [00:09<01:45,  1.09batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 19/25:   8%|█▉                      | 10/125 [00:10<01:45,  1.09batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 19/25:   9%|██                      | 11/125 [00:10<01:44,  1.09batch/s, Loss=0.547]\u001b[A\n",
      "Epoch 19/25:   9%|██                      | 11/125 [00:11<01:44,  1.09batch/s, Loss=0.814]\u001b[A\n",
      "Epoch 19/25:  10%|██▎                     | 12/125 [00:11<01:43,  1.09batch/s, Loss=0.814]\u001b[A\n",
      "Epoch 19/25:  10%|██▍                      | 12/125 [00:11<01:43,  1.09batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 19/25:  10%|██▌                      | 13/125 [00:11<01:43,  1.09batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 19/25:  10%|██▌                      | 13/125 [00:12<01:43,  1.09batch/s, Loss=0.99]\u001b[A\n",
      "Epoch 19/25:  11%|██▊                      | 14/125 [00:12<01:42,  1.09batch/s, Loss=0.99]\u001b[A\n",
      "Epoch 19/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.09batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 19/25:  12%|██▉                     | 15/125 [00:13<01:41,  1.09batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 19/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.09batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 19/25:  13%|███                     | 16/125 [00:14<01:40,  1.09batch/s, Loss=0.663]\u001b[A\n",
      "Epoch 19/25:  13%|███                     | 16/125 [00:15<01:40,  1.09batch/s, Loss=0.742]\u001b[A\n",
      "Epoch 19/25:  14%|███▎                    | 17/125 [00:15<01:39,  1.09batch/s, Loss=0.742]\u001b[A\n",
      "Epoch 19/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 19/25:  14%|███▍                    | 18/125 [00:16<01:38,  1.09batch/s, Loss=0.456]\u001b[A\n",
      "Epoch 19/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.09batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 19/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.09batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 19/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 19/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 19/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.09batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 19/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.488]\u001b[A\n",
      "Epoch 19/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 19/25:  18%|████▏                   | 22/125 [00:20<01:34,  1.09batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 19/25:  18%|████▏                   | 22/125 [00:21<01:34,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  18%|████▍                   | 23/125 [00:21<01:33,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  18%|████▍                   | 23/125 [00:22<01:33,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  19%|████▌                   | 24/125 [00:22<01:32,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  19%|████▌                   | 24/125 [00:23<01:32,  1.09batch/s, Loss=0.862]\u001b[A\n",
      "Epoch 19/25:  20%|████▊                   | 25/125 [00:23<01:31,  1.09batch/s, Loss=0.862]\u001b[A\n",
      "Epoch 19/25:  20%|████▊                   | 25/125 [00:23<01:31,  1.09batch/s, Loss=0.596]\u001b[A\n",
      "Epoch 19/25:  21%|████▉                   | 26/125 [00:23<01:30,  1.09batch/s, Loss=0.596]\u001b[A\n",
      "Epoch 19/25:  21%|████▉                   | 26/125 [00:24<01:30,  1.09batch/s, Loss=0.281]\u001b[A\n",
      "Epoch 19/25:  22%|█████▏                  | 27/125 [00:24<01:29,  1.09batch/s, Loss=0.281]\u001b[A\n",
      "Epoch 19/25:  22%|█████▏                  | 27/125 [00:25<01:29,  1.09batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 19/25:  22%|█████▍                  | 28/125 [00:25<01:29,  1.09batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 19/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 19/25:  23%|█████▌                  | 29/125 [00:26<01:28,  1.09batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 19/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.451]\u001b[A\n",
      "Epoch 19/25:  24%|█████▊                  | 30/125 [00:27<01:27,  1.09batch/s, Loss=0.451]\u001b[A\n",
      "Epoch 19/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 19/25:  25%|█████▉                  | 31/125 [00:28<01:26,  1.09batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 19/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 19/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 19/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 19/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 19/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 19/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 19/25:  27%|██████▊                  | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.41]\u001b[A\n",
      "Epoch 19/25:  28%|███████                  | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.41]\u001b[A\n",
      "Epoch 19/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.761]\u001b[A\n",
      "Epoch 19/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.761]\u001b[A\n",
      "Epoch 19/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.316]\u001b[A\n",
      "Epoch 19/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.316]\u001b[A\n",
      "Epoch 19/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.733]\u001b[A\n",
      "Epoch 19/25:  30%|███████▎                | 38/125 [00:34<01:20,  1.09batch/s, Loss=0.733]\u001b[A\n",
      "Epoch 19/25:  30%|███████▎                | 38/125 [00:35<01:20,  1.09batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 19/25:  31%|███████▍                | 39/125 [00:35<01:19,  1.09batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 19/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 19/25:  32%|███████▋                | 40/125 [00:36<01:18,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 19/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.816]\u001b[A\n",
      "Epoch 19/25:  33%|███████▊                | 41/125 [00:37<01:17,  1.08batch/s, Loss=0.816]\u001b[A\n",
      "Epoch 19/25:  33%|███████▊                | 41/125 [00:38<01:17,  1.08batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 19/25:  34%|████████                | 42/125 [00:38<01:16,  1.08batch/s, Loss=0.546]\u001b[A\n",
      "Epoch 19/25:  34%|████████                | 42/125 [00:39<01:16,  1.08batch/s, Loss=0.279]\u001b[A\n",
      "Epoch 19/25:  34%|████████▎               | 43/125 [00:39<01:15,  1.08batch/s, Loss=0.279]\u001b[A\n",
      "Epoch 19/25:  34%|████████▎               | 43/125 [00:40<01:15,  1.08batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 19/25:  35%|████████▍               | 44/125 [00:40<01:14,  1.08batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 19/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.08batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 19/25:  36%|████████▋               | 45/125 [00:41<01:13,  1.08batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 19/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.08batch/s, Loss=0.255]\u001b[A\n",
      "Epoch 19/25:  37%|████████▊               | 46/125 [00:42<01:13,  1.08batch/s, Loss=0.255]\u001b[A\n",
      "Epoch 19/25:  37%|████████▊               | 46/125 [00:43<01:13,  1.08batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 19/25:  38%|█████████               | 47/125 [00:43<01:12,  1.08batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 19/25:  38%|█████████               | 47/125 [00:44<01:12,  1.08batch/s, Loss=0.478]\u001b[A\n",
      "Epoch 19/25:  38%|█████████▏              | 48/125 [00:44<01:11,  1.08batch/s, Loss=0.478]\u001b[A\n",
      "Epoch 19/25:  38%|█████████▏              | 48/125 [00:45<01:11,  1.08batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 19/25:  39%|█████████▍              | 49/125 [00:45<01:10,  1.08batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 19/25:  39%|█████████▍              | 49/125 [00:46<01:10,  1.08batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 19/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.513]\u001b[A\n",
      "Epoch 19/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 19/25:  41%|█████████▊              | 51/125 [00:46<01:08,  1.08batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 19/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 19/25:  42%|█████████▉              | 52/125 [00:47<01:07,  1.09batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 19/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.09batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 19/25:  42%|██████████▏             | 53/125 [00:48<01:06,  1.09batch/s, Loss=0.722]\u001b[A\n",
      "Epoch 19/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 19/25:  43%|██████████▎             | 54/125 [00:49<01:05,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 19/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.09batch/s, Loss=0.435]\u001b[A\n",
      "Epoch 19/25:  44%|██████████▌             | 55/125 [00:50<01:04,  1.09batch/s, Loss=0.435]\u001b[A\n",
      "Epoch 19/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.09batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 19/25:  45%|██████████▊             | 56/125 [00:51<01:03,  1.09batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 19/25:  45%|██████████▊             | 56/125 [00:52<01:03,  1.09batch/s, Loss=0.286]\u001b[A\n",
      "Epoch 19/25:  46%|██████████▉             | 57/125 [00:52<01:02,  1.09batch/s, Loss=0.286]\u001b[A\n",
      "Epoch 19/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 19/25:  46%|███████████▏            | 58/125 [00:53<01:01,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 19/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.09batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 19/25:  47%|███████████▎            | 59/125 [00:54<01:00,  1.09batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 19/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.09batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 19/25:  48%|███████████▌            | 60/125 [00:55<00:59,  1.09batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 19/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.09batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 19/25:  49%|███████████▋            | 61/125 [00:56<00:58,  1.09batch/s, Loss=0.623]\u001b[A\n",
      "Epoch 19/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.377]\u001b[A\n",
      "Epoch 19/25:  50%|███████████▉            | 62/125 [00:57<00:57,  1.09batch/s, Loss=0.377]\u001b[A\n",
      "Epoch 19/25:  50%|████████████▍            | 62/125 [00:57<00:57,  1.09batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 19/25:  50%|████████████▌            | 63/125 [00:57<00:56,  1.09batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 19/25:  50%|████████████            | 63/125 [00:58<00:56,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 19/25:  51%|████████████▎           | 64/125 [00:58<00:56,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 19/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.09batch/s, Loss=0.355]\u001b[A\n",
      "Epoch 19/25:  52%|████████████▍           | 65/125 [00:59<00:55,  1.09batch/s, Loss=0.355]\u001b[A\n",
      "Epoch 19/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.09batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 19/25:  53%|████████████▋           | 66/125 [01:00<00:54,  1.09batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 19/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.09batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 19/25:  54%|████████████▊           | 67/125 [01:01<00:53,  1.09batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 19/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.09batch/s, Loss=0.407]\u001b[A\n",
      "Epoch 19/25:  54%|█████████████           | 68/125 [01:02<00:52,  1.09batch/s, Loss=0.407]\u001b[A\n",
      "Epoch 19/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 19/25:  55%|█████████████▏          | 69/125 [01:03<00:51,  1.09batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 19/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.596]\u001b[A\n",
      "Epoch 19/25:  56%|█████████████▍          | 70/125 [01:04<00:50,  1.09batch/s, Loss=0.596]\u001b[A\n",
      "Epoch 19/25:  56%|██████████████           | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.91]\u001b[A\n",
      "Epoch 19/25:  57%|██████████████▏          | 71/125 [01:05<00:49,  1.09batch/s, Loss=0.91]\u001b[A\n",
      "Epoch 19/25:  57%|█████████████▋          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.415]\u001b[A\n",
      "Epoch 19/25:  58%|█████████████▊          | 72/125 [01:06<00:48,  1.09batch/s, Loss=0.415]\u001b[A\n",
      "Epoch 19/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 19/25:  58%|██████████████          | 73/125 [01:07<00:47,  1.09batch/s, Loss=0.688]\u001b[A\n",
      "Epoch 19/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 19/25:  59%|██████████████▏         | 74/125 [01:08<00:46,  1.09batch/s, Loss=0.647]\u001b[A\n",
      "Epoch 19/25:  59%|██████████████▏         | 74/125 [01:09<00:46,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  60%|██████████████▍         | 75/125 [01:09<00:45,  1.09batch/s, Loss=0.598]\u001b[A\n",
      "Epoch 19/25:  60%|██████████████▍         | 75/125 [01:09<00:45,  1.09batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 19/25:  61%|██████████████▌         | 76/125 [01:09<00:45,  1.09batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 19/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 19/25:  62%|██████████████▊         | 77/125 [01:10<00:44,  1.08batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 19/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.08batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 19/25:  62%|██████████████▉         | 78/125 [01:11<00:43,  1.08batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 19/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.08batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 19/25:  63%|███████████████▏        | 79/125 [01:12<00:42,  1.08batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 19/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.08batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 19/25:  64%|███████████████▎        | 80/125 [01:13<00:41,  1.08batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 19/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.08batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 19/25:  65%|███████████████▌        | 81/125 [01:14<00:40,  1.08batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 19/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.08batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 19/25:  66%|███████████████▋        | 82/125 [01:15<00:39,  1.08batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 19/25:  66%|████████████████▍        | 82/125 [01:16<00:39,  1.08batch/s, Loss=0.28]\u001b[A\n",
      "Epoch 19/25:  66%|████████████████▌        | 83/125 [01:16<00:38,  1.08batch/s, Loss=0.28]\u001b[A\n",
      "Epoch 19/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.08batch/s, Loss=0.328]\u001b[A\n",
      "Epoch 19/25:  67%|████████████████▏       | 84/125 [01:17<00:37,  1.08batch/s, Loss=0.328]\u001b[A\n",
      "Epoch 19/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.08batch/s, Loss=0.416]\u001b[A\n",
      "Epoch 19/25:  68%|████████████████▎       | 85/125 [01:18<00:36,  1.08batch/s, Loss=0.416]\u001b[A\n",
      "Epoch 19/25:  68%|████████████████▎       | 85/125 [01:19<00:36,  1.08batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 19/25:  69%|████████████████▌       | 86/125 [01:19<00:36,  1.08batch/s, Loss=0.624]\u001b[A\n",
      "Epoch 19/25:  69%|████████████████▌       | 86/125 [01:20<00:36,  1.08batch/s, Loss=0.451]\u001b[A\n",
      "Epoch 19/25:  70%|████████████████▋       | 87/125 [01:20<00:35,  1.08batch/s, Loss=0.451]\u001b[A\n",
      "Epoch 19/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.08batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 19/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.09batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 19/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 19/25:  71%|█████████████████       | 89/125 [01:21<00:33,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 19/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.09batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 19/25:  72%|█████████████████▎      | 90/125 [01:22<00:32,  1.09batch/s, Loss=0.633]\u001b[A\n",
      "Epoch 19/25:  72%|██████████████████       | 90/125 [01:23<00:32,  1.09batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 19/25:  73%|██████████████████▏      | 91/125 [01:23<00:31,  1.09batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 19/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.09batch/s, Loss=0.738]\u001b[A\n",
      "Epoch 19/25:  74%|█████████████████▋      | 92/125 [01:24<00:30,  1.09batch/s, Loss=0.738]\u001b[A\n",
      "Epoch 19/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 19/25:  74%|█████████████████▊      | 93/125 [01:25<00:29,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 19/25:  74%|█████████████████▊      | 93/125 [01:26<00:29,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 19/25:  75%|██████████████████      | 94/125 [01:26<00:28,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 19/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.09batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 19/25:  76%|██████████████████▏     | 95/125 [01:27<00:27,  1.09batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 19/25:  76%|███████████████████▊      | 95/125 [01:28<00:27,  1.09batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 19/25:  77%|███████████████████▉      | 96/125 [01:28<00:26,  1.09batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 19/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 19/25:  78%|██████████████████▌     | 97/125 [01:29<00:25,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 19/25:  78%|███████████████████▍     | 97/125 [01:30<00:25,  1.09batch/s, Loss=0.21]\u001b[A\n",
      "Epoch 19/25:  78%|███████████████████▌     | 98/125 [01:30<00:24,  1.09batch/s, Loss=0.21]\u001b[A\n",
      "Epoch 19/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 19/25:  79%|███████████████████     | 99/125 [01:31<00:23,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 19/25:  79%|███████████████████     | 99/125 [01:32<00:23,  1.09batch/s, Loss=0.724]\u001b[A\n",
      "Epoch 19/25:  80%|██████████████████▍    | 100/125 [01:32<00:22,  1.09batch/s, Loss=0.724]\u001b[A\n",
      "Epoch 19/25:  80%|██████████████████▍    | 100/125 [01:32<00:22,  1.09batch/s, Loss=0.186]\u001b[A\n",
      "Epoch 19/25:  81%|██████████████████▌    | 101/125 [01:32<00:22,  1.09batch/s, Loss=0.186]\u001b[A\n",
      "Epoch 19/25:  81%|██████████████████▌    | 101/125 [01:33<00:22,  1.09batch/s, Loss=0.868]\u001b[A\n",
      "Epoch 19/25:  82%|██████████████████▊    | 102/125 [01:33<00:21,  1.09batch/s, Loss=0.868]\u001b[A\n",
      "Epoch 19/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.09batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 19/25:  82%|██████████████████▉    | 103/125 [01:34<00:20,  1.09batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 19/25:  82%|███████████████████▊    | 103/125 [01:35<00:20,  1.09batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 19/25:  83%|███████████████████▉    | 104/125 [01:35<00:19,  1.09batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 19/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.09batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 19/25:  84%|███████████████████▎   | 105/125 [01:36<00:18,  1.09batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 19/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.09batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 19/25:  85%|███████████████████▌   | 106/125 [01:37<00:17,  1.09batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 19/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 19/25:  86%|███████████████████▋   | 107/125 [01:38<00:16,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 19/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.09batch/s, Loss=0.695]\u001b[A\n",
      "Epoch 19/25:  86%|███████████████████▊   | 108/125 [01:39<00:15,  1.09batch/s, Loss=0.695]\u001b[A\n",
      "Epoch 19/25:  86%|████████████████████▋   | 108/125 [01:40<00:15,  1.09batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 19/25:  87%|████████████████████▉   | 109/125 [01:40<00:14,  1.09batch/s, Loss=0.48]\u001b[A\n",
      "Epoch 19/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.09batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 19/25:  88%|████████████████████▏  | 110/125 [01:41<00:13,  1.09batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 19/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.345]\u001b[A\n",
      "Epoch 19/25:  89%|████████████████████▍  | 111/125 [01:42<00:12,  1.09batch/s, Loss=0.345]\u001b[A\n",
      "Epoch 19/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 19/25:  90%|████████████████████▌  | 112/125 [01:43<00:11,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 19/25:  90%|████████████████████▌  | 112/125 [01:44<00:11,  1.09batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 19/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.09batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 19/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.09batch/s, Loss=0.628]\u001b[A\n",
      "Epoch 19/25:  91%|████████████████████▉  | 114/125 [01:44<00:10,  1.09batch/s, Loss=0.628]\u001b[A\n",
      "Epoch 19/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.09batch/s, Loss=0.224]\u001b[A\n",
      "Epoch 19/25:  92%|█████████████████████▏ | 115/125 [01:45<00:09,  1.09batch/s, Loss=0.224]\u001b[A\n",
      "Epoch 19/25:  92%|██████████████████████  | 115/125 [01:46<00:09,  1.09batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 19/25:  93%|██████████████████████▎ | 116/125 [01:46<00:08,  1.09batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 19/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.312]\u001b[A\n",
      "Epoch 19/25:  94%|█████████████████████▌ | 117/125 [01:47<00:07,  1.09batch/s, Loss=0.312]\u001b[A\n",
      "Epoch 19/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 19/25:  94%|█████████████████████▋ | 118/125 [01:48<00:06,  1.09batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 19/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 19/25:  95%|█████████████████████▉ | 119/125 [01:49<00:05,  1.09batch/s, Loss=0.543]\u001b[A\n",
      "Epoch 19/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 19/25:  96%|██████████████████████ | 120/125 [01:50<00:04,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 19/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.674]\u001b[A\n",
      "Epoch 19/25:  97%|██████████████████████▎| 121/125 [01:51<00:03,  1.09batch/s, Loss=0.674]\u001b[A\n",
      "Epoch 19/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.09batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 19/25:  98%|██████████████████████▍| 122/125 [01:52<00:02,  1.09batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 19/25:  98%|██████████████████████▍| 122/125 [01:53<00:02,  1.09batch/s, Loss=0.993]\u001b[A\n",
      "Epoch 19/25:  98%|██████████████████████▋| 123/125 [01:53<00:01,  1.09batch/s, Loss=0.993]\u001b[A\n",
      "Epoch 19/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.09batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 19/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.09batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 19/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.09batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 19/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.09batch/s, Loss=0.314]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 19/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   1%|▏                         | 1/125 [00:00<00:44,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   2%|▍                         | 2/125 [00:00<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   2%|▌                         | 3/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   3%|▊                         | 4/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   4%|█                         | 5/125 [00:01<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   6%|█▍                        | 7/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   8%|██                       | 10/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  12%|███                      | 15/125 [00:05<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  16%|████                     | 20/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  17%|████▏                    | 21/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  20%|█████                    | 25/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  24%|██████                   | 30/125 [00:10<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  28%|███████                  | 35/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  32%|████████                 | 40/125 [00:14<00:30,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  42%|██████████▍              | 52/125 [00:18<00:26,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  44%|███████████              | 55/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  47%|███████████▊             | 59/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  53%|█████████████▏           | 66/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  64%|████████████████         | 80/125 [00:28<00:15,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  83%|███████████████████▉    | 104/125 [00:36<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  94%|██████████████████████▋ | 118/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 19/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Eval Accuracy: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 20/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 20/25:   1%|▏                        | 1/125 [00:01<02:04,  1.01s/batch, Loss=0.522]\u001b[A\n",
      "Epoch 20/25:   1%|▏                         | 1/125 [00:01<02:04,  1.01s/batch, Loss=0.54]\u001b[A\n",
      "Epoch 20/25:   2%|▍                         | 2/125 [00:01<01:57,  1.04batch/s, Loss=0.54]\u001b[A\n",
      "Epoch 20/25:   2%|▍                        | 2/125 [00:02<01:57,  1.04batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 20/25:   2%|▌                        | 3/125 [00:02<01:55,  1.06batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 20/25:   2%|▌                        | 3/125 [00:03<01:55,  1.06batch/s, Loss=0.234]\u001b[A\n",
      "Epoch 20/25:   3%|▊                        | 4/125 [00:03<01:53,  1.07batch/s, Loss=0.234]\u001b[A\n",
      "Epoch 20/25:   3%|▊                        | 4/125 [00:04<01:53,  1.07batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 20/25:   4%|█                        | 5/125 [00:04<01:51,  1.08batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 20/25:   4%|█                        | 5/125 [00:05<01:51,  1.08batch/s, Loss=0.332]\u001b[A\n",
      "Epoch 20/25:   5%|█▏                       | 6/125 [00:05<01:50,  1.08batch/s, Loss=0.332]\u001b[A\n",
      "Epoch 20/25:   5%|█▏                       | 6/125 [00:06<01:50,  1.08batch/s, Loss=0.748]\u001b[A\n",
      "Epoch 20/25:   6%|█▍                       | 7/125 [00:06<01:49,  1.08batch/s, Loss=0.748]\u001b[A\n",
      "Epoch 20/25:   6%|█▍                       | 7/125 [00:07<01:49,  1.08batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 20/25:   6%|█▌                       | 8/125 [00:07<01:48,  1.08batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 20/25:   6%|█▌                       | 8/125 [00:08<01:48,  1.08batch/s, Loss=0.711]\u001b[A\n",
      "Epoch 20/25:   7%|█▊                       | 9/125 [00:08<01:46,  1.09batch/s, Loss=0.711]\u001b[A\n",
      "Epoch 20/25:   7%|█▊                       | 9/125 [00:09<01:46,  1.09batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 20/25:   8%|█▉                      | 10/125 [00:09<01:45,  1.09batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 20/25:   8%|██                       | 10/125 [00:10<01:45,  1.09batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 20/25:   9%|██▏                      | 11/125 [00:10<01:44,  1.09batch/s, Loss=0.61]\u001b[A\n",
      "Epoch 20/25:   9%|██                      | 11/125 [00:11<01:44,  1.09batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 20/25:  10%|██▎                     | 12/125 [00:11<01:44,  1.09batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 20/25:  10%|██▎                     | 12/125 [00:12<01:44,  1.09batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 20/25:  10%|██▍                     | 13/125 [00:12<01:43,  1.09batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 20/25:  10%|██▍                     | 13/125 [00:12<01:43,  1.09batch/s, Loss=0.561]\u001b[A\n",
      "Epoch 20/25:  11%|██▋                     | 14/125 [00:12<01:42,  1.09batch/s, Loss=0.561]\u001b[A\n",
      "Epoch 20/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.09batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 20/25:  12%|██▉                     | 15/125 [00:13<01:41,  1.09batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 20/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 20/25:  13%|███                     | 16/125 [00:14<01:40,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 20/25:  13%|███▏                     | 16/125 [00:15<01:40,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 20/25:  14%|███▍                     | 17/125 [00:15<01:39,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 20/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 20/25:  14%|███▍                    | 18/125 [00:16<01:38,  1.09batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 20/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.09batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 20/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.09batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 20/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 20/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 20/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.09batch/s, Loss=0.749]\u001b[A\n",
      "Epoch 20/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.749]\u001b[A\n",
      "Epoch 20/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 20/25:  18%|████▏                   | 22/125 [00:20<01:34,  1.09batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 20/25:  18%|████▏                   | 22/125 [00:21<01:34,  1.09batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 20/25:  18%|████▍                   | 23/125 [00:21<01:33,  1.09batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 20/25:  18%|████▍                   | 23/125 [00:22<01:33,  1.09batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 20/25:  19%|████▌                   | 24/125 [00:22<01:32,  1.09batch/s, Loss=0.402]\u001b[A\n",
      "Epoch 20/25:  19%|████▌                   | 24/125 [00:23<01:32,  1.09batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 20/25:  20%|████▊                   | 25/125 [00:23<01:31,  1.09batch/s, Loss=0.526]\u001b[A\n",
      "Epoch 20/25:  20%|████▊                   | 25/125 [00:24<01:31,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 20/25:  21%|████▉                   | 26/125 [00:24<01:30,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 20/25:  21%|████▉                   | 26/125 [00:24<01:30,  1.09batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 20/25:  22%|█████▏                  | 27/125 [00:24<01:30,  1.09batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 20/25:  22%|█████▏                  | 27/125 [00:25<01:30,  1.09batch/s, Loss=0.825]\u001b[A\n",
      "Epoch 20/25:  22%|█████▍                  | 28/125 [00:25<01:29,  1.09batch/s, Loss=0.825]\u001b[A\n",
      "Epoch 20/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 20/25:  23%|█████▌                  | 29/125 [00:26<01:28,  1.09batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 20/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 20/25:  24%|█████▊                  | 30/125 [00:27<01:27,  1.09batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 20/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.502]\u001b[A\n",
      "Epoch 20/25:  25%|█████▉                  | 31/125 [00:28<01:26,  1.09batch/s, Loss=0.502]\u001b[A\n",
      "Epoch 20/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 20/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 20/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 20/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 20/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 20/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 20/25:  27%|██████▌                 | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 20/25:  28%|██████▋                 | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.523]\u001b[A\n",
      "Epoch 20/25:  28%|███████▎                  | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.7]\u001b[A\n",
      "Epoch 20/25:  29%|███████▍                  | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.7]\u001b[A\n",
      "Epoch 20/25:  29%|███████▏                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 20/25:  30%|███████▍                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 20/25:  30%|███████                 | 37/125 [00:35<01:20,  1.09batch/s, Loss=0.813]\u001b[A\n",
      "Epoch 20/25:  30%|███████▎                | 38/125 [00:35<01:19,  1.09batch/s, Loss=0.813]\u001b[A\n",
      "Epoch 20/25:  30%|███████▎                | 38/125 [00:35<01:19,  1.09batch/s, Loss=0.359]\u001b[A\n",
      "Epoch 20/25:  31%|███████▍                | 39/125 [00:35<01:18,  1.09batch/s, Loss=0.359]\u001b[A\n",
      "Epoch 20/25:  31%|███████▍                | 39/125 [00:36<01:18,  1.09batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 20/25:  32%|███████▋                | 40/125 [00:36<01:18,  1.09batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 20/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 20/25:  33%|███████▊                | 41/125 [00:37<01:17,  1.09batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 20/25:  33%|████████▌                 | 41/125 [00:38<01:17,  1.09batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 20/25:  34%|████████▋                 | 42/125 [00:38<01:16,  1.09batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 20/25:  34%|████████                | 42/125 [00:39<01:16,  1.09batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  34%|████████▎               | 43/125 [00:39<01:15,  1.09batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  34%|████████▉                 | 43/125 [00:40<01:15,  1.09batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 20/25:  35%|█████████▏                | 44/125 [00:40<01:14,  1.09batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 20/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.09batch/s, Loss=0.339]\u001b[A\n",
      "Epoch 20/25:  36%|████████▋               | 45/125 [00:41<01:13,  1.08batch/s, Loss=0.339]\u001b[A\n",
      "Epoch 20/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.08batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 20/25:  37%|████████▊               | 46/125 [00:42<01:13,  1.08batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 20/25:  37%|████████▊               | 46/125 [00:43<01:13,  1.08batch/s, Loss=0.308]\u001b[A\n",
      "Epoch 20/25:  38%|█████████               | 47/125 [00:43<01:12,  1.08batch/s, Loss=0.308]\u001b[A\n",
      "Epoch 20/25:  38%|█████████               | 47/125 [00:44<01:12,  1.08batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 20/25:  38%|█████████▏              | 48/125 [00:44<01:11,  1.08batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 20/25:  38%|█████████▏              | 48/125 [00:45<01:11,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  39%|█████████▍              | 49/125 [00:45<01:10,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  39%|█████████▍              | 49/125 [00:46<01:10,  1.08batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 20/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.507]\u001b[A\n",
      "Epoch 20/25:  40%|█████████▌              | 50/125 [00:47<01:09,  1.08batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 20/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 20/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 20/25:  42%|█████████▉              | 52/125 [00:47<01:07,  1.08batch/s, Loss=0.643]\u001b[A\n",
      "Epoch 20/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.08batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 20/25:  42%|██████████▏             | 53/125 [00:48<01:06,  1.09batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 20/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.09batch/s, Loss=0.281]\u001b[A\n",
      "Epoch 20/25:  43%|██████████▎             | 54/125 [00:49<01:05,  1.09batch/s, Loss=0.281]\u001b[A\n",
      "Epoch 20/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.09batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  44%|██████████▌             | 55/125 [00:50<01:04,  1.09batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 20/25:  45%|██████████▊             | 56/125 [00:51<01:03,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 20/25:  45%|██████████▊             | 56/125 [00:52<01:03,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 20/25:  46%|██████████▉             | 57/125 [00:52<01:02,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 20/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.09batch/s, Loss=0.367]\u001b[A\n",
      "Epoch 20/25:  46%|███████████▏            | 58/125 [00:53<01:01,  1.09batch/s, Loss=0.367]\u001b[A\n",
      "Epoch 20/25:  46%|███████████▌             | 58/125 [00:54<01:01,  1.09batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 20/25:  47%|███████████▊             | 59/125 [00:54<01:00,  1.09batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 20/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.09batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 20/25:  48%|███████████▌            | 60/125 [00:55<00:59,  1.09batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 20/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.09batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 20/25:  49%|███████████▋            | 61/125 [00:56<00:58,  1.09batch/s, Loss=0.649]\u001b[A\n",
      "Epoch 20/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 20/25:  50%|███████████▉            | 62/125 [00:57<00:58,  1.08batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 20/25:  50%|███████████▉            | 62/125 [00:58<00:58,  1.08batch/s, Loss=0.377]\u001b[A\n",
      "Epoch 20/25:  50%|████████████            | 63/125 [00:58<00:57,  1.08batch/s, Loss=0.377]\u001b[A\n",
      "Epoch 20/25:  50%|████████████            | 63/125 [00:59<00:57,  1.08batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 20/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.08batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 20/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.08batch/s, Loss=0.287]\u001b[A\n",
      "Epoch 20/25:  52%|████████████▍           | 65/125 [00:59<00:55,  1.08batch/s, Loss=0.287]\u001b[A\n",
      "Epoch 20/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.08batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 20/25:  53%|████████████▋           | 66/125 [01:00<00:55,  1.07batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 20/25:  53%|████████████▋           | 66/125 [01:01<00:55,  1.07batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 20/25:  54%|████████████▊           | 67/125 [01:01<00:53,  1.08batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 20/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.08batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 20/25:  54%|█████████████           | 68/125 [01:02<00:52,  1.08batch/s, Loss=0.625]\u001b[A\n",
      "Epoch 20/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  55%|█████████████▏          | 69/125 [01:03<00:51,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.08batch/s, Loss=0.389]\u001b[A\n",
      "Epoch 20/25:  56%|█████████████▍          | 70/125 [01:04<00:50,  1.08batch/s, Loss=0.389]\u001b[A\n",
      "Epoch 20/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  57%|█████████████▋          | 71/125 [01:05<00:49,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 20/25:  57%|█████████████▋          | 71/125 [01:06<00:49,  1.08batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 20/25:  58%|█████████████▊          | 72/125 [01:06<00:48,  1.08batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 20/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.08batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 20/25:  58%|██████████████          | 73/125 [01:07<00:47,  1.08batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 20/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.08batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 20/25:  59%|██████████████▏         | 74/125 [01:08<00:47,  1.08batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 20/25:  59%|██████████████▏         | 74/125 [01:09<00:47,  1.08batch/s, Loss=0.259]\u001b[A\n",
      "Epoch 20/25:  60%|██████████████▍         | 75/125 [01:09<00:46,  1.08batch/s, Loss=0.259]\u001b[A\n",
      "Epoch 20/25:  60%|██████████████▍         | 75/125 [01:10<00:46,  1.08batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 20/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 20/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 20/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.329]\u001b[A\n",
      "Epoch 20/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.328]\u001b[A\n",
      "Epoch 20/25:  62%|██████████████▉         | 78/125 [01:11<00:43,  1.09batch/s, Loss=0.328]\u001b[A\n",
      "Epoch 20/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 20/25:  63%|███████████████▏        | 79/125 [01:12<00:42,  1.09batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 20/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.718]\u001b[A\n",
      "Epoch 20/25:  64%|███████████████▎        | 80/125 [01:13<00:41,  1.09batch/s, Loss=0.718]\u001b[A\n",
      "Epoch 20/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 20/25:  65%|███████████████▌        | 81/125 [01:14<00:40,  1.09batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 20/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 20/25:  66%|███████████████▋        | 82/125 [01:15<00:39,  1.09batch/s, Loss=0.639]\u001b[A\n",
      "Epoch 20/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 20/25:  66%|███████████████▉        | 83/125 [01:16<00:38,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 20/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.09batch/s, Loss=0.378]\u001b[A\n",
      "Epoch 20/25:  67%|████████████████▏       | 84/125 [01:17<00:37,  1.09batch/s, Loss=0.378]\u001b[A\n",
      "Epoch 20/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.09batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 20/25:  68%|████████████████▎       | 85/125 [01:18<00:36,  1.09batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 20/25:  68%|████████████████▎       | 85/125 [01:19<00:36,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 20/25:  69%|████████████████▌       | 86/125 [01:19<00:35,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 20/25:  69%|████████████████▌       | 86/125 [01:20<00:35,  1.09batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 20/25:  70%|████████████████▋       | 87/125 [01:20<00:35,  1.08batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 20/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.08batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.08batch/s, Loss=0.432]\u001b[A\n",
      "Epoch 20/25:  70%|█████████████████▌       | 88/125 [01:22<00:34,  1.08batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 20/25:  71%|█████████████████▊       | 89/125 [01:22<00:33,  1.09batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 20/25:  71%|█████████████████▊       | 89/125 [01:22<00:33,  1.09batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 20/25:  72%|██████████████████       | 90/125 [01:22<00:32,  1.08batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 20/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.08batch/s, Loss=0.723]\u001b[A\n",
      "Epoch 20/25:  73%|█████████████████▍      | 91/125 [01:23<00:31,  1.08batch/s, Loss=0.723]\u001b[A\n",
      "Epoch 20/25:  73%|██████████████████▏      | 91/125 [01:24<00:31,  1.08batch/s, Loss=0.79]\u001b[A\n",
      "Epoch 20/25:  74%|██████████████████▍      | 92/125 [01:24<00:30,  1.08batch/s, Loss=0.79]\u001b[A\n",
      "Epoch 20/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.08batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 20/25:  74%|█████████████████▊      | 93/125 [01:25<00:29,  1.08batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 20/25:  74%|█████████████████▊      | 93/125 [01:26<00:29,  1.08batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 20/25:  75%|██████████████████      | 94/125 [01:26<00:28,  1.09batch/s, Loss=0.496]\u001b[A\n",
      "Epoch 20/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.09batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 20/25:  76%|██████████████████▏     | 95/125 [01:27<00:27,  1.09batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 20/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 20/25:  77%|██████████████████▍     | 96/125 [01:28<00:26,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 20/25:  77%|███████████████████▏     | 96/125 [01:29<00:26,  1.09batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 20/25:  78%|███████████████████▍     | 97/125 [01:29<00:25,  1.09batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 20/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.09batch/s, Loss=0.302]\u001b[A\n",
      "Epoch 20/25:  78%|██████████████████▊     | 98/125 [01:30<00:24,  1.09batch/s, Loss=0.302]\u001b[A\n",
      "Epoch 20/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.09batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 20/25:  79%|███████████████████     | 99/125 [01:31<00:23,  1.09batch/s, Loss=0.535]\u001b[A\n",
      "Epoch 20/25:  79%|███████████████████     | 99/125 [01:32<00:23,  1.09batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 20/25:  80%|██████████████████▍    | 100/125 [01:32<00:22,  1.09batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 20/25:  80%|██████████████████▍    | 100/125 [01:33<00:22,  1.09batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 20/25:  81%|██████████████████▌    | 101/125 [01:33<00:22,  1.09batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 20/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 20/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 20/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.09batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 20/25:  82%|██████████████████▉    | 103/125 [01:34<00:20,  1.09batch/s, Loss=0.668]\u001b[A\n",
      "Epoch 20/25:  82%|██████████████████▉    | 103/125 [01:35<00:20,  1.09batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 20/25:  83%|███████████████████▏   | 104/125 [01:35<00:19,  1.09batch/s, Loss=0.442]\u001b[A\n",
      "Epoch 20/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 20/25:  84%|███████████████████▎   | 105/125 [01:36<00:18,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 20/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.09batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 20/25:  85%|███████████████████▌   | 106/125 [01:37<00:17,  1.09batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 20/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 20/25:  86%|███████████████████▋   | 107/125 [01:38<00:16,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 20/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.09batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 20/25:  86%|███████████████████▊   | 108/125 [01:39<00:15,  1.09batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 20/25:  86%|███████████████████▊   | 108/125 [01:40<00:15,  1.09batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 20/25:  87%|████████████████████   | 109/125 [01:40<00:14,  1.09batch/s, Loss=0.691]\u001b[A\n",
      "Epoch 20/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.09batch/s, Loss=0.268]\u001b[A\n",
      "Epoch 20/25:  88%|████████████████████▏  | 110/125 [01:41<00:13,  1.09batch/s, Loss=0.268]\u001b[A\n",
      "Epoch 20/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 20/25:  89%|████████████████████▍  | 111/125 [01:42<00:12,  1.09batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 20/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.09batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 20/25:  90%|████████████████████▌  | 112/125 [01:43<00:11,  1.09batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 20/25:  90%|████████████████████▌  | 112/125 [01:44<00:11,  1.09batch/s, Loss=0.178]\u001b[A\n",
      "Epoch 20/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.09batch/s, Loss=0.178]\u001b[A\n",
      "Epoch 20/25:  90%|█████████████████████▋  | 113/125 [01:45<00:11,  1.09batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 20/25:  91%|█████████████████████▉  | 114/125 [01:45<00:10,  1.09batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 20/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 20/25:  92%|█████████████████████▏ | 115/125 [01:45<00:09,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 20/25:  92%|██████████████████████  | 115/125 [01:46<00:09,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 20/25:  93%|██████████████████████▎ | 116/125 [01:46<00:08,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 20/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.347]\u001b[A\n",
      "Epoch 20/25:  94%|█████████████████████▌ | 117/125 [01:47<00:07,  1.09batch/s, Loss=0.347]\u001b[A\n",
      "Epoch 20/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 20/25:  94%|█████████████████████▋ | 118/125 [01:48<00:06,  1.09batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 20/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 20/25:  95%|█████████████████████▉ | 119/125 [01:49<00:05,  1.09batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 20/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 20/25:  96%|██████████████████████ | 120/125 [01:50<00:04,  1.09batch/s, Loss=0.411]\u001b[A\n",
      "Epoch 20/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 20/25:  97%|██████████████████████▎| 121/125 [01:51<00:03,  1.09batch/s, Loss=0.578]\u001b[A\n",
      "Epoch 20/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 20/25:  98%|██████████████████████▍| 122/125 [01:52<00:02,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 20/25:  98%|████████████████████████▍| 122/125 [01:53<00:02,  1.09batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 20/25:  98%|████████████████████████▌| 123/125 [01:53<00:01,  1.09batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 20/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.09batch/s, Loss=0.321]\u001b[A\n",
      "Epoch 20/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.09batch/s, Loss=0.321]\u001b[A\n",
      "Epoch 20/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.09batch/s, Loss=0.634]\u001b[A\n",
      "Epoch 20/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.09batch/s, Loss=0.634]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 20/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   1%|▏                         | 1/125 [00:00<00:44,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   2%|▍                         | 2/125 [00:00<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   2%|▌                         | 3/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   3%|▊                         | 4/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   4%|█                         | 5/125 [00:01<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   8%|██                       | 10/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  12%|███                      | 15/125 [00:05<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  16%|████                     | 20/125 [00:07<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  20%|█████                    | 25/125 [00:08<00:35,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  24%|██████                   | 30/125 [00:10<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  28%|███████                  | 35/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  32%|████████                 | 40/125 [00:14<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  40%|██████████               | 50/125 [00:17<00:26,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  44%|███████████              | 55/125 [00:19<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  48%|████████████             | 60/125 [00:21<00:23,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  53%|█████████████▏           | 66/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  64%|████████████████         | 80/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  71%|█████████████████▊       | 89/125 [00:31<00:13,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.73batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  74%|██████████████████▍      | 92/125 [00:32<00:12,  2.74batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  78%|███████████████████▍     | 97/125 [00:34<00:10,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  92%|██████████████████████  | 115/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 20/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.80batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Eval Accuracy: 0.7780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 21/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.713]\u001b[A\n",
      "Epoch 21/25:   1%|▏                        | 1/125 [00:01<02:59,  1.45s/batch, Loss=0.713]\u001b[A\n",
      "Epoch 21/25:   1%|▏                        | 1/125 [00:02<02:59,  1.45s/batch, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:   2%|▍                        | 2/125 [00:02<02:20,  1.14s/batch, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:   2%|▍                        | 2/125 [00:03<02:20,  1.14s/batch, Loss=0.481]\u001b[A\n",
      "Epoch 21/25:   2%|▌                        | 3/125 [00:03<02:06,  1.04s/batch, Loss=0.481]\u001b[A\n",
      "Epoch 21/25:   2%|▌                        | 3/125 [00:04<02:06,  1.04s/batch, Loss=0.656]\u001b[A\n",
      "Epoch 21/25:   3%|▊                        | 4/125 [00:04<02:00,  1.01batch/s, Loss=0.656]\u001b[A\n",
      "Epoch 21/25:   3%|▊                        | 4/125 [00:05<02:00,  1.01batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 21/25:   4%|█                        | 5/125 [00:05<01:55,  1.04batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 21/25:   4%|█                        | 5/125 [00:06<01:55,  1.04batch/s, Loss=0.717]\u001b[A\n",
      "Epoch 21/25:   5%|█▏                       | 6/125 [00:06<01:53,  1.05batch/s, Loss=0.717]\u001b[A\n",
      "Epoch 21/25:   5%|█▏                       | 6/125 [00:06<01:53,  1.05batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 21/25:   6%|█▍                       | 7/125 [00:06<01:50,  1.06batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 21/25:   6%|█▍                        | 7/125 [00:07<01:50,  1.06batch/s, Loss=0.27]\u001b[A\n",
      "Epoch 21/25:   6%|█▋                        | 8/125 [00:07<01:49,  1.07batch/s, Loss=0.27]\u001b[A\n",
      "Epoch 21/25:   6%|█▌                       | 8/125 [00:08<01:49,  1.07batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 21/25:   7%|█▊                       | 9/125 [00:08<01:47,  1.08batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 21/25:   7%|█▊                        | 9/125 [00:09<01:47,  1.08batch/s, Loss=0.33]\u001b[A\n",
      "Epoch 21/25:   8%|██                       | 10/125 [00:09<01:47,  1.07batch/s, Loss=0.33]\u001b[A\n",
      "Epoch 21/25:   8%|█▉                      | 10/125 [00:10<01:47,  1.07batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 21/25:   9%|██                      | 11/125 [00:10<01:45,  1.08batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 21/25:   9%|██                      | 11/125 [00:11<01:45,  1.08batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 21/25:  10%|██▎                     | 12/125 [00:11<01:44,  1.08batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 21/25:  10%|██▎                     | 12/125 [00:12<01:44,  1.08batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 21/25:  10%|██▍                     | 13/125 [00:12<01:43,  1.08batch/s, Loss=0.607]\u001b[A\n",
      "Epoch 21/25:  10%|██▍                     | 13/125 [00:13<01:43,  1.08batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 21/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.08batch/s, Loss=0.474]\u001b[A\n",
      "Epoch 21/25:  11%|██▋                     | 14/125 [00:14<01:42,  1.08batch/s, Loss=0.202]\u001b[A\n",
      "Epoch 21/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.202]\u001b[A\n",
      "Epoch 21/25:  12%|██▉                     | 15/125 [00:15<01:41,  1.08batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:  13%|███                     | 16/125 [00:15<01:40,  1.08batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:  13%|███                     | 16/125 [00:16<01:40,  1.08batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 21/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.08batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 21/25:  14%|███▍                     | 17/125 [00:17<01:39,  1.08batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 21/25:  14%|███▌                     | 18/125 [00:17<01:38,  1.08batch/s, Loss=0.46]\u001b[A\n",
      "Epoch 21/25:  14%|███▍                    | 18/125 [00:18<01:38,  1.08batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 21/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.08batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 21/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.08batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 21/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.08batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 21/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.08batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 21/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 21/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 21/25:  18%|████▏                   | 22/125 [00:20<01:35,  1.08batch/s, Loss=0.653]\u001b[A\n",
      "Epoch 21/25:  18%|████▏                   | 22/125 [00:21<01:35,  1.08batch/s, Loss=0.842]\u001b[A\n",
      "Epoch 21/25:  18%|████▍                   | 23/125 [00:21<01:34,  1.08batch/s, Loss=0.842]\u001b[A\n",
      "Epoch 21/25:  18%|████▍                   | 23/125 [00:22<01:34,  1.08batch/s, Loss=0.759]\u001b[A\n",
      "Epoch 21/25:  19%|████▌                   | 24/125 [00:22<01:33,  1.08batch/s, Loss=0.759]\u001b[A\n",
      "Epoch 21/25:  19%|████▊                    | 24/125 [00:23<01:33,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 21/25:  20%|█████                    | 25/125 [00:23<01:32,  1.09batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 21/25:  20%|████▊                   | 25/125 [00:24<01:32,  1.09batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 21/25:  21%|████▉                   | 26/125 [00:24<01:31,  1.09batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 21/25:  21%|█████▏                   | 26/125 [00:25<01:31,  1.09batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 21/25:  22%|█████▍                   | 27/125 [00:25<01:30,  1.08batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 21/25:  22%|█████▏                  | 27/125 [00:26<01:30,  1.08batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 21/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.08batch/s, Loss=0.491]\u001b[A\n",
      "Epoch 21/25:  22%|█████▍                  | 28/125 [00:27<01:29,  1.08batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 21/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.08batch/s, Loss=0.664]\u001b[A\n",
      "Epoch 21/25:  23%|█████▌                  | 29/125 [00:28<01:28,  1.08batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 21/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.08batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 21/25:  24%|█████▊                  | 30/125 [00:29<01:27,  1.08batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 21/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.08batch/s, Loss=0.572]\u001b[A\n",
      "Epoch 21/25:  25%|█████▉                  | 31/125 [00:30<01:26,  1.08batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 21/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.08batch/s, Loss=0.654]\u001b[A\n",
      "Epoch 21/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.08batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 21/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 21/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 21/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 21/25:  27%|██████▌                 | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 21/25:  28%|██████▋                 | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.673]\u001b[A\n",
      "Epoch 21/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 21/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 21/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 21/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.588]\u001b[A\n",
      "Epoch 21/25:  30%|███████                 | 37/125 [00:35<01:20,  1.09batch/s, Loss=0.671]\u001b[A\n",
      "Epoch 21/25:  30%|███████▎                | 38/125 [00:35<01:20,  1.09batch/s, Loss=0.671]\u001b[A\n",
      "Epoch 21/25:  30%|███████▎                | 38/125 [00:36<01:20,  1.09batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 21/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.09batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 21/25:  31%|███████▍                | 39/125 [00:37<01:19,  1.09batch/s, Loss=0.886]\u001b[A\n",
      "Epoch 21/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.886]\u001b[A\n",
      "Epoch 21/25:  32%|███████▋                | 40/125 [00:38<01:18,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 21/25:  33%|███████▊                | 41/125 [00:38<01:17,  1.09batch/s, Loss=0.636]\u001b[A\n",
      "Epoch 21/25:  33%|███████▊                | 41/125 [00:39<01:17,  1.09batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 21/25:  34%|████████                | 42/125 [00:39<01:16,  1.09batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 21/25:  34%|████████                | 42/125 [00:40<01:16,  1.09batch/s, Loss=0.804]\u001b[A\n",
      "Epoch 21/25:  34%|████████▎               | 43/125 [00:40<01:15,  1.08batch/s, Loss=0.804]\u001b[A\n",
      "Epoch 21/25:  34%|████████▎               | 43/125 [00:41<01:15,  1.08batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 21/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.09batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 21/25:  35%|████████▍               | 44/125 [00:42<01:14,  1.09batch/s, Loss=0.322]\u001b[A\n",
      "Epoch 21/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.09batch/s, Loss=0.322]\u001b[A\n",
      "Epoch 21/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.09batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 21/25:  37%|████████▊               | 46/125 [00:42<01:12,  1.09batch/s, Loss=0.388]\u001b[A\n",
      "Epoch 21/25:  37%|████████▊               | 46/125 [00:43<01:12,  1.09batch/s, Loss=0.236]\u001b[A\n",
      "Epoch 21/25:  38%|█████████               | 47/125 [00:43<01:11,  1.09batch/s, Loss=0.236]\u001b[A\n",
      "Epoch 21/25:  38%|█████████               | 47/125 [00:44<01:11,  1.09batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 21/25:  38%|█████████▏              | 48/125 [00:44<01:10,  1.09batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 21/25:  38%|█████████▏              | 48/125 [00:45<01:10,  1.09batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 21/25:  39%|█████████▍              | 49/125 [00:45<01:09,  1.09batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 21/25:  39%|█████████▍              | 49/125 [00:46<01:09,  1.09batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 21/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.09batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 21/25:  40%|█████████▌              | 50/125 [00:47<01:09,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 21/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 21/25:  41%|█████████▊              | 51/125 [00:48<01:08,  1.09batch/s, Loss=0.302]\u001b[A\n",
      "Epoch 21/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.08batch/s, Loss=0.302]\u001b[A\n",
      "Epoch 21/25:  42%|█████████▉              | 52/125 [00:49<01:07,  1.08batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 21/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.08batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 21/25:  42%|██████████▏             | 53/125 [00:50<01:06,  1.08batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 21/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 21/25:  43%|██████████▎             | 54/125 [00:51<01:05,  1.08batch/s, Loss=0.304]\u001b[A\n",
      "Epoch 21/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.08batch/s, Loss=0.304]\u001b[A\n",
      "Epoch 21/25:  44%|██████████▌             | 55/125 [00:52<01:04,  1.08batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 21/25:  45%|██████████▊             | 56/125 [00:52<01:03,  1.08batch/s, Loss=0.542]\u001b[A\n",
      "Epoch 21/25:  45%|██████████▊             | 56/125 [00:53<01:03,  1.08batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 21/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.08batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 21/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.08batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 21/25:  46%|███████████▏            | 58/125 [00:53<01:01,  1.08batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 21/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.08batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:  47%|███████████▎            | 59/125 [00:54<01:01,  1.08batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 21/25:  47%|███████████▎            | 59/125 [00:55<01:01,  1.08batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 21/25:  48%|███████████▌            | 60/125 [00:55<01:00,  1.08batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 21/25:  48%|███████████▌            | 60/125 [00:56<01:00,  1.08batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 21/25:  49%|███████████▋            | 61/125 [00:56<00:59,  1.07batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 21/25:  49%|███████████▋            | 61/125 [00:57<00:59,  1.07batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 21/25:  50%|███████████▉            | 62/125 [00:57<00:58,  1.08batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 21/25:  50%|███████████▉            | 62/125 [00:58<00:58,  1.08batch/s, Loss=0.347]\u001b[A\n",
      "Epoch 21/25:  50%|████████████            | 63/125 [00:58<00:57,  1.08batch/s, Loss=0.347]\u001b[A\n",
      "Epoch 21/25:  50%|████████████            | 63/125 [00:59<00:57,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 21/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 21/25:  51%|████████████▎           | 64/125 [01:00<00:56,  1.08batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 21/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.08batch/s, Loss=0.482]\u001b[A\n",
      "Epoch 21/25:  52%|████████████▍           | 65/125 [01:01<00:55,  1.08batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 21/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.08batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 21/25:  53%|████████████▋           | 66/125 [01:02<00:54,  1.08batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 21/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.08batch/s, Loss=0.394]\u001b[A\n",
      "Epoch 21/25:  54%|████████████▊           | 67/125 [01:03<00:53,  1.08batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 21/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 21/25:  54%|█████████████           | 68/125 [01:04<00:52,  1.09batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 21/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.594]\u001b[A\n",
      "Epoch 21/25:  55%|█████████████▏          | 69/125 [01:05<00:51,  1.09batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 21/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 21/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 21/25:  57%|█████████████▋          | 71/125 [01:05<00:49,  1.09batch/s, Loss=0.453]\u001b[A\n",
      "Epoch 21/25:  57%|█████████████▋          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.195]\u001b[A\n",
      "Epoch 21/25:  58%|█████████████▊          | 72/125 [01:06<00:48,  1.09batch/s, Loss=0.195]\u001b[A\n",
      "Epoch 21/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 21/25:  58%|██████████████          | 73/125 [01:07<00:47,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 21/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.403]\u001b[A\n",
      "Epoch 21/25:  59%|██████████████▏         | 74/125 [01:08<00:46,  1.09batch/s, Loss=0.403]\u001b[A\n",
      "Epoch 21/25:  59%|██████████████▏         | 74/125 [01:09<00:46,  1.09batch/s, Loss=0.291]\u001b[A\n",
      "Epoch 21/25:  60%|██████████████▍         | 75/125 [01:09<00:45,  1.09batch/s, Loss=0.291]\u001b[A\n",
      "Epoch 21/25:  60%|██████████████▍         | 75/125 [01:10<00:45,  1.09batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 21/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 21/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 21/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 21/25:  62%|██████████████▊         | 77/125 [01:12<00:44,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 21/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 21/25:  62%|██████████████▉         | 78/125 [01:13<00:43,  1.09batch/s, Loss=0.771]\u001b[A\n",
      "Epoch 21/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.771]\u001b[A\n",
      "Epoch 21/25:  63%|███████████████▏        | 79/125 [01:14<00:42,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 21/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 21/25:  64%|███████████████▎        | 80/125 [01:15<00:41,  1.09batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 21/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.493]\u001b[A\n",
      "Epoch 21/25:  65%|███████████████▌        | 81/125 [01:16<00:40,  1.09batch/s, Loss=0.637]\u001b[A\n",
      "Epoch 21/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.09batch/s, Loss=0.637]\u001b[A\n",
      "Epoch 21/25:  66%|███████████████▋        | 82/125 [01:17<00:39,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 21/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 21/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 21/25:  67%|████████████████▏       | 84/125 [01:17<00:37,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 21/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.09batch/s, Loss=0.745]\u001b[A\n",
      "Epoch 21/25:  68%|████████████████▎       | 85/125 [01:18<00:36,  1.09batch/s, Loss=0.745]\u001b[A\n",
      "Epoch 21/25:  68%|████████████████▎       | 85/125 [01:19<00:36,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 21/25:  69%|████████████████▌       | 86/125 [01:19<00:35,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 21/25:  69%|████████████████▌       | 86/125 [01:20<00:35,  1.09batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 21/25:  70%|████████████████▋       | 87/125 [01:20<00:35,  1.08batch/s, Loss=0.709]\u001b[A\n",
      "Epoch 21/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.08batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 21/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.08batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 21/25:  70%|████████████████▉       | 88/125 [01:22<00:34,  1.08batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 21/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.08batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 21/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.08batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 21/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.08batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 21/25:  72%|█████████████████▎      | 90/125 [01:24<00:32,  1.08batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 21/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.08batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 21/25:  73%|█████████████████▍      | 91/125 [01:25<00:31,  1.08batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 21/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.08batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 21/25:  74%|█████████████████▋      | 92/125 [01:26<00:30,  1.08batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 21/25:  74%|█████████████████▊      | 93/125 [01:26<00:29,  1.08batch/s, Loss=0.323]\u001b[A\n",
      "Epoch 21/25:  74%|█████████████████▊      | 93/125 [01:27<00:29,  1.08batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 21/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.08batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 21/25:  75%|██████████████████      | 94/125 [01:28<00:28,  1.08batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 21/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.08batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 21/25:  76%|██████████████████▏     | 95/125 [01:29<00:27,  1.08batch/s, Loss=0.304]\u001b[A\n",
      "Epoch 21/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.08batch/s, Loss=0.304]\u001b[A\n",
      "Epoch 21/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.08batch/s, Loss=0.247]\u001b[A\n",
      "Epoch 21/25:  78%|██████████████████▌     | 97/125 [01:29<00:25,  1.08batch/s, Loss=0.247]\u001b[A\n",
      "Epoch 21/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.08batch/s, Loss=0.743]\u001b[A\n",
      "Epoch 21/25:  78%|██████████████████▊     | 98/125 [01:30<00:24,  1.08batch/s, Loss=0.743]\u001b[A\n",
      "Epoch 21/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.08batch/s, Loss=0.316]\u001b[A\n",
      "Epoch 21/25:  79%|███████████████████     | 99/125 [01:31<00:24,  1.08batch/s, Loss=0.316]\u001b[A\n",
      "Epoch 21/25:  79%|███████████████████     | 99/125 [01:32<00:24,  1.08batch/s, Loss=0.224]\u001b[A\n",
      "Epoch 21/25:  80%|██████████████████▍    | 100/125 [01:32<00:23,  1.08batch/s, Loss=0.224]\u001b[A\n",
      "Epoch 21/25:  80%|██████████████████▍    | 100/125 [01:33<00:23,  1.08batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 21/25:  81%|██████████████████▌    | 101/125 [01:33<00:22,  1.08batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 21/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.08batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 21/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.08batch/s, Loss=0.592]\u001b[A\n",
      "Epoch 21/25:  82%|██████████████████▊    | 102/125 [01:35<00:21,  1.08batch/s, Loss=0.725]\u001b[A\n",
      "Epoch 21/25:  82%|██████████████████▉    | 103/125 [01:35<00:20,  1.08batch/s, Loss=0.725]\u001b[A\n",
      "Epoch 21/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.08batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 21/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.08batch/s, Loss=0.486]\u001b[A\n",
      "Epoch 21/25:  83%|███████████████████▏   | 104/125 [01:37<00:19,  1.08batch/s, Loss=0.232]\u001b[A\n",
      "Epoch 21/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.08batch/s, Loss=0.232]\u001b[A\n",
      "Epoch 21/25:  84%|████████████████████▏   | 105/125 [01:38<00:18,  1.08batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 21/25:  85%|████████████████████▎   | 106/125 [01:38<00:17,  1.09batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 21/25:  85%|███████████████████▌   | 106/125 [01:39<00:17,  1.09batch/s, Loss=0.287]\u001b[A\n",
      "Epoch 21/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.08batch/s, Loss=0.287]\u001b[A\n",
      "Epoch 21/25:  86%|████████████████████▌   | 107/125 [01:40<00:16,  1.08batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 21/25:  86%|████████████████████▋   | 108/125 [01:40<00:15,  1.08batch/s, Loss=0.31]\u001b[A\n",
      "Epoch 21/25:  86%|███████████████████▊   | 108/125 [01:41<00:15,  1.08batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 21/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.08batch/s, Loss=0.659]\u001b[A\n",
      "Epoch 21/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.08batch/s, Loss=0.509]\u001b[A\n",
      "Epoch 21/25:  88%|████████████████████▏  | 110/125 [01:41<00:13,  1.09batch/s, Loss=0.509]\u001b[A\n",
      "Epoch 21/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 21/25:  89%|████████████████████▍  | 111/125 [01:42<00:12,  1.09batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 21/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.09batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 21/25:  90%|████████████████████▌  | 112/125 [01:43<00:11,  1.09batch/s, Loss=0.548]\u001b[A\n",
      "Epoch 21/25:  90%|████████████████████▌  | 112/125 [01:44<00:11,  1.09batch/s, Loss=0.291]\u001b[A\n",
      "Epoch 21/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.09batch/s, Loss=0.291]\u001b[A\n",
      "Epoch 21/25:  90%|████████████████████▊  | 113/125 [01:45<00:11,  1.09batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 21/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.09batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 21/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.09batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 21/25:  92%|█████████████████████▏ | 115/125 [01:46<00:09,  1.09batch/s, Loss=0.531]\u001b[A\n",
      "Epoch 21/25:  92%|█████████████████████▏ | 115/125 [01:47<00:09,  1.09batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 21/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.582]\u001b[A\n",
      "Epoch 21/25:  93%|█████████████████████▎ | 116/125 [01:48<00:08,  1.09batch/s, Loss=0.242]\u001b[A\n",
      "Epoch 21/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.242]\u001b[A\n",
      "Epoch 21/25:  94%|█████████████████████▌ | 117/125 [01:49<00:07,  1.09batch/s, Loss=0.239]\u001b[A\n",
      "Epoch 21/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.239]\u001b[A\n",
      "Epoch 21/25:  94%|█████████████████████▋ | 118/125 [01:50<00:06,  1.09batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 21/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.439]\u001b[A\n",
      "Epoch 21/25:  95%|█████████████████████▉ | 119/125 [01:51<00:05,  1.09batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 21/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.522]\u001b[A\n",
      "Epoch 21/25:  96%|██████████████████████ | 120/125 [01:52<00:04,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 21/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.08batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 21/25:  97%|██████████████████████▎| 121/125 [01:53<00:03,  1.08batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 21/25:  98%|██████████████████████▍| 122/125 [01:53<00:02,  1.08batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 21/25:  98%|██████████████████████▍| 122/125 [01:53<00:02,  1.08batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 21/25:  98%|██████████████████████▋| 123/125 [01:53<00:01,  1.08batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 21/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.08batch/s, Loss=0.888]\u001b[A\n",
      "Epoch 21/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.08batch/s, Loss=0.888]\u001b[A\n",
      "Epoch 21/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.08batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 21/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.08batch/s, Loss=0.524]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Train Loss: 0.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 21/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   1%|▏                         | 1/125 [00:00<00:44,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   2%|▍                         | 2/125 [00:00<00:44,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   2%|▌                         | 3/125 [00:01<00:43,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   3%|▊                         | 4/125 [00:01<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   4%|█                         | 5/125 [00:01<00:42,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   8%|██                       | 10/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  12%|███                      | 15/125 [00:05<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  16%|████                     | 20/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  20%|█████                    | 25/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  24%|██████                   | 30/125 [00:10<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  28%|███████                  | 35/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  32%|████████                 | 40/125 [00:14<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  44%|███████████              | 55/125 [00:19<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  47%|███████████▊             | 59/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  53%|█████████████▏           | 66/125 [00:23<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  55%|█████████████▊           | 69/125 [00:24<00:20,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  64%|████████████████         | 80/125 [00:28<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 21/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Eval Accuracy: 0.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 22/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.457]\u001b[A\n",
      "Epoch 22/25:   1%|▏                        | 1/125 [00:01<02:28,  1.19s/batch, Loss=0.457]\u001b[A\n",
      "Epoch 22/25:   1%|▏                        | 1/125 [00:02<02:28,  1.19s/batch, Loss=0.199]\u001b[A\n",
      "Epoch 22/25:   2%|▍                        | 2/125 [00:02<02:06,  1.03s/batch, Loss=0.199]\u001b[A\n",
      "Epoch 22/25:   2%|▍                        | 2/125 [00:03<02:06,  1.03s/batch, Loss=0.246]\u001b[A\n",
      "Epoch 22/25:   2%|▌                        | 3/125 [00:03<01:59,  1.02batch/s, Loss=0.246]\u001b[A\n",
      "Epoch 22/25:   2%|▌                        | 3/125 [00:03<01:59,  1.02batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 22/25:   3%|▊                        | 4/125 [00:03<01:55,  1.05batch/s, Loss=0.499]\u001b[A\n",
      "Epoch 22/25:   3%|▊                         | 4/125 [00:04<01:55,  1.05batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 22/25:   4%|█                         | 5/125 [00:04<01:53,  1.06batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 22/25:   4%|█                        | 5/125 [00:05<01:53,  1.06batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 22/25:   5%|█▏                       | 6/125 [00:05<01:51,  1.07batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 22/25:   5%|█▏                       | 6/125 [00:06<01:51,  1.07batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 22/25:   6%|█▍                       | 7/125 [00:06<01:49,  1.08batch/s, Loss=0.438]\u001b[A\n",
      "Epoch 22/25:   6%|█▍                       | 7/125 [00:07<01:49,  1.08batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 22/25:   6%|█▌                       | 8/125 [00:07<01:48,  1.08batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 22/25:   6%|█▌                       | 8/125 [00:08<01:48,  1.08batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 22/25:   7%|█▊                       | 9/125 [00:08<01:47,  1.08batch/s, Loss=0.595]\u001b[A\n",
      "Epoch 22/25:   7%|█▊                       | 9/125 [00:09<01:47,  1.08batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 22/25:   8%|█▉                      | 10/125 [00:09<01:46,  1.08batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 22/25:   8%|█▉                      | 10/125 [00:10<01:46,  1.08batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 22/25:   9%|██                      | 11/125 [00:10<01:45,  1.09batch/s, Loss=0.651]\u001b[A\n",
      "Epoch 22/25:   9%|██                      | 11/125 [00:11<01:45,  1.09batch/s, Loss=0.331]\u001b[A\n",
      "Epoch 22/25:  10%|██▎                     | 12/125 [00:11<01:44,  1.09batch/s, Loss=0.331]\u001b[A\n",
      "Epoch 22/25:  10%|██▍                      | 12/125 [00:12<01:44,  1.09batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 22/25:  10%|██▌                      | 13/125 [00:12<01:43,  1.09batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 22/25:  10%|██▍                     | 13/125 [00:13<01:43,  1.09batch/s, Loss=0.856]\u001b[A\n",
      "Epoch 22/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.08batch/s, Loss=0.856]\u001b[A\n",
      "Epoch 22/25:  11%|██▊                      | 14/125 [00:14<01:42,  1.08batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 22/25:  12%|███                      | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 22/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 22/25:  13%|███                     | 16/125 [00:14<01:40,  1.09batch/s, Loss=0.608]\u001b[A\n",
      "Epoch 22/25:  13%|███▏                     | 16/125 [00:15<01:40,  1.09batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 22/25:  14%|███▍                     | 17/125 [00:15<01:39,  1.09batch/s, Loss=0.57]\u001b[A\n",
      "Epoch 22/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.904]\u001b[A\n",
      "Epoch 22/25:  14%|███▍                    | 18/125 [00:16<01:38,  1.09batch/s, Loss=0.904]\u001b[A\n",
      "Epoch 22/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.09batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 22/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.09batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 22/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.09batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 22/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.09batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 22/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.09batch/s, Loss=0.183]\u001b[A\n",
      "Epoch 22/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.183]\u001b[A\n",
      "Epoch 22/25:  17%|████▏                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 22/25:  18%|████▍                    | 22/125 [00:20<01:35,  1.08batch/s, Loss=0.66]\u001b[A\n",
      "Epoch 22/25:  18%|████▏                   | 22/125 [00:21<01:35,  1.08batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 22/25:  18%|████▍                   | 23/125 [00:21<01:34,  1.08batch/s, Loss=0.681]\u001b[A\n",
      "Epoch 22/25:  18%|████▍                   | 23/125 [00:22<01:34,  1.08batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 22/25:  19%|████▌                   | 24/125 [00:22<01:33,  1.08batch/s, Loss=0.534]\u001b[A\n",
      "Epoch 22/25:  19%|████▌                   | 24/125 [00:23<01:33,  1.08batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 22/25:  20%|████▊                   | 25/125 [00:23<01:32,  1.08batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 22/25:  20%|████▊                   | 25/125 [00:24<01:32,  1.08batch/s, Loss=0.221]\u001b[A\n",
      "Epoch 22/25:  21%|████▉                   | 26/125 [00:24<01:31,  1.08batch/s, Loss=0.221]\u001b[A\n",
      "Epoch 22/25:  21%|████▉                   | 26/125 [00:25<01:31,  1.08batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 22/25:  22%|█████▏                  | 27/125 [00:25<01:30,  1.09batch/s, Loss=0.576]\u001b[A\n",
      "Epoch 22/25:  22%|█████▏                  | 27/125 [00:26<01:30,  1.09batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 22/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 22/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 22/25:  23%|█████▌                  | 29/125 [00:26<01:28,  1.09batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 22/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 22/25:  24%|█████▊                  | 30/125 [00:27<01:27,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 22/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.331]\u001b[A\n",
      "Epoch 22/25:  25%|█████▉                  | 31/125 [00:28<01:26,  1.09batch/s, Loss=0.331]\u001b[A\n",
      "Epoch 22/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 22/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 22/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 22/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 22/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.609]\u001b[A\n",
      "Epoch 22/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.609]\u001b[A\n",
      "Epoch 22/25:  27%|██████▊                  | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 22/25:  28%|███████                  | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 22/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 22/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 22/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.634]\u001b[A\n",
      "Epoch 22/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.634]\u001b[A\n",
      "Epoch 22/25:  30%|███████                 | 37/125 [00:35<01:20,  1.09batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 22/25:  30%|███████▎                | 38/125 [00:35<01:19,  1.09batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 22/25:  30%|███████▎                | 38/125 [00:36<01:19,  1.09batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 22/25:  31%|███████▍                | 39/125 [00:36<01:18,  1.09batch/s, Loss=0.632]\u001b[A\n",
      "Epoch 22/25:  31%|███████▍                | 39/125 [00:37<01:18,  1.09batch/s, Loss=0.637]\u001b[A\n",
      "Epoch 22/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.637]\u001b[A\n",
      "Epoch 22/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 22/25:  33%|███████▊                | 41/125 [00:37<01:17,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 22/25:  33%|███████▊                | 41/125 [00:38<01:17,  1.09batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 22/25:  34%|████████                | 42/125 [00:38<01:16,  1.09batch/s, Loss=0.326]\u001b[A\n",
      "Epoch 22/25:  34%|████████                | 42/125 [00:39<01:16,  1.09batch/s, Loss=0.714]\u001b[A\n",
      "Epoch 22/25:  34%|████████▎               | 43/125 [00:39<01:15,  1.09batch/s, Loss=0.714]\u001b[A\n",
      "Epoch 22/25:  34%|████████▎               | 43/125 [00:40<01:15,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 22/25:  35%|████████▍               | 44/125 [00:40<01:14,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 22/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 22/25:  36%|████████▋               | 45/125 [00:41<01:13,  1.08batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 22/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.08batch/s, Loss=0.262]\u001b[A\n",
      "Epoch 22/25:  37%|████████▊               | 46/125 [00:42<01:12,  1.08batch/s, Loss=0.262]\u001b[A\n",
      "Epoch 22/25:  37%|█████████▌                | 46/125 [00:43<01:12,  1.08batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 22/25:  38%|█████████▊                | 47/125 [00:43<01:12,  1.08batch/s, Loss=0.5]\u001b[A\n",
      "Epoch 22/25:  38%|█████████               | 47/125 [00:44<01:12,  1.08batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 22/25:  38%|█████████▏              | 48/125 [00:44<01:11,  1.08batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 22/25:  38%|█████████▏              | 48/125 [00:45<01:11,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 22/25:  39%|█████████▍              | 49/125 [00:45<01:10,  1.08batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 22/25:  39%|█████████▍              | 49/125 [00:46<01:10,  1.08batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 22/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 22/25:  40%|█████████▌              | 50/125 [00:47<01:09,  1.08batch/s, Loss=0.429]\u001b[A\n",
      "Epoch 22/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.429]\u001b[A\n",
      "Epoch 22/25:  41%|█████████▊              | 51/125 [00:48<01:08,  1.08batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 22/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.08batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 22/25:  42%|██████████▍              | 52/125 [00:49<01:07,  1.08batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 22/25:  42%|██████████▌              | 53/125 [00:49<01:06,  1.08batch/s, Loss=0.45]\u001b[A\n",
      "Epoch 22/25:  42%|██████████▏             | 53/125 [00:50<01:06,  1.08batch/s, Loss=0.225]\u001b[A\n",
      "Epoch 22/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.225]\u001b[A\n",
      "Epoch 22/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.926]\u001b[A\n",
      "Epoch 22/25:  44%|██████████▌             | 55/125 [00:50<01:04,  1.09batch/s, Loss=0.926]\u001b[A\n",
      "Epoch 22/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.09batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 22/25:  45%|██████████▊             | 56/125 [00:51<01:03,  1.09batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 22/25:  45%|███████████▏             | 56/125 [00:52<01:03,  1.09batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 22/25:  46%|███████████▍             | 57/125 [00:52<01:02,  1.09batch/s, Loss=0.38]\u001b[A\n",
      "Epoch 22/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.09batch/s, Loss=0.254]\u001b[A\n",
      "Epoch 22/25:  46%|███████████▏            | 58/125 [00:53<01:01,  1.09batch/s, Loss=0.254]\u001b[A\n",
      "Epoch 22/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.09batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 22/25:  47%|███████████▎            | 59/125 [00:54<01:00,  1.09batch/s, Loss=0.514]\u001b[A\n",
      "Epoch 22/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.09batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 22/25:  48%|███████████▌            | 60/125 [00:55<00:59,  1.09batch/s, Loss=0.662]\u001b[A\n",
      "Epoch 22/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.09batch/s, Loss=0.589]\u001b[A\n",
      "Epoch 22/25:  49%|███████████▋            | 61/125 [00:56<00:58,  1.09batch/s, Loss=0.589]\u001b[A\n",
      "Epoch 22/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.313]\u001b[A\n",
      "Epoch 22/25:  50%|███████████▉            | 62/125 [00:57<00:57,  1.09batch/s, Loss=0.313]\u001b[A\n",
      "Epoch 22/25:  50%|███████████▉            | 62/125 [00:58<00:57,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 22/25:  50%|████████████            | 63/125 [00:58<00:57,  1.09batch/s, Loss=0.552]\u001b[A\n",
      "Epoch 22/25:  50%|████████████            | 63/125 [00:59<00:57,  1.09batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 22/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.08batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 22/25:  51%|████████████▎           | 64/125 [01:00<00:56,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 22/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 22/25:  52%|████████████▍           | 65/125 [01:01<00:55,  1.08batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 22/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.08batch/s, Loss=0.506]\u001b[A\n",
      "Epoch 22/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.08batch/s, Loss=0.279]\u001b[A\n",
      "Epoch 22/25:  54%|████████████▊           | 67/125 [01:01<00:53,  1.09batch/s, Loss=0.279]\u001b[A\n",
      "Epoch 22/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.09batch/s, Loss=0.309]\u001b[A\n",
      "Epoch 22/25:  54%|█████████████           | 68/125 [01:02<00:52,  1.09batch/s, Loss=0.309]\u001b[A\n",
      "Epoch 22/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.206]\u001b[A\n",
      "Epoch 22/25:  55%|█████████████▏          | 69/125 [01:03<00:51,  1.09batch/s, Loss=0.206]\u001b[A\n",
      "Epoch 22/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 22/25:  56%|█████████████▍          | 70/125 [01:04<00:50,  1.09batch/s, Loss=0.464]\u001b[A\n",
      "Epoch 22/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.802]\u001b[A\n",
      "Epoch 22/25:  57%|█████████████▋          | 71/125 [01:05<00:49,  1.09batch/s, Loss=0.802]\u001b[A\n",
      "Epoch 22/25:  57%|█████████████▋          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.381]\u001b[A\n",
      "Epoch 22/25:  58%|█████████████▊          | 72/125 [01:06<00:48,  1.09batch/s, Loss=0.381]\u001b[A\n",
      "Epoch 22/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 22/25:  58%|██████████████          | 73/125 [01:07<00:47,  1.09batch/s, Loss=0.644]\u001b[A\n",
      "Epoch 22/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 22/25:  59%|██████████████▏         | 74/125 [01:08<00:46,  1.09batch/s, Loss=0.396]\u001b[A\n",
      "Epoch 22/25:  59%|██████████████▏         | 74/125 [01:09<00:46,  1.09batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 22/25:  60%|██████████████▍         | 75/125 [01:09<00:46,  1.09batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 22/25:  60%|██████████████▍         | 75/125 [01:10<00:46,  1.09batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 22/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 22/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 22/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 22/25:  62%|███████████████▍         | 77/125 [01:12<00:44,  1.09batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 22/25:  62%|███████████████▌         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.78]\u001b[A\n",
      "Epoch 22/25:  62%|██████████████▉         | 78/125 [01:13<00:43,  1.09batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 22/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 22/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 22/25:  64%|███████████████▎        | 80/125 [01:13<00:41,  1.09batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 22/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 22/25:  65%|███████████████▌        | 81/125 [01:14<00:40,  1.09batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 22/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 22/25:  66%|███████████████▋        | 82/125 [01:15<00:39,  1.09batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 22/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.09batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 22/25:  66%|███████████████▉        | 83/125 [01:16<00:38,  1.09batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 22/25:  66%|████████████████▌        | 83/125 [01:17<00:38,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 22/25:  67%|████████████████▊        | 84/125 [01:17<00:37,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 22/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.09batch/s, Loss=0.178]\u001b[A\n",
      "Epoch 22/25:  68%|████████████████▎       | 85/125 [01:18<00:36,  1.09batch/s, Loss=0.178]\u001b[A\n",
      "Epoch 22/25:  68%|████████████████▎       | 85/125 [01:19<00:36,  1.09batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 22/25:  69%|████████████████▌       | 86/125 [01:19<00:35,  1.09batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 22/25:  69%|████████████████▌       | 86/125 [01:20<00:35,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 22/25:  70%|████████████████▋       | 87/125 [01:20<00:34,  1.09batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 22/25:  70%|████████████████▋       | 87/125 [01:21<00:34,  1.09batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 22/25:  70%|████████████████▉       | 88/125 [01:21<00:33,  1.09batch/s, Loss=0.515]\u001b[A\n",
      "Epoch 22/25:  70%|████████████████▉       | 88/125 [01:22<00:33,  1.09batch/s, Loss=0.844]\u001b[A\n",
      "Epoch 22/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.09batch/s, Loss=0.844]\u001b[A\n",
      "Epoch 22/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.09batch/s, Loss=0.374]\u001b[A\n",
      "Epoch 22/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.09batch/s, Loss=0.374]\u001b[A\n",
      "Epoch 22/25:  72%|█████████████████▎      | 90/125 [01:24<00:32,  1.09batch/s, Loss=0.359]\u001b[A\n",
      "Epoch 22/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.09batch/s, Loss=0.359]\u001b[A\n",
      "Epoch 22/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.09batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 22/25:  74%|█████████████████▋      | 92/125 [01:24<00:30,  1.09batch/s, Loss=0.694]\u001b[A\n",
      "Epoch 22/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.09batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 22/25:  74%|█████████████████▊      | 93/125 [01:25<00:29,  1.09batch/s, Loss=0.475]\u001b[A\n",
      "Epoch 22/25:  74%|██████████████████▌      | 93/125 [01:26<00:29,  1.09batch/s, Loss=0.27]\u001b[A\n",
      "Epoch 22/25:  75%|██████████████████▊      | 94/125 [01:26<00:28,  1.09batch/s, Loss=0.27]\u001b[A\n",
      "Epoch 22/25:  75%|██████████████████▊      | 94/125 [01:27<00:28,  1.09batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 22/25:  76%|███████████████████      | 95/125 [01:27<00:27,  1.09batch/s, Loss=0.51]\u001b[A\n",
      "Epoch 22/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.09batch/s, Loss=0.335]\u001b[A\n",
      "Epoch 22/25:  77%|██████████████████▍     | 96/125 [01:28<00:27,  1.07batch/s, Loss=0.335]\u001b[A\n",
      "Epoch 22/25:  77%|███████████████████▏     | 96/125 [01:29<00:27,  1.07batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 22/25:  78%|███████████████████▍     | 97/125 [01:29<00:26,  1.06batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 22/25:  78%|██████████████████▌     | 97/125 [01:30<00:26,  1.06batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 22/25:  78%|██████████████████▊     | 98/125 [01:30<00:25,  1.06batch/s, Loss=0.557]\u001b[A\n",
      "Epoch 22/25:  78%|██████████████████▊     | 98/125 [01:31<00:25,  1.06batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 22/25:  79%|███████████████████     | 99/125 [01:31<00:24,  1.07batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 22/25:  79%|███████████████████     | 99/125 [01:32<00:24,  1.07batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 22/25:  80%|██████████████████▍    | 100/125 [01:32<00:23,  1.07batch/s, Loss=0.431]\u001b[A\n",
      "Epoch 22/25:  80%|██████████████████▍    | 100/125 [01:33<00:23,  1.07batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 22/25:  81%|██████████████████▌    | 101/125 [01:33<00:22,  1.07batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 22/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.07batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 22/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.08batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 22/25:  82%|██████████████████▊    | 102/125 [01:35<00:21,  1.08batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 22/25:  82%|██████████████████▉    | 103/125 [01:35<00:20,  1.08batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 22/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 22/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.08batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 22/25:  83%|███████████████████▉    | 104/125 [01:37<00:19,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 22/25:  84%|████████████████████▏   | 105/125 [01:37<00:18,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 22/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.08batch/s, Loss=0.413]\u001b[A\n",
      "Epoch 22/25:  85%|███████████████████▌   | 106/125 [01:37<00:17,  1.09batch/s, Loss=0.413]\u001b[A\n",
      "Epoch 22/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.09batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 22/25:  86%|███████████████████▋   | 107/125 [01:38<00:16,  1.09batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 22/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.09batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 22/25:  86%|███████████████████▊   | 108/125 [01:39<00:15,  1.09batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 22/25:  86%|███████████████████▊   | 108/125 [01:40<00:15,  1.09batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 22/25:  87%|████████████████████   | 109/125 [01:40<00:14,  1.09batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 22/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 22/25:  88%|████████████████████▏  | 110/125 [01:41<00:13,  1.09batch/s, Loss=0.521]\u001b[A\n",
      "Epoch 22/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.198]\u001b[A\n",
      "Epoch 22/25:  89%|████████████████████▍  | 111/125 [01:42<00:12,  1.09batch/s, Loss=0.198]\u001b[A\n",
      "Epoch 22/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.09batch/s, Loss=0.212]\u001b[A\n",
      "Epoch 22/25:  90%|████████████████████▌  | 112/125 [01:43<00:12,  1.08batch/s, Loss=0.212]\u001b[A\n",
      "Epoch 22/25:  90%|████████████████████▌  | 112/125 [01:44<00:12,  1.08batch/s, Loss=0.798]\u001b[A\n",
      "Epoch 22/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.07batch/s, Loss=0.798]\u001b[A\n",
      "Epoch 22/25:  90%|████████████████████▊  | 113/125 [01:45<00:11,  1.07batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 22/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.06batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 22/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.06batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 22/25:  92%|█████████████████████▏ | 115/125 [01:46<00:09,  1.07batch/s, Loss=0.612]\u001b[A\n",
      "Epoch 22/25:  92%|█████████████████████▏ | 115/125 [01:47<00:09,  1.07batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 22/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.07batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 22/25:  93%|█████████████████████▎ | 116/125 [01:48<00:08,  1.07batch/s, Loss=0.225]\u001b[A\n",
      "Epoch 22/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.07batch/s, Loss=0.225]\u001b[A\n",
      "Epoch 22/25:  94%|█████████████████████▌ | 117/125 [01:49<00:07,  1.07batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 22/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.07batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 22/25:  94%|█████████████████████▋ | 118/125 [01:50<00:06,  1.07batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 22/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.07batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 22/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.07batch/s, Loss=0.409]\u001b[A\n",
      "Epoch 22/25:  96%|██████████████████████ | 120/125 [01:50<00:04,  1.08batch/s, Loss=0.409]\u001b[A\n",
      "Epoch 22/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.08batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 22/25:  97%|██████████████████████▎| 121/125 [01:51<00:03,  1.07batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 22/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.07batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 22/25:  98%|██████████████████████▍| 122/125 [01:52<00:02,  1.07batch/s, Loss=0.417]\u001b[A\n",
      "Epoch 22/25:  98%|███████████████████████▍| 122/125 [01:53<00:02,  1.07batch/s, Loss=0.86]\u001b[A\n",
      "Epoch 22/25:  98%|███████████████████████▌| 123/125 [01:53<00:01,  1.07batch/s, Loss=0.86]\u001b[A\n",
      "Epoch 22/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.07batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 22/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.08batch/s, Loss=0.559]\u001b[A\n",
      "Epoch 22/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.08batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 22/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.08batch/s, Loss=0.447]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Train Loss: 0.0578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 22/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   1%|▏                         | 1/125 [00:00<00:44,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   2%|▍                         | 2/125 [00:00<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   2%|▌                         | 3/125 [00:01<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   3%|▊                         | 4/125 [00:01<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   4%|█                         | 5/125 [00:01<00:42,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   8%|██                       | 10/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  12%|███                      | 15/125 [00:05<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  16%|████                     | 20/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  20%|█████                    | 25/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  24%|██████                   | 30/125 [00:10<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  28%|███████                  | 35/125 [00:12<00:31,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  32%|████████                 | 40/125 [00:14<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  36%|█████████                | 45/125 [00:16<00:28,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  44%|███████████              | 55/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  53%|█████████████▏           | 66/125 [00:23<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  64%|████████████████         | 80/125 [00:28<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 22/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Eval Accuracy: 0.7920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 23/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 23/25:   1%|▏                        | 1/125 [00:01<02:33,  1.24s/batch, Loss=0.519]\u001b[A\n",
      "Epoch 23/25:   1%|▏                        | 1/125 [00:02<02:33,  1.24s/batch, Loss=0.374]\u001b[A\n",
      "Epoch 23/25:   2%|▍                        | 2/125 [00:02<02:10,  1.06s/batch, Loss=0.374]\u001b[A\n",
      "Epoch 23/25:   2%|▍                         | 2/125 [00:03<02:10,  1.06s/batch, Loss=0.37]\u001b[A\n",
      "Epoch 23/25:   2%|▌                         | 3/125 [00:03<02:03,  1.01s/batch, Loss=0.37]\u001b[A\n",
      "Epoch 23/25:   2%|▌                        | 3/125 [00:04<02:03,  1.01s/batch, Loss=0.834]\u001b[A\n",
      "Epoch 23/25:   3%|▊                        | 4/125 [00:04<01:58,  1.02batch/s, Loss=0.834]\u001b[A\n",
      "Epoch 23/25:   3%|▊                        | 4/125 [00:04<01:58,  1.02batch/s, Loss=0.198]\u001b[A\n",
      "Epoch 23/25:   4%|█                        | 5/125 [00:04<01:55,  1.04batch/s, Loss=0.198]\u001b[A\n",
      "Epoch 23/25:   4%|█                        | 5/125 [00:05<01:55,  1.04batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 23/25:   5%|█▏                       | 6/125 [00:05<01:53,  1.05batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 23/25:   5%|█▏                       | 6/125 [00:06<01:53,  1.05batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 23/25:   6%|█▍                       | 7/125 [00:06<01:51,  1.06batch/s, Loss=0.565]\u001b[A\n",
      "Epoch 23/25:   6%|█▍                       | 7/125 [00:07<01:51,  1.06batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 23/25:   6%|█▌                       | 8/125 [00:07<01:50,  1.06batch/s, Loss=0.627]\u001b[A\n",
      "Epoch 23/25:   6%|█▌                       | 8/125 [00:08<01:50,  1.06batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 23/25:   7%|█▊                       | 9/125 [00:08<01:48,  1.07batch/s, Loss=0.336]\u001b[A\n",
      "Epoch 23/25:   7%|█▊                       | 9/125 [00:09<01:48,  1.07batch/s, Loss=0.885]\u001b[A\n",
      "Epoch 23/25:   8%|█▉                      | 10/125 [00:09<01:47,  1.07batch/s, Loss=0.885]\u001b[A\n",
      "Epoch 23/25:   8%|█▉                      | 10/125 [00:10<01:47,  1.07batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 23/25:   9%|██                      | 11/125 [00:10<01:46,  1.07batch/s, Loss=0.293]\u001b[A\n",
      "Epoch 23/25:   9%|██                      | 11/125 [00:11<01:46,  1.07batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 23/25:  10%|██▎                     | 12/125 [00:11<01:44,  1.08batch/s, Loss=0.449]\u001b[A\n",
      "Epoch 23/25:  10%|██▎                     | 12/125 [00:12<01:44,  1.08batch/s, Loss=0.202]\u001b[A\n",
      "Epoch 23/25:  10%|██▍                     | 13/125 [00:12<01:43,  1.08batch/s, Loss=0.202]\u001b[A\n",
      "Epoch 23/25:  10%|██▍                     | 13/125 [00:13<01:43,  1.08batch/s, Loss=0.415]\u001b[A\n",
      "Epoch 23/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.08batch/s, Loss=0.415]\u001b[A\n",
      "Epoch 23/25:  11%|██▊                      | 14/125 [00:14<01:42,  1.08batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 23/25:  12%|███                      | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.37]\u001b[A\n",
      "Epoch 23/25:  12%|██▉                     | 15/125 [00:15<01:41,  1.08batch/s, Loss=0.294]\u001b[A\n",
      "Epoch 23/25:  13%|███                     | 16/125 [00:15<01:40,  1.09batch/s, Loss=0.294]\u001b[A\n",
      "Epoch 23/25:  13%|███                     | 16/125 [00:16<01:40,  1.09batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 23/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.09batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 23/25:  14%|███▎                    | 17/125 [00:17<01:39,  1.09batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 23/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.08batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 23/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.08batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 23/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.08batch/s, Loss=0.314]\u001b[A\n",
      "Epoch 23/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.08batch/s, Loss=0.211]\u001b[A\n",
      "Epoch 23/25:  16%|███▊                    | 20/125 [00:18<01:37,  1.08batch/s, Loss=0.211]\u001b[A\n",
      "Epoch 23/25:  16%|███▊                    | 20/125 [00:19<01:37,  1.08batch/s, Loss=0.779]\u001b[A\n",
      "Epoch 23/25:  17%|████                    | 21/125 [00:19<01:36,  1.07batch/s, Loss=0.779]\u001b[A\n",
      "Epoch 23/25:  17%|████▎                     | 21/125 [00:20<01:36,  1.07batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 23/25:  18%|████▌                     | 22/125 [00:20<01:35,  1.07batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 23/25:  18%|████▏                   | 22/125 [00:21<01:35,  1.07batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 23/25:  18%|████▍                   | 23/125 [00:21<01:34,  1.08batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 23/25:  18%|████▍                   | 23/125 [00:22<01:34,  1.08batch/s, Loss=0.342]\u001b[A\n",
      "Epoch 23/25:  19%|████▌                   | 24/125 [00:22<01:33,  1.08batch/s, Loss=0.342]\u001b[A\n",
      "Epoch 23/25:  19%|████▌                   | 24/125 [00:23<01:33,  1.08batch/s, Loss=0.252]\u001b[A\n",
      "Epoch 23/25:  20%|████▊                   | 25/125 [00:23<01:32,  1.08batch/s, Loss=0.252]\u001b[A\n",
      "Epoch 23/25:  20%|█████                    | 25/125 [00:24<01:32,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 23/25:  21%|█████▏                   | 26/125 [00:24<01:31,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 23/25:  21%|████▉                   | 26/125 [00:25<01:31,  1.08batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 23/25:  22%|█████▏                  | 27/125 [00:25<01:30,  1.09batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 23/25:  22%|█████▏                  | 27/125 [00:26<01:30,  1.09batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 23/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.566]\u001b[A\n",
      "Epoch 23/25:  22%|█████▍                  | 28/125 [00:27<01:29,  1.09batch/s, Loss=0.278]\u001b[A\n",
      "Epoch 23/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.278]\u001b[A\n",
      "Epoch 23/25:  23%|█████▌                  | 29/125 [00:28<01:28,  1.09batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 23/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 23/25:  24%|█████▊                  | 30/125 [00:29<01:27,  1.09batch/s, Loss=0.793]\u001b[A\n",
      "Epoch 23/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.793]\u001b[A\n",
      "Epoch 23/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 23/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.387]\u001b[A\n",
      "Epoch 23/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 23/25:  26%|██████▎                 | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.729]\u001b[A\n",
      "Epoch 23/25:  26%|██████▌                  | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.23]\u001b[A\n",
      "Epoch 23/25:  27%|██████▊                  | 34/125 [00:31<01:23,  1.08batch/s, Loss=0.23]\u001b[A\n",
      "Epoch 23/25:  27%|██████▌                 | 34/125 [00:32<01:23,  1.08batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  28%|██████▋                 | 35/125 [00:32<01:22,  1.08batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.08batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 23/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 23/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 23/25:  30%|███████                 | 37/125 [00:34<01:21,  1.09batch/s, Loss=0.479]\u001b[A\n",
      "Epoch 23/25:  30%|███████                 | 37/125 [00:35<01:21,  1.09batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 23/25:  30%|███████▎                | 38/125 [00:35<01:20,  1.09batch/s, Loss=0.525]\u001b[A\n",
      "Epoch 23/25:  30%|███████▎                | 38/125 [00:36<01:20,  1.09batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 23/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.09batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 23/25:  31%|███████▍                | 39/125 [00:37<01:19,  1.09batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 23/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.424]\u001b[A\n",
      "Epoch 23/25:  32%|████████                 | 40/125 [00:38<01:18,  1.09batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 23/25:  33%|████████▏                | 41/125 [00:38<01:17,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 23/25:  33%|███████▊                | 41/125 [00:39<01:17,  1.08batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 23/25:  34%|████████                | 42/125 [00:39<01:16,  1.08batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 23/25:  34%|████████                | 42/125 [00:40<01:16,  1.08batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 23/25:  34%|████████▎               | 43/125 [00:40<01:16,  1.08batch/s, Loss=0.544]\u001b[A\n",
      "Epoch 23/25:  34%|████████▎               | 43/125 [00:41<01:16,  1.08batch/s, Loss=0.626]\u001b[A\n",
      "Epoch 23/25:  35%|████████▍               | 44/125 [00:41<01:15,  1.08batch/s, Loss=0.626]\u001b[A\n",
      "Epoch 23/25:  35%|████████▍               | 44/125 [00:41<01:15,  1.08batch/s, Loss=0.288]\u001b[A\n",
      "Epoch 23/25:  36%|████████▋               | 45/125 [00:41<01:14,  1.07batch/s, Loss=0.288]\u001b[A\n",
      "Epoch 23/25:  36%|████████▋               | 45/125 [00:42<01:14,  1.07batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 23/25:  37%|████████▊               | 46/125 [00:42<01:13,  1.08batch/s, Loss=0.606]\u001b[A\n",
      "Epoch 23/25:  37%|████████▊               | 46/125 [00:43<01:13,  1.08batch/s, Loss=0.466]\u001b[A\n",
      "Epoch 23/25:  38%|█████████               | 47/125 [00:43<01:12,  1.08batch/s, Loss=0.466]\u001b[A\n",
      "Epoch 23/25:  38%|█████████▍               | 47/125 [00:44<01:12,  1.08batch/s, Loss=0.14]\u001b[A\n",
      "Epoch 23/25:  38%|█████████▌               | 48/125 [00:44<01:11,  1.08batch/s, Loss=0.14]\u001b[A\n",
      "Epoch 23/25:  38%|█████████▏              | 48/125 [00:45<01:11,  1.08batch/s, Loss=0.243]\u001b[A\n",
      "Epoch 23/25:  39%|█████████▍              | 49/125 [00:45<01:10,  1.08batch/s, Loss=0.243]\u001b[A\n",
      "Epoch 23/25:  39%|█████████▍              | 49/125 [00:46<01:10,  1.08batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 23/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 23/25:  40%|██████████               | 50/125 [00:47<01:09,  1.08batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 23/25:  41%|██████████▏              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 23/25:  41%|█████████▊              | 51/125 [00:48<01:08,  1.08batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 23/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.07batch/s, Loss=0.599]\u001b[A\n",
      "Epoch 23/25:  42%|█████████▉              | 52/125 [00:49<01:07,  1.07batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 23/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.08batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 23/25:  42%|██████████▏             | 53/125 [00:50<01:06,  1.08batch/s, Loss=0.247]\u001b[A\n",
      "Epoch 23/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.247]\u001b[A\n",
      "Epoch 23/25:  43%|██████████▎             | 54/125 [00:51<01:05,  1.08batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  44%|██████████▌             | 55/125 [00:51<01:05,  1.08batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  44%|██████████▌             | 55/125 [00:52<01:05,  1.08batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 23/25:  45%|██████████▊             | 56/125 [00:52<01:04,  1.08batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 23/25:  45%|███████████▏             | 56/125 [00:53<01:04,  1.08batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 23/25:  46%|███████████▍             | 57/125 [00:53<01:03,  1.08batch/s, Loss=0.42]\u001b[A\n",
      "Epoch 23/25:  46%|██████████▉             | 57/125 [00:54<01:03,  1.08batch/s, Loss=0.367]\u001b[A\n",
      "Epoch 23/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.08batch/s, Loss=0.367]\u001b[A\n",
      "Epoch 23/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.08batch/s, Loss=0.212]\u001b[A\n",
      "Epoch 23/25:  47%|███████████▎            | 59/125 [00:54<01:00,  1.08batch/s, Loss=0.212]\u001b[A\n",
      "Epoch 23/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.08batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 23/25:  48%|███████████▌            | 60/125 [00:55<00:59,  1.09batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 23/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.09batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 23/25:  49%|███████████▋            | 61/125 [00:56<00:58,  1.09batch/s, Loss=0.418]\u001b[A\n",
      "Epoch 23/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 23/25:  50%|███████████▉            | 62/125 [00:57<00:57,  1.09batch/s, Loss=0.452]\u001b[A\n",
      "Epoch 23/25:  50%|███████████▉            | 62/125 [00:58<00:57,  1.09batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 23/25:  50%|████████████            | 63/125 [00:58<00:57,  1.09batch/s, Loss=0.631]\u001b[A\n",
      "Epoch 23/25:  50%|████████████            | 63/125 [00:59<00:57,  1.09batch/s, Loss=0.179]\u001b[A\n",
      "Epoch 23/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.09batch/s, Loss=0.179]\u001b[A\n",
      "Epoch 23/25:  51%|████████████▎           | 64/125 [01:00<00:56,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  52%|████████████▍           | 65/125 [01:01<00:55,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  53%|████████████▋           | 66/125 [01:02<00:54,  1.09batch/s, Loss=0.701]\u001b[A\n",
      "Epoch 23/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.09batch/s, Loss=0.701]\u001b[A\n",
      "Epoch 23/25:  54%|████████████▊           | 67/125 [01:03<00:53,  1.09batch/s, Loss=0.391]\u001b[A\n",
      "Epoch 23/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.391]\u001b[A\n",
      "Epoch 23/25:  54%|█████████████           | 68/125 [01:04<00:52,  1.09batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 23/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 23/25:  55%|█████████████▏          | 69/125 [01:05<00:51,  1.09batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 23/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 23/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 23/25:  57%|█████████████▋          | 71/125 [01:05<00:49,  1.09batch/s, Loss=0.473]\u001b[A\n",
      "Epoch 23/25:  57%|██████████████▏          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 23/25:  58%|██████████████▍          | 72/125 [01:06<00:48,  1.09batch/s, Loss=0.44]\u001b[A\n",
      "Epoch 23/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.405]\u001b[A\n",
      "Epoch 23/25:  58%|██████████████          | 73/125 [01:07<00:47,  1.08batch/s, Loss=0.405]\u001b[A\n",
      "Epoch 23/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.08batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 23/25:  59%|██████████████▏         | 74/125 [01:08<00:47,  1.08batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 23/25:  59%|██████████████▏         | 74/125 [01:09<00:47,  1.08batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 23/25:  60%|██████████████▍         | 75/125 [01:09<00:46,  1.08batch/s, Loss=0.264]\u001b[A\n",
      "Epoch 23/25:  60%|██████████████▍         | 75/125 [01:10<00:46,  1.08batch/s, Loss=0.743]\u001b[A\n",
      "Epoch 23/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.743]\u001b[A\n",
      "Epoch 23/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 23/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 23/25:  62%|██████████████▊         | 77/125 [01:12<00:44,  1.09batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 23/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.358]\u001b[A\n",
      "Epoch 23/25:  62%|███████████████▌         | 78/125 [01:13<00:43,  1.09batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 23/25:  63%|███████████████▊         | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.58]\u001b[A\n",
      "Epoch 23/25:  63%|███████████████▏        | 79/125 [01:14<00:42,  1.09batch/s, Loss=0.261]\u001b[A\n",
      "Epoch 23/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.261]\u001b[A\n",
      "Epoch 23/25:  64%|███████████████▎        | 80/125 [01:15<00:41,  1.09batch/s, Loss=0.344]\u001b[A\n",
      "Epoch 23/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.08batch/s, Loss=0.344]\u001b[A\n",
      "Epoch 23/25:  65%|███████████████▌        | 81/125 [01:16<00:40,  1.08batch/s, Loss=0.738]\u001b[A\n",
      "Epoch 23/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.08batch/s, Loss=0.738]\u001b[A\n",
      "Epoch 23/25:  66%|███████████████▋        | 82/125 [01:17<00:39,  1.08batch/s, Loss=0.176]\u001b[A\n",
      "Epoch 23/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.08batch/s, Loss=0.176]\u001b[A\n",
      "Epoch 23/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.08batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 23/25:  67%|████████████████▏       | 84/125 [01:17<00:37,  1.08batch/s, Loss=0.621]\u001b[A\n",
      "Epoch 23/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.08batch/s, Loss=0.229]\u001b[A\n",
      "Epoch 23/25:  68%|████████████████▎       | 85/125 [01:18<00:37,  1.08batch/s, Loss=0.229]\u001b[A\n",
      "Epoch 23/25:  68%|████████████████▎       | 85/125 [01:19<00:37,  1.08batch/s, Loss=0.292]\u001b[A\n",
      "Epoch 23/25:  69%|████████████████▌       | 86/125 [01:19<00:36,  1.07batch/s, Loss=0.292]\u001b[A\n",
      "Epoch 23/25:  69%|████████████████▌       | 86/125 [01:20<00:36,  1.07batch/s, Loss=0.216]\u001b[A\n",
      "Epoch 23/25:  70%|████████████████▋       | 87/125 [01:20<00:35,  1.07batch/s, Loss=0.216]\u001b[A\n",
      "Epoch 23/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.07batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.07batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 23/25:  70%|████████████████▉       | 88/125 [01:22<00:34,  1.07batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 23/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.07batch/s, Loss=0.584]\u001b[A\n",
      "Epoch 23/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.07batch/s, Loss=0.149]\u001b[A\n",
      "Epoch 23/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.07batch/s, Loss=0.149]\u001b[A\n",
      "Epoch 23/25:  72%|█████████████████▎      | 90/125 [01:24<00:32,  1.07batch/s, Loss=0.251]\u001b[A\n",
      "Epoch 23/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.08batch/s, Loss=0.251]\u001b[A\n",
      "Epoch 23/25:  73%|█████████████████▍      | 91/125 [01:25<00:31,  1.08batch/s, Loss=0.175]\u001b[A\n",
      "Epoch 23/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.08batch/s, Loss=0.175]\u001b[A\n",
      "Epoch 23/25:  74%|█████████████████▋      | 92/125 [01:26<00:30,  1.08batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 23/25:  74%|█████████████████▊      | 93/125 [01:26<00:29,  1.08batch/s, Loss=0.586]\u001b[A\n",
      "Epoch 23/25:  74%|█████████████████▊      | 93/125 [01:27<00:29,  1.08batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 23/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.08batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 23/25:  75%|██████████████████      | 94/125 [01:28<00:28,  1.08batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 23/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.08batch/s, Loss=0.554]\u001b[A\n",
      "Epoch 23/25:  76%|██████████████████▏     | 95/125 [01:29<00:27,  1.08batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 23/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.08batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 23/25:  77%|██████████████████▍     | 96/125 [01:30<00:26,  1.08batch/s, Loss=0.365]\u001b[A\n",
      "Epoch 23/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.08batch/s, Loss=0.365]\u001b[A\n",
      "Epoch 23/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.08batch/s, Loss=0.626]\u001b[A\n",
      "Epoch 23/25:  78%|██████████████████▊     | 98/125 [01:30<00:24,  1.09batch/s, Loss=0.626]\u001b[A\n",
      "Epoch 23/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  79%|███████████████████     | 99/125 [01:31<00:23,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 23/25:  79%|███████████████████     | 99/125 [01:32<00:23,  1.09batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 23/25:  80%|██████████████████▍    | 100/125 [01:32<00:23,  1.09batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 23/25:  80%|███████████████████▏    | 100/125 [01:33<00:23,  1.09batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 23/25:  81%|███████████████████▍    | 101/125 [01:33<00:22,  1.09batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 23/25:  81%|███████████████████▍    | 101/125 [01:34<00:22,  1.09batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 23/25:  82%|███████████████████▌    | 102/125 [01:34<00:21,  1.08batch/s, Loss=0.53]\u001b[A\n",
      "Epoch 23/25:  82%|██████████████████▊    | 102/125 [01:35<00:21,  1.08batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 23/25:  82%|██████████████████▉    | 103/125 [01:35<00:20,  1.08batch/s, Loss=0.539]\u001b[A\n",
      "Epoch 23/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.08batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 23/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.08batch/s, Loss=0.498]\u001b[A\n",
      "Epoch 23/25:  83%|███████████████████▏   | 104/125 [01:37<00:19,  1.08batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 23/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.08batch/s, Loss=0.512]\u001b[A\n",
      "Epoch 23/25:  84%|███████████████████▎   | 105/125 [01:38<00:18,  1.08batch/s, Loss=0.261]\u001b[A\n",
      "Epoch 23/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.08batch/s, Loss=0.261]\u001b[A\n",
      "Epoch 23/25:  85%|███████████████████▌   | 106/125 [01:39<00:17,  1.08batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 23/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.08batch/s, Loss=0.579]\u001b[A\n",
      "Epoch 23/25:  86%|███████████████████▋   | 107/125 [01:40<00:16,  1.08batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 23/25:  86%|███████████████████▊   | 108/125 [01:40<00:15,  1.08batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 23/25:  86%|███████████████████▊   | 108/125 [01:41<00:15,  1.08batch/s, Loss=0.956]\u001b[A\n",
      "Epoch 23/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.08batch/s, Loss=0.956]\u001b[A\n",
      "Epoch 23/25:  87%|████████████████████   | 109/125 [01:42<00:14,  1.08batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 23/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 23/25:  88%|█████████████████████   | 110/125 [01:42<00:13,  1.09batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 23/25:  89%|█████████████████████▎  | 111/125 [01:42<00:12,  1.09batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 23/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.09batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 23/25:  90%|████████████████████▌  | 112/125 [01:43<00:11,  1.09batch/s, Loss=0.454]\u001b[A\n",
      "Epoch 23/25:  90%|████████████████████▌  | 112/125 [01:44<00:11,  1.09batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 23/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.09batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 23/25:  90%|████████████████████▊  | 113/125 [01:45<00:11,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 23/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 23/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 23/25:  92%|█████████████████████▏ | 115/125 [01:46<00:09,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 23/25:  92%|██████████████████████  | 115/125 [01:47<00:09,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 23/25:  93%|██████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.73]\u001b[A\n",
      "Epoch 23/25:  93%|█████████████████████▎ | 116/125 [01:48<00:08,  1.09batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 23/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.538]\u001b[A\n",
      "Epoch 23/25:  94%|█████████████████████▌ | 117/125 [01:49<00:07,  1.09batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 23/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.495]\u001b[A\n",
      "Epoch 23/25:  94%|█████████████████████▋ | 118/125 [01:50<00:06,  1.09batch/s, Loss=0.265]\u001b[A\n",
      "Epoch 23/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.265]\u001b[A\n",
      "Epoch 23/25:  95%|█████████████████████▉ | 119/125 [01:51<00:05,  1.09batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 23/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.08batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 23/25:  96%|██████████████████████ | 120/125 [01:52<00:04,  1.08batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 23/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.08batch/s, Loss=0.529]\u001b[A\n",
      "Epoch 23/25:  97%|███████████████████████▏| 121/125 [01:53<00:03,  1.08batch/s, Loss=0.94]\u001b[A\n",
      "Epoch 23/25:  98%|███████████████████████▍| 122/125 [01:53<00:02,  1.08batch/s, Loss=0.94]\u001b[A\n",
      "Epoch 23/25:  98%|██████████████████████▍| 122/125 [01:54<00:02,  1.08batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 23/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.08batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 23/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.08batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 23/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.08batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 23/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.08batch/s, Loss=0.458]\u001b[A\n",
      "Epoch 23/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.08batch/s, Loss=0.458]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Train Loss: 0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 23/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   1%|▏                         | 1/125 [00:00<00:44,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   2%|▍                         | 2/125 [00:00<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   2%|▌                         | 3/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   3%|▊                         | 4/125 [00:01<00:43,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   4%|█                         | 5/125 [00:01<00:42,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   6%|█▍                        | 7/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   8%|██                       | 10/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  12%|███                      | 15/125 [00:05<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  16%|████                     | 20/125 [00:07<00:37,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  19%|████▊                    | 24/125 [00:08<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  20%|█████                    | 25/125 [00:08<00:35,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  22%|█████▍                   | 27/125 [00:09<00:35,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  24%|██████                   | 30/125 [00:10<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  28%|███████                  | 35/125 [00:12<00:32,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  30%|███████▌                 | 38/125 [00:13<00:31,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  32%|████████                 | 40/125 [00:14<00:30,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  33%|████████▏                | 41/125 [00:14<00:30,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  39%|█████████▊               | 49/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  44%|███████████              | 55/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  53%|█████████████▏           | 66/125 [00:23<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  64%|████████████████         | 80/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 23/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Eval Accuracy: 0.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 24/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 24/25:   1%|▏                        | 1/125 [00:01<03:13,  1.56s/batch, Loss=0.448]\u001b[A\n",
      "Epoch 24/25:   1%|▏                        | 1/125 [00:02<03:13,  1.56s/batch, Loss=0.422]\u001b[A\n",
      "Epoch 24/25:   2%|▍                        | 2/125 [00:02<02:25,  1.18s/batch, Loss=0.422]\u001b[A\n",
      "Epoch 24/25:   2%|▍                        | 2/125 [00:03<02:25,  1.18s/batch, Loss=0.574]\u001b[A\n",
      "Epoch 24/25:   2%|▌                        | 3/125 [00:03<02:09,  1.07s/batch, Loss=0.574]\u001b[A\n",
      "Epoch 24/25:   2%|▌                        | 3/125 [00:04<02:09,  1.07s/batch, Loss=0.355]\u001b[A\n",
      "Epoch 24/25:   3%|▊                        | 4/125 [00:04<02:02,  1.01s/batch, Loss=0.355]\u001b[A\n",
      "Epoch 24/25:   3%|▊                        | 4/125 [00:05<02:02,  1.01s/batch, Loss=0.751]\u001b[A\n",
      "Epoch 24/25:   4%|█                        | 5/125 [00:05<01:57,  1.02batch/s, Loss=0.751]\u001b[A\n",
      "Epoch 24/25:   4%|█                        | 5/125 [00:06<01:57,  1.02batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 24/25:   5%|█▏                       | 6/125 [00:06<01:54,  1.04batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 24/25:   5%|█▏                        | 6/125 [00:07<01:54,  1.04batch/s, Loss=0.18]\u001b[A\n",
      "Epoch 24/25:   6%|█▍                        | 7/125 [00:07<01:51,  1.06batch/s, Loss=0.18]\u001b[A\n",
      "Epoch 24/25:   6%|█▍                       | 7/125 [00:08<01:51,  1.06batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:   6%|█▌                       | 8/125 [00:08<01:50,  1.06batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:   6%|█▌                       | 8/125 [00:08<01:50,  1.06batch/s, Loss=0.288]\u001b[A\n",
      "Epoch 24/25:   7%|█▊                       | 9/125 [00:08<01:48,  1.07batch/s, Loss=0.288]\u001b[A\n",
      "Epoch 24/25:   7%|█▊                       | 9/125 [00:09<01:48,  1.07batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 24/25:   8%|█▉                      | 10/125 [00:09<01:47,  1.07batch/s, Loss=0.756]\u001b[A\n",
      "Epoch 24/25:   8%|█▉                      | 10/125 [00:10<01:47,  1.07batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 24/25:   9%|██                      | 11/125 [00:10<01:47,  1.06batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 24/25:   9%|██                      | 11/125 [00:11<01:47,  1.06batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 24/25:  10%|██▎                     | 12/125 [00:11<01:45,  1.07batch/s, Loss=0.697]\u001b[A\n",
      "Epoch 24/25:  10%|██▎                     | 12/125 [00:12<01:45,  1.07batch/s, Loss=0.741]\u001b[A\n",
      "Epoch 24/25:  10%|██▍                     | 13/125 [00:12<01:44,  1.08batch/s, Loss=0.741]\u001b[A\n",
      "Epoch 24/25:  10%|██▍                     | 13/125 [00:13<01:44,  1.08batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 24/25:  11%|██▋                     | 14/125 [00:13<01:42,  1.08batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 24/25:  11%|██▋                     | 14/125 [00:14<01:42,  1.08batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 24/25:  12%|██▉                     | 15/125 [00:14<01:41,  1.08batch/s, Loss=0.354]\u001b[A\n",
      "Epoch 24/25:  12%|██▉                     | 15/125 [00:15<01:41,  1.08batch/s, Loss=0.365]\u001b[A\n",
      "Epoch 24/25:  13%|███                     | 16/125 [00:15<01:40,  1.08batch/s, Loss=0.365]\u001b[A\n",
      "Epoch 24/25:  13%|███                     | 16/125 [00:16<01:40,  1.08batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 24/25:  14%|███▎                    | 17/125 [00:16<01:39,  1.08batch/s, Loss=0.299]\u001b[A\n",
      "Epoch 24/25:  14%|███▎                    | 17/125 [00:17<01:39,  1.08batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 24/25:  14%|███▍                    | 18/125 [00:17<01:38,  1.08batch/s, Loss=0.467]\u001b[A\n",
      "Epoch 24/25:  14%|███▍                    | 18/125 [00:18<01:38,  1.08batch/s, Loss=0.445]\u001b[A\n",
      "Epoch 24/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.08batch/s, Loss=0.445]\u001b[A\n",
      "Epoch 24/25:  15%|███▋                    | 19/125 [00:19<01:37,  1.08batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 24/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.08batch/s, Loss=0.428]\u001b[A\n",
      "Epoch 24/25:  16%|███▊                    | 20/125 [00:20<01:36,  1.08batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 24/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.382]\u001b[A\n",
      "Epoch 24/25:  17%|████                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  18%|████▏                   | 22/125 [00:20<01:34,  1.09batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  18%|████▏                   | 22/125 [00:21<01:34,  1.09batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 24/25:  18%|████▍                   | 23/125 [00:21<01:33,  1.09batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 24/25:  18%|████▍                   | 23/125 [00:22<01:33,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 24/25:  19%|████▌                   | 24/125 [00:22<01:33,  1.09batch/s, Loss=0.581]\u001b[A\n",
      "Epoch 24/25:  19%|████▌                   | 24/125 [00:23<01:33,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 24/25:  20%|████▊                   | 25/125 [00:23<01:32,  1.09batch/s, Loss=0.462]\u001b[A\n",
      "Epoch 24/25:  20%|████▊                   | 25/125 [00:24<01:32,  1.09batch/s, Loss=0.309]\u001b[A\n",
      "Epoch 24/25:  21%|████▉                   | 26/125 [00:24<01:31,  1.09batch/s, Loss=0.309]\u001b[A\n",
      "Epoch 24/25:  21%|████▉                   | 26/125 [00:25<01:31,  1.09batch/s, Loss=0.374]\u001b[A\n",
      "Epoch 24/25:  22%|█████▏                  | 27/125 [00:25<01:30,  1.09batch/s, Loss=0.374]\u001b[A\n",
      "Epoch 24/25:  22%|█████▏                  | 27/125 [00:26<01:30,  1.09batch/s, Loss=0.501]\u001b[A\n",
      "Epoch 24/25:  22%|█████▍                  | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.501]\u001b[A\n",
      "Epoch 24/25:  22%|█████▍                  | 28/125 [00:27<01:29,  1.09batch/s, Loss=0.298]\u001b[A\n",
      "Epoch 24/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.298]\u001b[A\n",
      "Epoch 24/25:  23%|█████▊                   | 29/125 [00:28<01:28,  1.09batch/s, Loss=0.49]\u001b[A\n",
      "Epoch 24/25:  24%|██████                   | 30/125 [00:28<01:27,  1.09batch/s, Loss=0.49]\u001b[A\n",
      "Epoch 24/25:  24%|█████▊                  | 30/125 [00:29<01:27,  1.09batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 24/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.487]\u001b[A\n",
      "Epoch 24/25:  25%|█████▉                  | 31/125 [00:30<01:26,  1.09batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 24/25:  26%|██████▏                 | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.361]\u001b[A\n",
      "Epoch 24/25:  26%|██████▏                 | 32/125 [00:31<01:25,  1.09batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 24/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 24/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.249]\u001b[A\n",
      "Epoch 24/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.249]\u001b[A\n",
      "Epoch 24/25:  27%|██████▌                 | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.366]\u001b[A\n",
      "Epoch 24/25:  28%|██████▋                 | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.366]\u001b[A\n",
      "Epoch 24/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.378]\u001b[A\n",
      "Epoch 24/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.378]\u001b[A\n",
      "Epoch 24/25:  29%|███████▏                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 24/25:  30%|███████▍                 | 37/125 [00:34<01:21,  1.09batch/s, Loss=0.39]\u001b[A\n",
      "Epoch 24/25:  30%|███████                 | 37/125 [00:35<01:21,  1.09batch/s, Loss=0.283]\u001b[A\n",
      "Epoch 24/25:  30%|███████▎                | 38/125 [00:35<01:20,  1.08batch/s, Loss=0.283]\u001b[A\n",
      "Epoch 24/25:  30%|███████▎                | 38/125 [00:36<01:20,  1.08batch/s, Loss=0.211]\u001b[A\n",
      "Epoch 24/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.08batch/s, Loss=0.211]\u001b[A\n",
      "Epoch 24/25:  31%|███████▍                | 39/125 [00:37<01:19,  1.08batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 24/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.08batch/s, Loss=0.349]\u001b[A\n",
      "Epoch 24/25:  32%|████████                 | 40/125 [00:38<01:18,  1.08batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 24/25:  33%|████████▏                | 41/125 [00:38<01:17,  1.08batch/s, Loss=0.26]\u001b[A\n",
      "Epoch 24/25:  33%|███████▊                | 41/125 [00:39<01:17,  1.08batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 24/25:  34%|████████                | 42/125 [00:39<01:16,  1.08batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 24/25:  34%|████████                | 42/125 [00:40<01:16,  1.08batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:  34%|████████▎               | 43/125 [00:40<01:15,  1.08batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:  34%|████████▎               | 43/125 [00:41<01:15,  1.08batch/s, Loss=0.421]\u001b[A\n",
      "Epoch 24/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.08batch/s, Loss=0.421]\u001b[A\n",
      "Epoch 24/25:  35%|████████▍               | 44/125 [00:42<01:14,  1.08batch/s, Loss=0.301]\u001b[A\n",
      "Epoch 24/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.08batch/s, Loss=0.301]\u001b[A\n",
      "Epoch 24/25:  36%|████████▋               | 45/125 [00:43<01:13,  1.08batch/s, Loss=0.719]\u001b[A\n",
      "Epoch 24/25:  37%|████████▊               | 46/125 [00:43<01:13,  1.07batch/s, Loss=0.719]\u001b[A\n",
      "Epoch 24/25:  37%|████████▊               | 46/125 [00:44<01:13,  1.07batch/s, Loss=0.851]\u001b[A\n",
      "Epoch 24/25:  38%|█████████               | 47/125 [00:44<01:13,  1.06batch/s, Loss=0.851]\u001b[A\n",
      "Epoch 24/25:  38%|█████████▍               | 47/125 [00:45<01:13,  1.06batch/s, Loss=1.16]\u001b[A\n",
      "Epoch 24/25:  38%|█████████▌               | 48/125 [00:45<01:12,  1.06batch/s, Loss=1.16]\u001b[A\n",
      "Epoch 24/25:  38%|█████████▏              | 48/125 [00:45<01:12,  1.06batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 24/25:  39%|█████████▍              | 49/125 [00:45<01:11,  1.06batch/s, Loss=0.641]\u001b[A\n",
      "Epoch 24/25:  39%|█████████▍              | 49/125 [00:46<01:11,  1.06batch/s, Loss=0.782]\u001b[A\n",
      "Epoch 24/25:  40%|█████████▌              | 50/125 [00:46<01:10,  1.06batch/s, Loss=0.782]\u001b[A\n",
      "Epoch 24/25:  40%|█████████▌              | 50/125 [00:47<01:10,  1.06batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 24/25:  41%|█████████▊              | 51/125 [00:47<01:09,  1.07batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 24/25:  41%|██████████▏              | 51/125 [00:48<01:09,  1.07batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 24/25:  42%|██████████▍              | 52/125 [00:48<01:08,  1.07batch/s, Loss=0.63]\u001b[A\n",
      "Epoch 24/25:  42%|█████████▉              | 52/125 [00:49<01:08,  1.07batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 24/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.07batch/s, Loss=0.519]\u001b[A\n",
      "Epoch 24/25:  42%|██████████▏             | 53/125 [00:50<01:06,  1.07batch/s, Loss=0.701]\u001b[A\n",
      "Epoch 24/25:  43%|██████████▎             | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.701]\u001b[A\n",
      "Epoch 24/25:  43%|██████████▊              | 54/125 [00:51<01:05,  1.08batch/s, Loss=0.75]\u001b[A\n",
      "Epoch 24/25:  44%|███████████              | 55/125 [00:51<01:04,  1.08batch/s, Loss=0.75]\u001b[A\n",
      "Epoch 24/25:  44%|██████████▌             | 55/125 [00:52<01:04,  1.08batch/s, Loss=0.321]\u001b[A\n",
      "Epoch 24/25:  45%|██████████▊             | 56/125 [00:52<01:03,  1.08batch/s, Loss=0.321]\u001b[A\n",
      "Epoch 24/25:  45%|██████████▊             | 56/125 [00:53<01:03,  1.08batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 24/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.08batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 24/25:  46%|██████████▉             | 57/125 [00:54<01:02,  1.08batch/s, Loss=0.348]\u001b[A\n",
      "Epoch 24/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.08batch/s, Loss=0.348]\u001b[A\n",
      "Epoch 24/25:  46%|███████████▏            | 58/125 [00:55<01:01,  1.08batch/s, Loss=0.206]\u001b[A\n",
      "Epoch 24/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.08batch/s, Loss=0.206]\u001b[A\n",
      "Epoch 24/25:  47%|███████████▎            | 59/125 [00:56<01:00,  1.08batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.08batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  48%|███████████▌            | 60/125 [00:57<00:59,  1.08batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 24/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 24/25:  49%|███████████▋            | 61/125 [00:57<00:58,  1.09batch/s, Loss=0.391]\u001b[A\n",
      "Epoch 24/25:  50%|███████████▉            | 62/125 [00:57<00:57,  1.09batch/s, Loss=0.391]\u001b[A\n",
      "Epoch 24/25:  50%|███████████▉            | 62/125 [00:58<00:57,  1.09batch/s, Loss=0.241]\u001b[A\n",
      "Epoch 24/25:  50%|████████████            | 63/125 [00:58<00:56,  1.09batch/s, Loss=0.241]\u001b[A\n",
      "Epoch 24/25:  50%|████████████▌            | 63/125 [00:59<00:56,  1.09batch/s, Loss=0.16]\u001b[A\n",
      "Epoch 24/25:  51%|████████████▊            | 64/125 [00:59<00:56,  1.09batch/s, Loss=0.16]\u001b[A\n",
      "Epoch 24/25:  51%|████████████▎           | 64/125 [01:00<00:56,  1.09batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  52%|████████████▍           | 65/125 [01:00<00:55,  1.09batch/s, Loss=0.444]\u001b[A\n",
      "Epoch 24/25:  52%|████████████▍           | 65/125 [01:01<00:55,  1.09batch/s, Loss=0.558]\u001b[A\n",
      "Epoch 24/25:  53%|████████████▋           | 66/125 [01:01<00:54,  1.09batch/s, Loss=0.558]\u001b[A\n",
      "Epoch 24/25:  53%|████████████▋           | 66/125 [01:02<00:54,  1.09batch/s, Loss=0.369]\u001b[A\n",
      "Epoch 24/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.09batch/s, Loss=0.369]\u001b[A\n",
      "Epoch 24/25:  54%|█████████████▍           | 67/125 [01:03<00:53,  1.09batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 24/25:  54%|█████████████▌           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 24/25:  54%|█████████████           | 68/125 [01:04<00:52,  1.09batch/s, Loss=0.251]\u001b[A\n",
      "Epoch 24/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.251]\u001b[A\n",
      "Epoch 24/25:  55%|█████████████▏          | 69/125 [01:05<00:51,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 24/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 24/25:  56%|█████████████▍          | 70/125 [01:06<00:50,  1.09batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 24/25:  57%|█████████████▋          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.497]\u001b[A\n",
      "Epoch 24/25:  57%|█████████████▋          | 71/125 [01:07<00:49,  1.09batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 24/25:  58%|█████████████▊          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.635]\u001b[A\n",
      "Epoch 24/25:  58%|█████████████▊          | 72/125 [01:08<00:48,  1.09batch/s, Loss=0.185]\u001b[A\n",
      "Epoch 24/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.185]\u001b[A\n",
      "Epoch 24/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.352]\u001b[A\n",
      "Epoch 24/25:  59%|██████████████▏         | 74/125 [01:08<00:46,  1.09batch/s, Loss=0.352]\u001b[A\n",
      "Epoch 24/25:  59%|██████████████▏         | 74/125 [01:09<00:46,  1.09batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 24/25:  60%|██████████████▍         | 75/125 [01:09<00:46,  1.09batch/s, Loss=0.362]\u001b[A\n",
      "Epoch 24/25:  60%|██████████████▍         | 75/125 [01:10<00:46,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 24/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 24/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.294]\u001b[A\n",
      "Epoch 24/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.08batch/s, Loss=0.294]\u001b[A\n",
      "Epoch 24/25:  62%|██████████████▊         | 77/125 [01:12<00:44,  1.08batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 24/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 24/25:  62%|██████████████▉         | 78/125 [01:13<00:43,  1.09batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 24/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.516]\u001b[A\n",
      "Epoch 24/25:  63%|███████████████▏        | 79/125 [01:14<00:42,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 24/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.419]\u001b[A\n",
      "Epoch 24/25:  64%|███████████████▎        | 80/125 [01:15<00:41,  1.09batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 24/25:  65%|███████████████▌        | 81/125 [01:16<00:40,  1.09batch/s, Loss=0.369]\u001b[A\n",
      "Epoch 24/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.09batch/s, Loss=0.369]\u001b[A\n",
      "Epoch 24/25:  66%|███████████████▋        | 82/125 [01:17<00:39,  1.09batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 24/25:  66%|███████████████▉        | 83/125 [01:17<00:38,  1.09batch/s, Loss=0.441]\u001b[A\n",
      "Epoch 24/25:  66%|███████████████▉        | 83/125 [01:18<00:38,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 24/25:  67%|████████████████▏       | 84/125 [01:18<00:37,  1.09batch/s, Loss=0.568]\u001b[A\n",
      "Epoch 24/25:  67%|████████████████▏       | 84/125 [01:19<00:37,  1.09batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 24/25:  68%|████████████████▎       | 85/125 [01:19<00:36,  1.09batch/s, Loss=0.282]\u001b[A\n",
      "Epoch 24/25:  68%|████████████████▎       | 85/125 [01:20<00:36,  1.09batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 24/25:  69%|████████████████▌       | 86/125 [01:20<00:35,  1.09batch/s, Loss=0.277]\u001b[A\n",
      "Epoch 24/25:  69%|█████████████████▏       | 86/125 [01:20<00:35,  1.09batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 24/25:  70%|█████████████████▍       | 87/125 [01:20<00:34,  1.09batch/s, Loss=0.56]\u001b[A\n",
      "Epoch 24/25:  70%|████████████████▋       | 87/125 [01:21<00:34,  1.09batch/s, Loss=0.166]\u001b[A\n",
      "Epoch 24/25:  70%|████████████████▉       | 88/125 [01:21<00:33,  1.09batch/s, Loss=0.166]\u001b[A\n",
      "Epoch 24/25:  70%|████████████████▉       | 88/125 [01:22<00:33,  1.09batch/s, Loss=0.285]\u001b[A\n",
      "Epoch 24/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.09batch/s, Loss=0.285]\u001b[A\n",
      "Epoch 24/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.09batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 24/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.08batch/s, Loss=0.503]\u001b[A\n",
      "Epoch 24/25:  72%|█████████████████▎      | 90/125 [01:24<00:32,  1.08batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 24/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.08batch/s, Loss=0.392]\u001b[A\n",
      "Epoch 24/25:  73%|█████████████████▍      | 91/125 [01:25<00:31,  1.08batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 24/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.08batch/s, Loss=0.696]\u001b[A\n",
      "Epoch 24/25:  74%|██████████████████▍      | 92/125 [01:26<00:30,  1.08batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 24/25:  74%|██████████████████▌      | 93/125 [01:26<00:29,  1.08batch/s, Loss=0.47]\u001b[A\n",
      "Epoch 24/25:  74%|█████████████████▊      | 93/125 [01:27<00:29,  1.08batch/s, Loss=0.308]\u001b[A\n",
      "Epoch 24/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.08batch/s, Loss=0.308]\u001b[A\n",
      "Epoch 24/25:  75%|██████████████████      | 94/125 [01:28<00:28,  1.08batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 24/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.08batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 24/25:  76%|██████████████████▏     | 95/125 [01:29<00:27,  1.08batch/s, Loss=0.208]\u001b[A\n",
      "Epoch 24/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.08batch/s, Loss=0.208]\u001b[A\n",
      "Epoch 24/25:  77%|██████████████████▍     | 96/125 [01:30<00:26,  1.08batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 24/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.08batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 24/25:  78%|██████████████████▌     | 97/125 [01:31<00:25,  1.08batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 24/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.08batch/s, Loss=0.597]\u001b[A\n",
      "Epoch 24/25:  78%|██████████████████▊     | 98/125 [01:32<00:24,  1.08batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 24/25:  79%|███████████████████     | 99/125 [01:32<00:24,  1.07batch/s, Loss=0.468]\u001b[A\n",
      "Epoch 24/25:  79%|███████████████████     | 99/125 [01:33<00:24,  1.07batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 24/25:  80%|██████████████████▍    | 100/125 [01:33<00:23,  1.06batch/s, Loss=0.461]\u001b[A\n",
      "Epoch 24/25:  80%|██████████████████▍    | 100/125 [01:33<00:23,  1.06batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 24/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.05batch/s, Loss=0.426]\u001b[A\n",
      "Epoch 24/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.05batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 24/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.06batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 24/25:  82%|████████████████████▍    | 102/125 [01:35<00:21,  1.06batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 24/25:  82%|████████████████████▌    | 103/125 [01:35<00:20,  1.07batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 24/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.07batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 24/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.07batch/s, Loss=0.485]\u001b[A\n",
      "Epoch 24/25:  83%|███████████████████▏   | 104/125 [01:37<00:19,  1.07batch/s, Loss=0.301]\u001b[A\n",
      "Epoch 24/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.07batch/s, Loss=0.301]\u001b[A\n",
      "Epoch 24/25:  84%|███████████████████▎   | 105/125 [01:38<00:18,  1.07batch/s, Loss=0.455]\u001b[A\n",
      "Epoch 24/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.08batch/s, Loss=0.455]\u001b[A\n",
      "Epoch 24/25:  85%|███████████████████▌   | 106/125 [01:39<00:17,  1.08batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 24/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.08batch/s, Loss=0.524]\u001b[A\n",
      "Epoch 24/25:  86%|███████████████████▋   | 107/125 [01:40<00:16,  1.08batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 24/25:  86%|███████████████████▊   | 108/125 [01:40<00:15,  1.08batch/s, Loss=0.319]\u001b[A\n",
      "Epoch 24/25:  86%|███████████████████▊   | 108/125 [01:41<00:15,  1.08batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 24/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.08batch/s, Loss=0.511]\u001b[A\n",
      "Epoch 24/25:  87%|████████████████████   | 109/125 [01:42<00:14,  1.08batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 24/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.08batch/s, Loss=0.412]\u001b[A\n",
      "Epoch 24/25:  88%|████████████████████▏  | 110/125 [01:43<00:13,  1.08batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 24/25:  89%|████████████████████▍  | 111/125 [01:43<00:12,  1.08batch/s, Loss=0.406]\u001b[A\n",
      "Epoch 24/25:  89%|████████████████████▍  | 111/125 [01:44<00:12,  1.08batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 24/25:  90%|████████████████████▌  | 112/125 [01:44<00:12,  1.08batch/s, Loss=0.537]\u001b[A\n",
      "Epoch 24/25:  90%|████████████████████▌  | 112/125 [01:45<00:12,  1.08batch/s, Loss=0.407]\u001b[A\n",
      "Epoch 24/25:  90%|████████████████████▊  | 113/125 [01:45<00:11,  1.08batch/s, Loss=0.407]\u001b[A\n",
      "Epoch 24/25:  90%|████████████████████▊  | 113/125 [01:46<00:11,  1.08batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 24/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.09batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 24/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 24/25:  92%|█████████████████████▏ | 115/125 [01:46<00:09,  1.09batch/s, Loss=0.408]\u001b[A\n",
      "Epoch 24/25:  92%|█████████████████████▏ | 115/125 [01:47<00:09,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 24/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 24/25:  93%|█████████████████████▎ | 116/125 [01:48<00:08,  1.09batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 24/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.657]\u001b[A\n",
      "Epoch 24/25:  94%|█████████████████████▌ | 117/125 [01:49<00:07,  1.09batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 24/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.395]\u001b[A\n",
      "Epoch 24/25:  94%|█████████████████████▋ | 118/125 [01:50<00:06,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 24/25:  95%|█████████████████████▉ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.703]\u001b[A\n",
      "Epoch 24/25:  95%|█████████████████████▉ | 119/125 [01:51<00:05,  1.09batch/s, Loss=0.162]\u001b[A\n",
      "Epoch 24/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.162]\u001b[A\n",
      "Epoch 24/25:  96%|██████████████████████ | 120/125 [01:52<00:04,  1.09batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 24/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.09batch/s, Loss=0.658]\u001b[A\n",
      "Epoch 24/25:  97%|██████████████████████▎| 121/125 [01:53<00:03,  1.09batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 24/25:  98%|██████████████████████▍| 122/125 [01:53<00:02,  1.09batch/s, Loss=0.527]\u001b[A\n",
      "Epoch 24/25:  98%|██████████████████████▍| 122/125 [01:54<00:02,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 24/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.09batch/s, Loss=0.583]\u001b[A\n",
      "Epoch 24/25:  98%|██████████████████████▋| 123/125 [01:55<00:01,  1.09batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 24/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.09batch/s, Loss=0.296]\u001b[A\n",
      "Epoch 24/25:  99%|██████████████████████▊| 124/125 [01:56<00:00,  1.09batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 24/25: 100%|███████████████████████| 125/125 [01:56<00:00,  1.08batch/s, Loss=0.356]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Train Loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 24/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   1%|▏                         | 1/125 [00:00<00:44,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   2%|▍                         | 2/125 [00:00<00:43,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   2%|▌                         | 3/125 [00:01<00:44,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   3%|▊                         | 4/125 [00:01<00:43,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   4%|█                         | 5/125 [00:01<00:43,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   5%|█▏                        | 6/125 [00:02<00:42,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   6%|█▋                        | 8/125 [00:02<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   8%|██                       | 10/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  11%|██▊                      | 14/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  12%|███                      | 15/125 [00:05<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  16%|████                     | 20/125 [00:07<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  17%|████▏                    | 21/125 [00:07<00:37,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  20%|█████                    | 25/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  22%|█████▌                   | 28/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  24%|██████                   | 30/125 [00:10<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  28%|███████                  | 35/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  32%|████████                 | 40/125 [00:14<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  44%|███████████              | 55/125 [00:19<00:24,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  53%|█████████████▏           | 66/125 [00:23<00:21,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  55%|█████████████▊           | 69/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  58%|██████████████▌          | 73/125 [00:25<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  64%|████████████████         | 80/125 [00:28<00:16,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  66%|████████████████▌        | 83/125 [00:29<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  70%|█████████████████▍       | 87/125 [00:30<00:13,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  81%|███████████████████▍    | 101/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 24/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Eval Accuracy: 0.8060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/25:   0%|                                             | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 25/25:   0%|                                 | 0/125 [00:01<?, ?batch/s, Loss=0.389]\u001b[A\n",
      "Epoch 25/25:   1%|▏                        | 1/125 [00:01<02:34,  1.24s/batch, Loss=0.389]\u001b[A\n",
      "Epoch 25/25:   1%|▏                         | 1/125 [00:02<02:34,  1.24s/batch, Loss=0.33]\u001b[A\n",
      "Epoch 25/25:   2%|▍                         | 2/125 [00:02<02:10,  1.06s/batch, Loss=0.33]\u001b[A\n",
      "Epoch 25/25:   2%|▍                        | 2/125 [00:03<02:10,  1.06s/batch, Loss=0.268]\u001b[A\n",
      "Epoch 25/25:   2%|▌                        | 3/125 [00:03<02:01,  1.00batch/s, Loss=0.268]\u001b[A\n",
      "Epoch 25/25:   2%|▌                        | 3/125 [00:04<02:01,  1.00batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 25/25:   3%|▊                        | 4/125 [00:04<01:57,  1.03batch/s, Loss=0.492]\u001b[A\n",
      "Epoch 25/25:   3%|▊                        | 4/125 [00:04<01:57,  1.03batch/s, Loss=0.292]\u001b[A\n",
      "Epoch 25/25:   4%|█                        | 5/125 [00:04<01:54,  1.05batch/s, Loss=0.292]\u001b[A\n",
      "Epoch 25/25:   4%|█                        | 5/125 [00:05<01:54,  1.05batch/s, Loss=0.364]\u001b[A\n",
      "Epoch 25/25:   5%|█▏                       | 6/125 [00:05<01:51,  1.06batch/s, Loss=0.364]\u001b[A\n",
      "Epoch 25/25:   5%|█▏                        | 6/125 [00:06<01:51,  1.06batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 25/25:   6%|█▍                        | 7/125 [00:06<01:50,  1.07batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 25/25:   6%|█▍                        | 7/125 [00:07<01:50,  1.07batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 25/25:   6%|█▋                        | 8/125 [00:07<01:48,  1.08batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 25/25:   6%|█▌                       | 8/125 [00:08<01:48,  1.08batch/s, Loss=0.809]\u001b[A\n",
      "Epoch 25/25:   7%|█▊                       | 9/125 [00:08<01:47,  1.08batch/s, Loss=0.809]\u001b[A\n",
      "Epoch 25/25:   7%|█▊                       | 9/125 [00:09<01:47,  1.08batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 25/25:   8%|█▉                      | 10/125 [00:09<01:46,  1.08batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 25/25:   8%|█▉                      | 10/125 [00:10<01:46,  1.08batch/s, Loss=0.458]\u001b[A\n",
      "Epoch 25/25:   9%|██                      | 11/125 [00:10<01:46,  1.07batch/s, Loss=0.458]\u001b[A\n",
      "Epoch 25/25:   9%|██                      | 11/125 [00:11<01:46,  1.07batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 25/25:  10%|██▎                     | 12/125 [00:11<01:45,  1.07batch/s, Loss=0.571]\u001b[A\n",
      "Epoch 25/25:  10%|██▎                     | 12/125 [00:12<01:45,  1.07batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 25/25:  10%|██▍                     | 13/125 [00:12<01:44,  1.07batch/s, Loss=0.423]\u001b[A\n",
      "Epoch 25/25:  10%|██▌                      | 13/125 [00:13<01:44,  1.07batch/s, Loss=0.97]\u001b[A\n",
      "Epoch 25/25:  11%|██▊                      | 14/125 [00:13<01:43,  1.07batch/s, Loss=0.97]\u001b[A\n",
      "Epoch 25/25:  11%|██▋                     | 14/125 [00:14<01:43,  1.07batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 25/25:  12%|██▉                     | 15/125 [00:14<01:42,  1.07batch/s, Loss=0.383]\u001b[A\n",
      "Epoch 25/25:  12%|██▉                     | 15/125 [00:15<01:42,  1.07batch/s, Loss=0.341]\u001b[A\n",
      "Epoch 25/25:  13%|███                     | 16/125 [00:15<01:41,  1.08batch/s, Loss=0.341]\u001b[A\n",
      "Epoch 25/25:  13%|███                     | 16/125 [00:16<01:41,  1.08batch/s, Loss=0.795]\u001b[A\n",
      "Epoch 25/25:  14%|███▎                    | 17/125 [00:16<01:40,  1.08batch/s, Loss=0.795]\u001b[A\n",
      "Epoch 25/25:  14%|███▎                    | 17/125 [00:16<01:40,  1.08batch/s, Loss=0.172]\u001b[A\n",
      "Epoch 25/25:  14%|███▍                    | 18/125 [00:16<01:39,  1.08batch/s, Loss=0.172]\u001b[A\n",
      "Epoch 25/25:  14%|███▍                    | 18/125 [00:17<01:39,  1.08batch/s, Loss=0.262]\u001b[A\n",
      "Epoch 25/25:  15%|███▋                    | 19/125 [00:17<01:37,  1.08batch/s, Loss=0.262]\u001b[A\n",
      "Epoch 25/25:  15%|███▋                    | 19/125 [00:18<01:37,  1.08batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 25/25:  16%|███▊                    | 20/125 [00:18<01:36,  1.08batch/s, Loss=0.353]\u001b[A\n",
      "Epoch 25/25:  16%|███▊                    | 20/125 [00:19<01:36,  1.08batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 25/25:  17%|████                    | 21/125 [00:19<01:35,  1.09batch/s, Loss=0.553]\u001b[A\n",
      "Epoch 25/25:  17%|████▏                    | 21/125 [00:20<01:35,  1.09batch/s, Loss=0.81]\u001b[A\n",
      "Epoch 25/25:  18%|████▍                    | 22/125 [00:20<01:34,  1.09batch/s, Loss=0.81]\u001b[A\n",
      "Epoch 25/25:  18%|████▏                   | 22/125 [00:21<01:34,  1.09batch/s, Loss=0.244]\u001b[A\n",
      "Epoch 25/25:  18%|████▍                   | 23/125 [00:21<01:33,  1.09batch/s, Loss=0.244]\u001b[A\n",
      "Epoch 25/25:  18%|████▍                   | 23/125 [00:22<01:33,  1.09batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 25/25:  19%|████▌                   | 24/125 [00:22<01:32,  1.09batch/s, Loss=0.397]\u001b[A\n",
      "Epoch 25/25:  19%|████▌                   | 24/125 [00:23<01:32,  1.09batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 25/25:  20%|████▊                   | 25/125 [00:23<01:31,  1.09batch/s, Loss=0.676]\u001b[A\n",
      "Epoch 25/25:  20%|████▊                   | 25/125 [00:24<01:31,  1.09batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 25/25:  21%|████▉                   | 26/125 [00:24<01:31,  1.09batch/s, Loss=0.385]\u001b[A\n",
      "Epoch 25/25:  21%|████▉                   | 26/125 [00:25<01:31,  1.09batch/s, Loss=0.337]\u001b[A\n",
      "Epoch 25/25:  22%|█████▏                  | 27/125 [00:25<01:30,  1.09batch/s, Loss=0.337]\u001b[A\n",
      "Epoch 25/25:  22%|█████▍                   | 27/125 [00:26<01:30,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 25/25:  22%|█████▌                   | 28/125 [00:26<01:29,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 25/25:  22%|█████▍                  | 28/125 [00:27<01:29,  1.09batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 25/25:  23%|█████▌                  | 29/125 [00:27<01:28,  1.09batch/s, Loss=0.661]\u001b[A\n",
      "Epoch 25/25:  23%|█████▌                  | 29/125 [00:28<01:28,  1.09batch/s, Loss=0.229]\u001b[A\n",
      "Epoch 25/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.08batch/s, Loss=0.229]\u001b[A\n",
      "Epoch 25/25:  24%|█████▊                  | 30/125 [00:28<01:27,  1.08batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 25/25:  25%|█████▉                  | 31/125 [00:28<01:26,  1.09batch/s, Loss=0.447]\u001b[A\n",
      "Epoch 25/25:  25%|█████▉                  | 31/125 [00:29<01:26,  1.09batch/s, Loss=0.357]\u001b[A\n",
      "Epoch 25/25:  26%|██████▏                 | 32/125 [00:29<01:25,  1.09batch/s, Loss=0.357]\u001b[A\n",
      "Epoch 25/25:  26%|██████▍                  | 32/125 [00:30<01:25,  1.09batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 25/25:  26%|██████▌                  | 33/125 [00:30<01:24,  1.09batch/s, Loss=0.36]\u001b[A\n",
      "Epoch 25/25:  26%|██████▎                 | 33/125 [00:31<01:24,  1.09batch/s, Loss=0.267]\u001b[A\n",
      "Epoch 25/25:  27%|██████▌                 | 34/125 [00:31<01:23,  1.09batch/s, Loss=0.267]\u001b[A\n",
      "Epoch 25/25:  27%|██████▌                 | 34/125 [00:32<01:23,  1.09batch/s, Loss=0.231]\u001b[A\n",
      "Epoch 25/25:  28%|██████▋                 | 35/125 [00:32<01:22,  1.09batch/s, Loss=0.231]\u001b[A\n",
      "Epoch 25/25:  28%|██████▋                 | 35/125 [00:33<01:22,  1.09batch/s, Loss=0.253]\u001b[A\n",
      "Epoch 25/25:  29%|██████▉                 | 36/125 [00:33<01:21,  1.09batch/s, Loss=0.253]\u001b[A\n",
      "Epoch 25/25:  29%|██████▉                 | 36/125 [00:34<01:21,  1.09batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 25/25:  30%|███████                 | 37/125 [00:34<01:20,  1.09batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 25/25:  30%|███████                 | 37/125 [00:35<01:20,  1.09batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 25/25:  30%|███████▎                | 38/125 [00:35<01:20,  1.09batch/s, Loss=0.334]\u001b[A\n",
      "Epoch 25/25:  30%|███████▎                | 38/125 [00:36<01:20,  1.09batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 25/25:  31%|███████▍                | 39/125 [00:36<01:19,  1.09batch/s, Loss=0.346]\u001b[A\n",
      "Epoch 25/25:  31%|███████▍                | 39/125 [00:37<01:19,  1.09batch/s, Loss=0.193]\u001b[A\n",
      "Epoch 25/25:  32%|███████▋                | 40/125 [00:37<01:18,  1.09batch/s, Loss=0.193]\u001b[A\n",
      "Epoch 25/25:  32%|███████▋                | 40/125 [00:38<01:18,  1.09batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 25/25:  33%|███████▊                | 41/125 [00:38<01:17,  1.09batch/s, Loss=0.436]\u001b[A\n",
      "Epoch 25/25:  33%|███████▊                | 41/125 [00:39<01:17,  1.09batch/s, Loss=0.422]\u001b[A\n",
      "Epoch 25/25:  34%|████████                | 42/125 [00:39<01:16,  1.09batch/s, Loss=0.422]\u001b[A\n",
      "Epoch 25/25:  34%|████████                | 42/125 [00:39<01:16,  1.09batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 25/25:  34%|████████▎               | 43/125 [00:39<01:15,  1.09batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 25/25:  34%|████████▎               | 43/125 [00:40<01:15,  1.09batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 25/25:  35%|████████▍               | 44/125 [00:40<01:14,  1.09batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 25/25:  35%|████████▍               | 44/125 [00:41<01:14,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 25/25:  36%|████████▋               | 45/125 [00:41<01:13,  1.09batch/s, Loss=0.541]\u001b[A\n",
      "Epoch 25/25:  36%|████████▋               | 45/125 [00:42<01:13,  1.09batch/s, Loss=0.687]\u001b[A\n",
      "Epoch 25/25:  37%|████████▊               | 46/125 [00:42<01:12,  1.09batch/s, Loss=0.687]\u001b[A\n",
      "Epoch 25/25:  37%|████████▊               | 46/125 [00:43<01:12,  1.09batch/s, Loss=0.695]\u001b[A\n",
      "Epoch 25/25:  38%|█████████               | 47/125 [00:43<01:12,  1.08batch/s, Loss=0.695]\u001b[A\n",
      "Epoch 25/25:  38%|█████████               | 47/125 [00:44<01:12,  1.08batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 25/25:  38%|█████████▏              | 48/125 [00:44<01:11,  1.08batch/s, Loss=0.289]\u001b[A\n",
      "Epoch 25/25:  38%|█████████▏              | 48/125 [00:45<01:11,  1.08batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 25/25:  39%|█████████▍              | 49/125 [00:45<01:10,  1.08batch/s, Loss=0.518]\u001b[A\n",
      "Epoch 25/25:  39%|█████████▍              | 49/125 [00:46<01:10,  1.08batch/s, Loss=0.602]\u001b[A\n",
      "Epoch 25/25:  40%|█████████▌              | 50/125 [00:46<01:09,  1.08batch/s, Loss=0.602]\u001b[A\n",
      "Epoch 25/25:  40%|█████████▌              | 50/125 [00:47<01:09,  1.08batch/s, Loss=0.815]\u001b[A\n",
      "Epoch 25/25:  41%|█████████▊              | 51/125 [00:47<01:08,  1.08batch/s, Loss=0.815]\u001b[A\n",
      "Epoch 25/25:  41%|█████████▊              | 51/125 [00:48<01:08,  1.08batch/s, Loss=0.227]\u001b[A\n",
      "Epoch 25/25:  42%|█████████▉              | 52/125 [00:48<01:07,  1.08batch/s, Loss=0.227]\u001b[A\n",
      "Epoch 25/25:  42%|█████████▉              | 52/125 [00:49<01:07,  1.08batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 25/25:  42%|██████████▏             | 53/125 [00:49<01:06,  1.08batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 25/25:  42%|██████████▌              | 53/125 [00:50<01:06,  1.08batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 25/25:  43%|██████████▊              | 54/125 [00:50<01:05,  1.08batch/s, Loss=0.34]\u001b[A\n",
      "Epoch 25/25:  43%|██████████▎             | 54/125 [00:51<01:05,  1.08batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 25/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.08batch/s, Loss=0.604]\u001b[A\n",
      "Epoch 25/25:  44%|██████████▌             | 55/125 [00:51<01:04,  1.08batch/s, Loss=0.455]\u001b[A\n",
      "Epoch 25/25:  45%|██████████▊             | 56/125 [00:51<01:03,  1.08batch/s, Loss=0.455]\u001b[A\n",
      "Epoch 25/25:  45%|██████████▊             | 56/125 [00:52<01:03,  1.08batch/s, Loss=0.372]\u001b[A\n",
      "Epoch 25/25:  46%|██████████▉             | 57/125 [00:52<01:02,  1.08batch/s, Loss=0.372]\u001b[A\n",
      "Epoch 25/25:  46%|██████████▉             | 57/125 [00:53<01:02,  1.08batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 25/25:  46%|███████████▏            | 58/125 [00:53<01:01,  1.08batch/s, Loss=0.555]\u001b[A\n",
      "Epoch 25/25:  46%|███████████▏            | 58/125 [00:54<01:01,  1.08batch/s, Loss=0.213]\u001b[A\n",
      "Epoch 25/25:  47%|███████████▎            | 59/125 [00:54<01:00,  1.09batch/s, Loss=0.213]\u001b[A\n",
      "Epoch 25/25:  47%|███████████▎            | 59/125 [00:55<01:00,  1.09batch/s, Loss=0.335]\u001b[A\n",
      "Epoch 25/25:  48%|███████████▌            | 60/125 [00:55<00:59,  1.09batch/s, Loss=0.335]\u001b[A\n",
      "Epoch 25/25:  48%|███████████▌            | 60/125 [00:56<00:59,  1.09batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 25/25:  49%|███████████▋            | 61/125 [00:56<00:59,  1.08batch/s, Loss=0.343]\u001b[A\n",
      "Epoch 25/25:  49%|███████████▋            | 61/125 [00:57<00:59,  1.08batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 25/25:  50%|███████████▉            | 62/125 [00:57<00:58,  1.08batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 25/25:  50%|███████████▉            | 62/125 [00:58<00:58,  1.08batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 25/25:  50%|████████████            | 63/125 [00:58<00:57,  1.08batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 25/25:  50%|████████████            | 63/125 [00:59<00:57,  1.08batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 25/25:  51%|████████████▎           | 64/125 [00:59<00:56,  1.08batch/s, Loss=0.393]\u001b[A\n",
      "Epoch 25/25:  51%|█████████████▎            | 64/125 [01:00<00:56,  1.08batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 25/25:  52%|█████████████▌            | 65/125 [01:00<00:55,  1.08batch/s, Loss=0.3]\u001b[A\n",
      "Epoch 25/25:  52%|█████████████            | 65/125 [01:01<00:55,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 25/25:  53%|█████████████▏           | 66/125 [01:01<00:54,  1.08batch/s, Loss=0.35]\u001b[A\n",
      "Epoch 25/25:  53%|████████████▋           | 66/125 [01:02<00:54,  1.08batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 25/25:  54%|████████████▊           | 67/125 [01:02<00:53,  1.08batch/s, Loss=0.333]\u001b[A\n",
      "Epoch 25/25:  54%|████████████▊           | 67/125 [01:03<00:53,  1.08batch/s, Loss=0.422]\u001b[A\n",
      "Epoch 25/25:  54%|█████████████           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.422]\u001b[A\n",
      "Epoch 25/25:  54%|█████████████▌           | 68/125 [01:03<00:52,  1.09batch/s, Loss=0.32]\u001b[A\n",
      "Epoch 25/25:  55%|█████████████▊           | 69/125 [01:03<00:51,  1.09batch/s, Loss=0.32]\u001b[A\n",
      "Epoch 25/25:  55%|█████████████▏          | 69/125 [01:04<00:51,  1.09batch/s, Loss=0.771]\u001b[A\n",
      "Epoch 25/25:  56%|█████████████▍          | 70/125 [01:04<00:50,  1.09batch/s, Loss=0.771]\u001b[A\n",
      "Epoch 25/25:  56%|█████████████▍          | 70/125 [01:05<00:50,  1.09batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 25/25:  57%|█████████████▋          | 71/125 [01:05<00:49,  1.09batch/s, Loss=0.274]\u001b[A\n",
      "Epoch 25/25:  57%|██████████████▏          | 71/125 [01:06<00:49,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 25/25:  58%|██████████████▍          | 72/125 [01:06<00:48,  1.09batch/s, Loss=0.55]\u001b[A\n",
      "Epoch 25/25:  58%|██████████████▍          | 72/125 [01:07<00:48,  1.09batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 25/25:  58%|██████████████▌          | 73/125 [01:07<00:47,  1.09batch/s, Loss=0.43]\u001b[A\n",
      "Epoch 25/25:  58%|██████████████          | 73/125 [01:08<00:47,  1.09batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 25/25:  59%|██████████████▏         | 74/125 [01:08<00:46,  1.09batch/s, Loss=0.536]\u001b[A\n",
      "Epoch 25/25:  59%|██████████████▏         | 74/125 [01:09<00:46,  1.09batch/s, Loss=0.242]\u001b[A\n",
      "Epoch 25/25:  60%|██████████████▍         | 75/125 [01:09<00:45,  1.09batch/s, Loss=0.242]\u001b[A\n",
      "Epoch 25/25:  60%|██████████████▍         | 75/125 [01:10<00:45,  1.09batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 25/25:  61%|██████████████▌         | 76/125 [01:10<00:45,  1.09batch/s, Loss=0.276]\u001b[A\n",
      "Epoch 25/25:  61%|██████████████▌         | 76/125 [01:11<00:45,  1.09batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 25/25:  62%|██████████████▊         | 77/125 [01:11<00:44,  1.09batch/s, Loss=0.489]\u001b[A\n",
      "Epoch 25/25:  62%|██████████████▊         | 77/125 [01:12<00:44,  1.09batch/s, Loss=0.209]\u001b[A\n",
      "Epoch 25/25:  62%|██████████████▉         | 78/125 [01:12<00:43,  1.09batch/s, Loss=0.209]\u001b[A\n",
      "Epoch 25/25:  62%|██████████████▉         | 78/125 [01:13<00:43,  1.09batch/s, Loss=0.208]\u001b[A\n",
      "Epoch 25/25:  63%|███████████████▏        | 79/125 [01:13<00:42,  1.09batch/s, Loss=0.208]\u001b[A\n",
      "Epoch 25/25:  63%|███████████████▏        | 79/125 [01:14<00:42,  1.09batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 25/25:  64%|███████████████▎        | 80/125 [01:14<00:41,  1.09batch/s, Loss=0.317]\u001b[A\n",
      "Epoch 25/25:  64%|███████████████▎        | 80/125 [01:15<00:41,  1.09batch/s, Loss=0.909]\u001b[A\n",
      "Epoch 25/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.909]\u001b[A\n",
      "Epoch 25/25:  65%|███████████████▌        | 81/125 [01:15<00:40,  1.09batch/s, Loss=0.652]\u001b[A\n",
      "Epoch 25/25:  66%|███████████████▋        | 82/125 [01:15<00:39,  1.09batch/s, Loss=0.652]\u001b[A\n",
      "Epoch 25/25:  66%|███████████████▋        | 82/125 [01:16<00:39,  1.09batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 25/25:  66%|███████████████▉        | 83/125 [01:16<00:39,  1.07batch/s, Loss=0.399]\u001b[A\n",
      "Epoch 25/25:  66%|███████████████▉        | 83/125 [01:17<00:39,  1.07batch/s, Loss=0.185]\u001b[A\n",
      "Epoch 25/25:  67%|████████████████▏       | 84/125 [01:17<00:38,  1.07batch/s, Loss=0.185]\u001b[A\n",
      "Epoch 25/25:  67%|████████████████▏       | 84/125 [01:18<00:38,  1.07batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 25/25:  68%|████████████████▎       | 85/125 [01:18<00:37,  1.07batch/s, Loss=0.769]\u001b[A\n",
      "Epoch 25/25:  68%|█████████████████▋        | 85/125 [01:19<00:37,  1.07batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 25/25:  69%|█████████████████▉        | 86/125 [01:19<00:36,  1.08batch/s, Loss=0.4]\u001b[A\n",
      "Epoch 25/25:  69%|████████████████▌       | 86/125 [01:20<00:36,  1.08batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 25/25:  70%|████████████████▋       | 87/125 [01:20<00:35,  1.08batch/s, Loss=0.256]\u001b[A\n",
      "Epoch 25/25:  70%|████████████████▋       | 87/125 [01:21<00:35,  1.08batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 25/25:  70%|████████████████▉       | 88/125 [01:21<00:34,  1.08batch/s, Loss=0.443]\u001b[A\n",
      "Epoch 25/25:  70%|████████████████▉       | 88/125 [01:22<00:34,  1.08batch/s, Loss=0.777]\u001b[A\n",
      "Epoch 25/25:  71%|█████████████████       | 89/125 [01:22<00:33,  1.08batch/s, Loss=0.777]\u001b[A\n",
      "Epoch 25/25:  71%|█████████████████       | 89/125 [01:23<00:33,  1.08batch/s, Loss=0.533]\u001b[A\n",
      "Epoch 25/25:  72%|█████████████████▎      | 90/125 [01:23<00:32,  1.08batch/s, Loss=0.533]\u001b[A\n",
      "Epoch 25/25:  72%|█████████████████▎      | 90/125 [01:24<00:32,  1.08batch/s, Loss=0.237]\u001b[A\n",
      "Epoch 25/25:  73%|█████████████████▍      | 91/125 [01:24<00:31,  1.09batch/s, Loss=0.237]\u001b[A\n",
      "Epoch 25/25:  73%|█████████████████▍      | 91/125 [01:25<00:31,  1.09batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 25/25:  74%|█████████████████▋      | 92/125 [01:25<00:30,  1.08batch/s, Loss=0.476]\u001b[A\n",
      "Epoch 25/25:  74%|█████████████████▋      | 92/125 [01:26<00:30,  1.08batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 25/25:  74%|█████████████████▊      | 93/125 [01:26<00:29,  1.08batch/s, Loss=0.465]\u001b[A\n",
      "Epoch 25/25:  74%|█████████████████▊      | 93/125 [01:27<00:29,  1.08batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 25/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.09batch/s, Loss=0.311]\u001b[A\n",
      "Epoch 25/25:  75%|██████████████████      | 94/125 [01:27<00:28,  1.09batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 25/25:  76%|██████████████████▏     | 95/125 [01:27<00:27,  1.09batch/s, Loss=0.297]\u001b[A\n",
      "Epoch 25/25:  76%|██████████████████▏     | 95/125 [01:28<00:27,  1.09batch/s, Loss=0.398]\u001b[A\n",
      "Epoch 25/25:  77%|██████████████████▍     | 96/125 [01:28<00:26,  1.09batch/s, Loss=0.398]\u001b[A\n",
      "Epoch 25/25:  77%|██████████████████▍     | 96/125 [01:29<00:26,  1.09batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 25/25:  78%|██████████████████▌     | 97/125 [01:29<00:25,  1.09batch/s, Loss=0.617]\u001b[A\n",
      "Epoch 25/25:  78%|██████████████████▌     | 97/125 [01:30<00:25,  1.09batch/s, Loss=0.477]\u001b[A\n",
      "Epoch 25/25:  78%|██████████████████▊     | 98/125 [01:30<00:24,  1.09batch/s, Loss=0.477]\u001b[A\n",
      "Epoch 25/25:  78%|██████████████████▊     | 98/125 [01:31<00:24,  1.09batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 25/25:  79%|███████████████████     | 99/125 [01:31<00:23,  1.09batch/s, Loss=0.619]\u001b[A\n",
      "Epoch 25/25:  79%|███████████████████     | 99/125 [01:32<00:23,  1.09batch/s, Loss=0.348]\u001b[A\n",
      "Epoch 25/25:  80%|██████████████████▍    | 100/125 [01:32<00:22,  1.09batch/s, Loss=0.348]\u001b[A\n",
      "Epoch 25/25:  80%|██████████████████▍    | 100/125 [01:33<00:22,  1.09batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 25/25:  81%|██████████████████▌    | 101/125 [01:33<00:22,  1.09batch/s, Loss=0.459]\u001b[A\n",
      "Epoch 25/25:  81%|██████████████████▌    | 101/125 [01:34<00:22,  1.09batch/s, Loss=0.342]\u001b[A\n",
      "Epoch 25/25:  82%|██████████████████▊    | 102/125 [01:34<00:21,  1.08batch/s, Loss=0.342]\u001b[A\n",
      "Epoch 25/25:  82%|██████████████████▊    | 102/125 [01:35<00:21,  1.08batch/s, Loss=0.155]\u001b[A\n",
      "Epoch 25/25:  82%|██████████████████▉    | 103/125 [01:35<00:20,  1.08batch/s, Loss=0.155]\u001b[A\n",
      "Epoch 25/25:  82%|██████████████████▉    | 103/125 [01:36<00:20,  1.08batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 25/25:  83%|███████████████████▏   | 104/125 [01:36<00:19,  1.08batch/s, Loss=0.356]\u001b[A\n",
      "Epoch 25/25:  83%|███████████████████▏   | 104/125 [01:37<00:19,  1.08batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 25/25:  84%|███████████████████▎   | 105/125 [01:37<00:18,  1.09batch/s, Loss=0.448]\u001b[A\n",
      "Epoch 25/25:  84%|███████████████████▎   | 105/125 [01:38<00:18,  1.09batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 25/25:  85%|███████████████████▌   | 106/125 [01:38<00:17,  1.09batch/s, Loss=0.327]\u001b[A\n",
      "Epoch 25/25:  85%|███████████████████▌   | 106/125 [01:39<00:17,  1.09batch/s, Loss=0.143]\u001b[A\n",
      "Epoch 25/25:  86%|███████████████████▋   | 107/125 [01:39<00:16,  1.09batch/s, Loss=0.143]\u001b[A\n",
      "Epoch 25/25:  86%|████████████████████▌   | 107/125 [01:39<00:16,  1.09batch/s, Loss=0.15]\u001b[A\n",
      "Epoch 25/25:  86%|████████████████████▋   | 108/125 [01:39<00:15,  1.09batch/s, Loss=0.15]\u001b[A\n",
      "Epoch 25/25:  86%|███████████████████▊   | 108/125 [01:40<00:15,  1.09batch/s, Loss=0.232]\u001b[A\n",
      "Epoch 25/25:  87%|████████████████████   | 109/125 [01:40<00:14,  1.09batch/s, Loss=0.232]\u001b[A\n",
      "Epoch 25/25:  87%|████████████████████   | 109/125 [01:41<00:14,  1.09batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 25/25:  88%|████████████████████▏  | 110/125 [01:41<00:13,  1.08batch/s, Loss=0.433]\u001b[A\n",
      "Epoch 25/25:  88%|████████████████████▏  | 110/125 [01:42<00:13,  1.08batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 25/25:  89%|████████████████████▍  | 111/125 [01:42<00:13,  1.08batch/s, Loss=0.434]\u001b[A\n",
      "Epoch 25/25:  89%|████████████████████▍  | 111/125 [01:43<00:13,  1.08batch/s, Loss=0.231]\u001b[A\n",
      "Epoch 25/25:  90%|████████████████████▌  | 112/125 [01:43<00:12,  1.08batch/s, Loss=0.231]\u001b[A\n",
      "Epoch 25/25:  90%|████████████████████▌  | 112/125 [01:44<00:12,  1.08batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 25/25:  90%|████████████████████▊  | 113/125 [01:44<00:11,  1.08batch/s, Loss=0.446]\u001b[A\n",
      "Epoch 25/25:  90%|████████████████████▊  | 113/125 [01:45<00:11,  1.08batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 25/25:  91%|████████████████████▉  | 114/125 [01:45<00:10,  1.08batch/s, Loss=0.338]\u001b[A\n",
      "Epoch 25/25:  91%|████████████████████▉  | 114/125 [01:46<00:10,  1.08batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 25/25:  92%|█████████████████████▏ | 115/125 [01:46<00:09,  1.08batch/s, Loss=0.371]\u001b[A\n",
      "Epoch 25/25:  92%|█████████████████████▏ | 115/125 [01:47<00:09,  1.08batch/s, Loss=0.253]\u001b[A\n",
      "Epoch 25/25:  93%|█████████████████████▎ | 116/125 [01:47<00:08,  1.09batch/s, Loss=0.253]\u001b[A\n",
      "Epoch 25/25:  93%|█████████████████████▎ | 116/125 [01:48<00:08,  1.09batch/s, Loss=0.684]\u001b[A\n",
      "Epoch 25/25:  94%|█████████████████████▌ | 117/125 [01:48<00:07,  1.09batch/s, Loss=0.684]\u001b[A\n",
      "Epoch 25/25:  94%|█████████████████████▌ | 117/125 [01:49<00:07,  1.09batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 25/25:  94%|█████████████████████▋ | 118/125 [01:49<00:06,  1.09batch/s, Loss=0.384]\u001b[A\n",
      "Epoch 25/25:  94%|██████████████████████▋ | 118/125 [01:50<00:06,  1.09batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 25/25:  95%|██████████████████████▊ | 119/125 [01:50<00:05,  1.09batch/s, Loss=0.69]\u001b[A\n",
      "Epoch 25/25:  95%|██████████████████████▊ | 119/125 [01:51<00:05,  1.09batch/s, Loss=0.13]\u001b[A\n",
      "Epoch 25/25:  96%|███████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.13]\u001b[A\n",
      "Epoch 25/25:  96%|██████████████████████ | 120/125 [01:51<00:04,  1.09batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 25/25:  97%|██████████████████████▎| 121/125 [01:51<00:03,  1.09batch/s, Loss=0.551]\u001b[A\n",
      "Epoch 25/25:  97%|██████████████████████▎| 121/125 [01:52<00:03,  1.09batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 25/25:  98%|██████████████████████▍| 122/125 [01:52<00:02,  1.09batch/s, Loss=0.263]\u001b[A\n",
      "Epoch 25/25:  98%|███████████████████████▍| 122/125 [01:53<00:02,  1.09batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 25/25:  98%|███████████████████████▌| 123/125 [01:53<00:01,  1.09batch/s, Loss=0.59]\u001b[A\n",
      "Epoch 25/25:  98%|██████████████████████▋| 123/125 [01:54<00:01,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 25/25:  99%|██████████████████████▊| 124/125 [01:54<00:00,  1.09batch/s, Loss=0.379]\u001b[A\n",
      "Epoch 25/25:  99%|██████████████████████▊| 124/125 [01:55<00:00,  1.09batch/s, Loss=0.386]\u001b[A\n",
      "Epoch 25/25: 100%|███████████████████████| 125/125 [01:55<00:00,  1.08batch/s, Loss=0.386]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Epoch 25/25:   0%|                                  | 0/125 [00:00<?, ?batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   1%|▏                         | 1/125 [00:00<00:44,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   2%|▍                         | 2/125 [00:00<00:45,  2.70batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   2%|▌                         | 3/125 [00:01<00:44,  2.72batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   3%|▊                         | 4/125 [00:01<00:43,  2.75batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   4%|█                         | 5/125 [00:01<00:43,  2.76batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   5%|█▏                        | 6/125 [00:02<00:43,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   6%|█▍                        | 7/125 [00:02<00:42,  2.77batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   6%|█▋                        | 8/125 [00:02<00:42,  2.78batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   7%|█▊                        | 9/125 [00:03<00:41,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   8%|██                       | 10/125 [00:03<00:41,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:   9%|██▏                      | 11/125 [00:03<00:40,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  10%|██▍                      | 12/125 [00:04<00:40,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  10%|██▌                      | 13/125 [00:04<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  11%|██▊                      | 14/125 [00:05<00:39,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  12%|███                      | 15/125 [00:05<00:39,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  13%|███▏                     | 16/125 [00:05<00:38,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  14%|███▍                     | 17/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  14%|███▌                     | 18/125 [00:06<00:38,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  15%|███▊                     | 19/125 [00:06<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  16%|████                     | 20/125 [00:07<00:37,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  17%|████▏                    | 21/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  18%|████▍                    | 22/125 [00:07<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  18%|████▌                    | 23/125 [00:08<00:36,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  19%|████▊                    | 24/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  20%|█████                    | 25/125 [00:08<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  21%|█████▏                   | 26/125 [00:09<00:35,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  22%|█████▍                   | 27/125 [00:09<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  22%|█████▌                   | 28/125 [00:10<00:34,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  23%|█████▊                   | 29/125 [00:10<00:34,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  24%|██████                   | 30/125 [00:10<00:33,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  25%|██████▏                  | 31/125 [00:11<00:33,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  26%|██████▍                  | 32/125 [00:11<00:33,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  26%|██████▌                  | 33/125 [00:11<00:32,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  27%|██████▊                  | 34/125 [00:12<00:32,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  28%|███████                  | 35/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  29%|███████▏                 | 36/125 [00:12<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  30%|███████▍                 | 37/125 [00:13<00:31,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  30%|███████▌                 | 38/125 [00:13<00:30,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  31%|███████▊                 | 39/125 [00:13<00:30,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  32%|████████                 | 40/125 [00:14<00:30,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  33%|████████▏                | 41/125 [00:14<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  34%|████████▍                | 42/125 [00:14<00:29,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  34%|████████▌                | 43/125 [00:15<00:29,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  35%|████████▊                | 44/125 [00:15<00:28,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  36%|█████████                | 45/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  37%|█████████▏               | 46/125 [00:16<00:28,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  38%|█████████▍               | 47/125 [00:16<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  38%|█████████▌               | 48/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  39%|█████████▊               | 49/125 [00:17<00:27,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  40%|██████████               | 50/125 [00:17<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  41%|██████████▏              | 51/125 [00:18<00:26,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  42%|██████████▍              | 52/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  42%|██████████▌              | 53/125 [00:18<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  43%|██████████▊              | 54/125 [00:19<00:25,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  44%|███████████              | 55/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  45%|███████████▏             | 56/125 [00:19<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  46%|███████████▍             | 57/125 [00:20<00:24,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  46%|███████████▌             | 58/125 [00:20<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  47%|███████████▊             | 59/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  48%|████████████             | 60/125 [00:21<00:23,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  49%|████████████▏            | 61/125 [00:21<00:22,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  50%|████████████▍            | 62/125 [00:22<00:22,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  50%|████████████▌            | 63/125 [00:22<00:22,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  51%|████████████▊            | 64/125 [00:22<00:21,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  52%|█████████████            | 65/125 [00:23<00:21,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  53%|█████████████▏           | 66/125 [00:23<00:21,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  54%|█████████████▍           | 67/125 [00:23<00:20,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  54%|█████████████▌           | 68/125 [00:24<00:20,  2.79batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  55%|█████████████▊           | 69/125 [00:24<00:20,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  56%|██████████████           | 70/125 [00:24<00:19,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  57%|██████████████▏          | 71/125 [00:25<00:19,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  58%|██████████████▍          | 72/125 [00:25<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  58%|██████████████▌          | 73/125 [00:26<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  59%|██████████████▊          | 74/125 [00:26<00:18,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  60%|███████████████          | 75/125 [00:26<00:17,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  61%|███████████████▏         | 76/125 [00:27<00:17,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  62%|███████████████▍         | 77/125 [00:27<00:17,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  62%|███████████████▌         | 78/125 [00:27<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  63%|███████████████▊         | 79/125 [00:28<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  64%|████████████████         | 80/125 [00:28<00:16,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  65%|████████████████▏        | 81/125 [00:28<00:15,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  66%|████████████████▍        | 82/125 [00:29<00:15,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  66%|████████████████▌        | 83/125 [00:29<00:15,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  67%|████████████████▊        | 84/125 [00:29<00:14,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  68%|█████████████████        | 85/125 [00:30<00:14,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  69%|█████████████████▏       | 86/125 [00:30<00:13,  2.80batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  70%|█████████████████▍       | 87/125 [00:31<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  70%|█████████████████▌       | 88/125 [00:31<00:13,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  71%|█████████████████▊       | 89/125 [00:31<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  72%|██████████████████       | 90/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  73%|██████████████████▏      | 91/125 [00:32<00:12,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  74%|██████████████████▍      | 92/125 [00:32<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  74%|██████████████████▌      | 93/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  75%|██████████████████▊      | 94/125 [00:33<00:11,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  76%|███████████████████      | 95/125 [00:33<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  77%|███████████████████▏     | 96/125 [00:34<00:10,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  78%|███████████████████▍     | 97/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  78%|███████████████████▌     | 98/125 [00:34<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  79%|███████████████████▊     | 99/125 [00:35<00:09,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  80%|███████████████████▏    | 100/125 [00:35<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  81%|███████████████████▍    | 101/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  82%|███████████████████▌    | 102/125 [00:36<00:08,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  82%|███████████████████▊    | 103/125 [00:36<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  83%|███████████████████▉    | 104/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  84%|████████████████████▏   | 105/125 [00:37<00:07,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  85%|████████████████████▎   | 106/125 [00:37<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  86%|████████████████████▌   | 107/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  86%|████████████████████▋   | 108/125 [00:38<00:06,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  87%|████████████████████▉   | 109/125 [00:38<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  88%|█████████████████████   | 110/125 [00:39<00:05,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  89%|█████████████████████▎  | 111/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  90%|█████████████████████▌  | 112/125 [00:39<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  90%|█████████████████████▋  | 113/125 [00:40<00:04,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  91%|█████████████████████▉  | 114/125 [00:40<00:03,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  92%|██████████████████████  | 115/125 [00:40<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  93%|██████████████████████▎ | 116/125 [00:41<00:03,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  94%|██████████████████████▍ | 117/125 [00:41<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  94%|██████████████████████▋ | 118/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  95%|██████████████████████▊ | 119/125 [00:42<00:02,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  96%|███████████████████████ | 120/125 [00:42<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  97%|███████████████████████▏| 121/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  98%|███████████████████████▍| 122/125 [00:43<00:01,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  98%|███████████████████████▌| 123/125 [00:43<00:00,  2.81batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25:  99%|███████████████████████▊| 124/125 [00:44<00:00,  2.82batch/s]\u001b[A\n",
      "Evaluating Epoch 25/25: 100%|████████████████████████| 125/125 [00:44<00:00,  2.81batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Eval Accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch], dim=0)\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch], dim=0)\n",
    "    label = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 将预测转换成对数\n",
    "def compute_metrics(predictions, labels):\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 检查模型是否在正确的设备上\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 设置训练超参数\n",
    "num_epochs = 25\n",
    "batch_size = 8\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_input_ids = torch.stack([torch.tensor(t) for t in small_train_dataset[\"input_ids\"]], dim=0).to(device)\n",
    "train_attention_mask = torch.stack([torch.tensor(t) for t in small_train_dataset[\"attention_mask\"]], dim=0).to(device)\n",
    "train_labels = torch.tensor(small_train_dataset[\"label\"], dtype=torch.long).to(device)\n",
    "\n",
    "eval_input_ids = torch.stack([torch.tensor(t) for t in small_eval_dataset[\"input_ids\"]], dim=0).to(device)\n",
    "eval_attention_mask = torch.stack([torch.tensor(t) for t in small_eval_dataset[\"attention_mask\"]], dim=0).to(device)\n",
    "eval_labels = torch.tensor(small_eval_dataset[\"label\"], dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "eval_dataset = TensorDataset(eval_input_ids, eval_attention_mask, eval_labels)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask, label = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_samples += input_ids.size(0)\n",
    "        \n",
    "        train_progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    avg_train_loss = total_loss / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(eval_dataloader, desc=f\"Evaluating Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "            input_ids, attention_mask, label = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(output.logits, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        \n",
    "        eval_accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab4805-5f9b-4adf-a983-ef7901c1b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查训练数据\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"]))\n",
    "    print(\"Input IDs example:\", batch[\"input_ids\"][0])\n",
    "    print(\"Attention Mask shape:\", batch[\"attention_mask\"].shape)\n",
    "    print(\"Attention Mask example:\", batch[\"attention_mask\"][0])\n",
    "    print(\"Labels shape:\", batch[\"label\"].shape)\n",
    "    print(\"Labels example:\", batch[\"label\"][0])\n",
    "    break\n",
    "\n",
    "# 检查验证数据\n",
    "for batch in eval_dataloader:\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Input IDs example:\", batch[\"input_ids\"][0])\n",
    "    print(\"Attention Mask shape:\", batch[\"attention_mask\"].shape)\n",
    "    print(\"Attention Mask example:\", batch[\"attention_mask\"][0])\n",
    "    print(\"Labels shape:\", batch[\"label\"].shape)\n",
    "    print(\"Labels example:\", batch[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215c832-d300-4ef6-a362-7d4e5545777b",
   "metadata": {},
   "source": [
    "# 2. 成分相似性任务\n",
    "## 2.1. 准备BioBert微调需要的的标注数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5528b48-7a1e-41b4-841b-632e4ed04b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -!-!- BACKUP -!-!- 去掉宝拉珍选网站没有评级成分的functions的统一结尾，之后考虑回来进行对比分析！\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('../Desktop/Paula_s_Choice/Paula_SUM_LIST.csv')\n",
    "\n",
    "# 处理'functions'列\n",
    "df['functions'] = df['functions'].str.replace('We have not yet rated this ingredient because we have not had a chance to review the research on it.', '')\n",
    "\n",
    "# 保存新的CSV文件\n",
    "df.to_csv('../Desktop/Paula_s_Choice/Paula_SUM_LIST_NEW.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e6c3c9-ed13-4224-9773-96f78954b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df_sum = pd.read_csv('../Desktop/Paula_s_Choice/Paula_SUM_LIST.csv')\n",
    "df_final = pd.read_csv('../Desktop/Paula_s_Choice/Paula_detail_final_422_prepare.csv')\n",
    "\n",
    "# 合并两个 DataFrame,保留所有df_sum的行\n",
    "merged_df = pd.merge(df_sum, df_final, on='ingredient_name', how='left')\n",
    "\n",
    "# 填充空值\n",
    "merged_df['description'] = merged_df['description'].fillna('')\n",
    "merged_df['functions'] = merged_df['functions'].fillna('')\n",
    "merged_df['glance'] = merged_df['glance'].fillna('')\n",
    "\n",
    "# 比较三个列,将其合并为一个新的列\n",
    "merged_df['combined_text'] = merged_df.apply(lambda row: row['description'] if row['functions'] in row['description'] else \n",
    "                                            row['description'] + ' ' + row['functions'] if row['functions'] not in row['description'] else\n",
    "                                            row['description'], axis=1)\n",
    "\n",
    "# 保存合并后的 DataFrame 到 CSV 文件\n",
    "merged_df.to_csv('../Desktop/Paula_s_Choice/Paula_embedding_SUMLIST_before_422.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff0746fc-7e7a-4e6a-b715-6f685efb78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备最原始的替换性成分组\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('../Desktop/DBCosmetic/6903_Ingredients_INFO_After.csv')\n",
    "\n",
    "# 创建新的DataFrame\n",
    "pre_similarity = pd.DataFrame(columns=['component1', 'component2', 'similarity'])\n",
    "\n",
    "# 遍历每一行数据\n",
    "for index, row in df.iterrows():\n",
    "    ingredient_name = row['ingredient_name']\n",
    "    alternatives = row['alternatives']\n",
    "    \n",
    "    # 如果alternatives不为空\n",
    "    if alternatives and isinstance(alternatives, str):\n",
    "        # 将alternatives字符串按回车符分割成列表\n",
    "        alt_list = [alt.strip() for alt in alternatives.split('\\n') if alt.strip()]\n",
    "        \n",
    "        # 为每个替代成分创建一行数据\n",
    "        for alt in alt_list:\n",
    "            # 再次检查是否存在逗号,如果存在则按逗号分割\n",
    "            sub_alts = [sub_alt.strip() for sub_alt in alt.split(',') if sub_alt.strip()]\n",
    "            for sub_alt in sub_alts:\n",
    "                pre_similarity = pd.concat([pre_similarity, pd.DataFrame({'component1': [ingredient_name], 'component2': [sub_alt], 'similarity': [1]})], ignore_index=True)\n",
    "\n",
    "pre_similarity.to_csv('../Desktop/BioBERT/pre_alternatives.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6811f042-64f3-4fcd-9548-7a55b7f2218b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "python(9505) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(9506) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(9507) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(9508) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9509) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9510) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9511) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing ForkPoolWorker-5:   0%|                               | 0/1334 [00:00<?, ?it/s]python(9512) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9513) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9514) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9515) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9516) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9517) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9518) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9519) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9520) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9521) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9523) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9524) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9525) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "python(9526) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing ForkPoolWorker-5: 100%|████████████████████| 1334/1334 [05:48<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 更改相似性\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import multiprocess as mp\n",
    "\n",
    "# 读取6903_Ingredients_INFO_After.csv文件\n",
    "pre_similarity_df = pd.read_csv('../Desktop/T5/pre_alternatives.csv')\n",
    "\n",
    "# 读取Paula_SUM_LIST.csv文件\n",
    "paula_sum_list_df = pd.read_csv('../Desktop/Paula_s_Choice/Paula_embedding_SUMLIST_before_422.csv')\n",
    "\n",
    "# 定义一个函数来处理单个dataframe\n",
    "def process_dataframe(df):\n",
    "    standard_ingredients = {name.lower(): name for name in paula_sum_list_df['ingredient_name']}\n",
    "    df['component1'] = df['component1'].str.lower()\n",
    "    df['component2'] = df['component2'].str.lower()\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f'Processing {mp.current_process().name}'):\n",
    "        component1 = row['component1']\n",
    "        component2 = row['component2']\n",
    "        \n",
    "        # 先尝试精确匹配\n",
    "        if component1 in standard_ingredients:\n",
    "            df.at[index, 'component1'] = standard_ingredients[component1]\n",
    "        else:\n",
    "            # 如果没有精确匹配,再尝试模糊匹配\n",
    "            best_match1 = max(standard_ingredients.items(), key=lambda x: fuzz.token_set_ratio(x[0], component1))\n",
    "            if best_match1[1] != component1:\n",
    "                df.at[index, 'component1'] = best_match1[1]\n",
    "        \n",
    "        if component2 in standard_ingredients:\n",
    "            df.at[index, 'component2'] = standard_ingredients[component2]\n",
    "        else:\n",
    "            best_match2 = max(standard_ingredients.items(), key=lambda x: fuzz.token_set_ratio(x[0], component2))\n",
    "            if best_match2[1] != component2:\n",
    "                df.at[index, 'component2'] = best_match2[1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 创建进程池并处理数据\n",
    "num_processes = mp.cpu_count() * 2\n",
    "pool = mp.Pool(processes=num_processes)\n",
    "pre_similarity_df = pool.apply_async(process_dataframe, args=(pre_similarity_df.copy(),)).get()\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pre_similarity_df.to_csv('../Desktop/BioBERT/pre_alternatives2_422.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6abb1b34-b0ce-4d81-aee0-fdc1b2256fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个CSV文件\n",
    "pre_similarity_df = pd.read_csv('../Desktop/BioBERT/pre_alternatives2_422.csv')\n",
    "paula_detail_df = pd.read_csv('../Desktop/Paula_s_Choice/Paula_embedding_SUMLIST_before_422.csv')\n",
    "\n",
    "# 创建一个空列表来存储合并后的数据行\n",
    "merged_rows = []\n",
    "\n",
    "for index, row in pre_similarity_df.iterrows():\n",
    "    component1 = row['component1']\n",
    "    component2 = row['component2']\n",
    "    \n",
    "    # 在paula_detail_df中查找匹配的ingredient_name\n",
    "    row1 = paula_detail_df[paula_detail_df['ingredient_name'] == component1].iloc[0].to_dict()\n",
    "    row2 = paula_detail_df[paula_detail_df['ingredient_name'] == component2].iloc[0].to_dict()\n",
    "    \n",
    "    # 创建一个新的字典,包含所需的列\n",
    "    new_row = {\n",
    "        'ingredient_name1': row1['ingredient_name'],\n",
    "        'rating1': row1['rating'],\n",
    "        'functions1': row1['functions'],\n",
    "        'link1': row1['link'],\n",
    "        'benefits1': row1['benefits'],\n",
    "        'categories1': row1['categories'],\n",
    "        'glance1': row1['glance'],\n",
    "        'description1': row1['description'],\n",
    "        'references1': row1['references'],\n",
    "        'combined_text1': row1['combined_text'],\n",
    "        'ingredient_name2': row2['ingredient_name'],\n",
    "        'rating2': row2['rating'],\n",
    "        'functions2': row2['functions'],\n",
    "        'link2': row2['link'],\n",
    "        'benefits2': row2['benefits'],\n",
    "        'categories2': row2['categories'],\n",
    "        'glance2': row2['glance'],\n",
    "        'description2': row2['description'],\n",
    "        'references2': row2['references'],\n",
    "        'combined_text2': row2['combined_text']\n",
    "    }\n",
    "    \n",
    "    # 将新的字典添加到列表\n",
    "    merged_rows.append(new_row)\n",
    "\n",
    "# 创建合并后的DataFrame\n",
    "merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "# 保存合并后的DataFrame到新的CSV文件\n",
    "merged_df.to_csv('../Desktop/BioBERT/pre_alternatives3_422.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c56160f-409f-4774-9234-ebd9496eeec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有 0 行数据存在component1或component2为空值。\n",
      "Empty DataFrame\n",
      "Columns: [ingredient_name1, rating1, functions1, link1, benefits1, categories1, glance1, description1, references1, combined_text1, ingredient_name2, rating2, functions2, link2, benefits2, categories2, glance2, description2, references2, combined_text2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 检查有没有空的\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('../Desktop/BioBERT/pre_alternatives3_422.csv')\n",
    "\n",
    "# 检查ingredient_name_1和ingredient_name_2是否存在空值\n",
    "null_rows = df[(df['ingredient_name1'].isnull()) | (df['ingredient_name2'].isnull())]\n",
    "\n",
    "# 统计空值行数\n",
    "null_count = len(null_rows)\n",
    "print(f\"有 {null_count} 行数据存在component1或component2为空值。\")\n",
    "\n",
    "# 打印这些含有空值的数据行\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e140d-c374-4fd1-9327-0c21ea0cf157",
   "metadata": {},
   "source": [
    "## 2.2. 正式微调\n",
    "### 法1. 将一对embedding分别作为x、y进行训练（放弃）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5e2b5f-314b-4622-8e87-9f3c678385c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# 加载数据集\n",
    "data = pd.read_csv('../Desktop/BioBERT/pre_alternatives3.csv')\n",
    "ingredient_text1 = data['combined_text1'].tolist()\n",
    "ingredient_text2 = data['combined_text2'].tolist()\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "\n",
    "# 定义LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    # bias=\"none\",\n",
    "    # target_modules=[\"encoder.layernorm\", \"decoder.layernorm\", \"encoder.layer.*.attention.k_proj\", \"encoder.layer.*.attention.v_proj\", \"decoder.layer.*.attention.k_proj\", \"decoder.layer.*.attention.v_proj\", \"encoder.layer.*.ffn.intermediate.dense\", \"decoder.layer.*.ffn.intermediate.dense\"],\n",
    ")\n",
    "\n",
    "# 将LoRA插入模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658e00a5-0733-4530-ab96-58337784927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n",
      "1334\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 36] at entry 0 and [1, 25] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m valid_output_ids \u001b[38;5;241m=\u001b[39m [tokenizer(t[\u001b[38;5;241m1\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m valid_data]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 创建 TensorDataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mstack(train_output_ids))\n\u001b[1;32m     19\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mstack(valid_input_ids), torch\u001b[38;5;241m.\u001b[39mstack(valid_output_ids))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 36] at entry 0 and [1, 25] at entry 1"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# 打包成元组\n",
    "X = list(zip(ingredient_text1, ingredient_text2))\n",
    "print(len(X))\n",
    "train_data, valid_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# 将训练集和验证集分别编码为输入和输出\n",
    "train_input_ids = [tokenizer(t[0], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"] for t in train_data]\n",
    "train_output_ids = [tokenizer(t[1], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"] for t in train_data]\n",
    "\n",
    "valid_input_ids = [tokenizer(t[0], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"] for t in valid_data]\n",
    "valid_output_ids = [tokenizer(t[1], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"] for t in valid_data]\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(torch.stack(train_input_ids), torch.stack(train_output_ids))\n",
    "valid_dataset = TensorDataset(torch.stack(valid_input_ids), torch.stack(valid_output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ba620b-8feb-40b0-be9f-530dc924f86e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:759\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 759\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 36 at dim 1 (got 25)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m train_text1, val_text1, train_text2, val_text2 \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      7\u001b[0m     ingredient_text1, ingredient_text2, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 编码数据\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_text1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m train_output_ids \u001b[38;5;241m=\u001b[39m tokenizer(train_text2, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m val_input_ids \u001b[38;5;241m=\u001b[39m tokenizer(val_text1, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m         )\n\u001b[1;32m   2957\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2997\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3149\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3140\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3141\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3142\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3147\u001b[0m )\n\u001b[0;32m-> 3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:224\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    220\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biobert-finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    771\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    772\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    778\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 检查是否可以使用 MPS 设备\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 将模型移到合适的设备上\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义优化器和训练超参数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, output_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        \n",
    "        output = model(input_ids=input_ids, labels=output_ids)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        total_train_samples += input_ids.size(0)\n",
    "        \n",
    "        train_progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    avg_train_loss = total_train_loss / total_train_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # 进行验证\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    total_valid_samples = 0\n",
    "    \n",
    "    for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        input_ids, output_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, labels=output_ids)\n",
    "            valid_loss = output.loss\n",
    "        \n",
    "        total_valid_loss += valid_loss.item()\n",
    "        total_valid_samples += input_ids.size(0)\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / total_valid_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_valid_loss:.4f}\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "model.save_pretrained(\"path/to/finetuned-biobert-nli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bd3a6-7552-480b-a801-a47de7c2cdd4",
   "metadata": {},
   "source": [
    "### 法2. 相似度 *** 重点 ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28779c93-6240-4f4b-85d3-5d94159fece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1067\n",
      "Validation set size: 267\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../Desktop/BioBERT/pre_alternatives3_422.csv')\n",
    "texts1 = data['combined_text1'].tolist()\n",
    "texts2 = data['combined_text2'].tolist()\n",
    "texts = list(zip(texts1, texts2))\n",
    "\n",
    "# 将数据集分为训练集和验证集\n",
    "train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c93816-1538-4f78-8a0c-43655a4d9b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Functions: Hair Conditioning, Skin Conditioning. We have not yet rated this ingredient because we have not had a chance to review the research on it.',\n",
       " 'Disodium laureth sulfosuccinate is a cleansing agent found in products such as face wash, bubble bath, and shampoo, as well as other personal care products. It can help boost the foaming properties of such formulas plus enhance the water solubility of other surfactants. Suppliers of this ingredient note its gentleness on skin.\\n\\nDisodium laureth sulfosuccinate can be sourced naturally (plant derived) or synthetically (lab created). It is described as a clear, colorless to slightly yellowish liquid in raw material form. Technically speaking, it is the disodium salt of an ethoxylated lauryl alcohol half ester of sulfosuccinic acid.\\n\\nThe 2015 Cosmetic Ingredient Review Expert Panel surveyed 607 personal care products containing disodium laureth sulfosuccinate in concentrations between 0.06% to 2% for leave-on formulas and 0.4% to 10% for rinse-off. They concluded that this ingredient is “safe in the present practices of use and concentration described in this safety assessment when formulated to be nonirritating.” Gentle cleansing agent noted for boosting foaming properties.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e4e7e8-ea17-402c-a448-83d815ac41c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# 检查是否可以使用 MPS 设备\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4ef004-4afe-48b4-8658-7d43bc93c6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,864 || all params: 108,347,136 || trainable%: 0.0340239727241152\n",
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 将 LoRA 插入模型\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 设置 LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    r=1, \n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd44a5-9bcd-4a5b-868f-7834f2c9b26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 (Training):   0%|                                    | 0/134 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/3 (Training): 100%|████| 134/134 [5:10:54<00:00, 139.21s/batch, Train Loss=-0.305]\n",
      "Epoch 1/3 (Validation): 100%|█████████| 34/34 [50:01<00:00, 88.29s/batch, Val Loss=-0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: -0.3052, Val Loss: -0.5084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 (Training):  13%|▋    | 18/134 [2:14:07<2:47:10, 86.47s/batch, Train Loss=-0.31]"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class TextPairDataset(Dataset):\n",
    "#     def __init__(self, text_pairs):\n",
    "#         self.text_pairs = text_pairs\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.text_pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text1, text2 = self.text_pairs[idx]\n",
    "#         # print(text1)\n",
    "#         # text1 = tokenizer.encode(text1, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#         # text2 = tokenizer.encode(text2,padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#         # print(text1)\n",
    "#         return text1, text2\n",
    "\n",
    "# def fine_tune_model(model, train_texts, val_texts, num_epochs=3, batch_size=8, lr=2e-5):\n",
    "#     train_dataset = TextPairDataset(train_texts)\n",
    "#     val_dataset = TextPairDataset(val_texts)\n",
    "\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "#     optimizer = AdamW(model.parameters(), lr=lr)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs\n",
    "#     )\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\", unit=\"batch\")\n",
    "#         for sentences_a, sentences_b in train_progress_bar:\n",
    "#             encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "#             encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#             sentence_embeddings_a = model(**encoded_input_a).last_hidden_state\n",
    "#             sentence_embeddings_b = model(**encoded_input_b).last_hidden_state\n",
    "\n",
    "#             loss = -torch.mean(torch.cosine_similarity(sentence_embeddings_a, sentence_embeddings_b, dim=1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             train_loss += loss.item()\n",
    "#             train_progress_bar.set_postfix({\"Train Loss\": train_loss / (train_progress_bar.n + 1)})\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "        \n",
    "#         val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\", unit=\"batch\")\n",
    "#         for sentences_a, sentences_b in val_progress_bar:\n",
    "#             encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "#             encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 sentence_embeddings_a = model(**encoded_input_a).last_hidden_state\n",
    "#                 sentence_embeddings_b = model(**encoded_input_b).last_hidden_state\n",
    "#             loss = -torch.mean(torch.cosine_similarity(sentence_embeddings_a, sentence_embeddings_b, dim=-1))\n",
    "#             val_loss += loss.item()\n",
    "#             val_progress_bar.set_postfix({\"Val Loss\": val_loss / (val_progress_bar.n + 1)})\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_dataloader):.4f}, Val Loss: {val_loss/len(val_dataloader):.4f}\")\n",
    "\n",
    "#     return model\n",
    "\n",
    "# fine_tune_model(model, train_texts, val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9fe57-853a-42b0-a8cc-9b75f206a3a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 修正了一些细节\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, text_pairs):\n",
    "        self.text_pairs = text_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, text2 = self.text_pairs[idx]\n",
    "        return text1, text2\n",
    "\n",
    "def fine_tune_model(model, train_texts, val_texts, num_epochs=3, batch_size=8, lr=5e-6, accumulate_grad_batches=1):\n",
    "    train_dataset = TextPairDataset(train_texts)\n",
    "    val_dataset = TextPairDataset(val_texts)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)  # 添加 L2 正则化\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs // accumulate_grad_batches\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\", unit=\"batch\")\n",
    "        for i, (sentences_a, sentences_b) in enumerate(train_progress_bar):\n",
    "            encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            sentence_embeddings_a = model(**encoded_input_a).last_hidden_state\n",
    "            sentence_embeddings_b = model(**encoded_input_b).last_hidden_state\n",
    "\n",
    "            loss = torch.mean(1 - torch.cosine_similarity(sentence_embeddings_a, sentence_embeddings_b, dim=1))\n",
    "            loss = loss / accumulate_grad_batches\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulate_grad_batches == 0 or (i + 1) == len(train_dataloader):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_loss += loss.item() * accumulate_grad_batches\n",
    "            train_progress_bar.set_postfix({\"Train Loss\": train_loss / ((i + 1) * batch_size)})\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_cos_sim = 0\n",
    "        val_acc = 0\n",
    "        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\", unit=\"batch\")\n",
    "        for sentences_a, sentences_b in val_progress_bar:\n",
    "            encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sentence_embeddings_a = model(**encoded_input_a).last_hidden_state\n",
    "                sentence_embeddings_b = model(**encoded_input_b).last_hidden_state\n",
    "                cos_sim = torch.cosine_similarity(sentence_embeddings_a, sentence_embeddings_b, dim=-1)\n",
    "                val_cos_sim += cos_sim.sum().item()\n",
    "                val_acc += (cos_sim >= 0.7).sum().item()\n",
    "            loss = torch.mean(1 - cos_sim)\n",
    "            val_loss += loss.item()\n",
    "            val_progress_bar.set_postfix({\"Val Loss\": val_loss / (val_progress_bar.n + 1)})\n",
    "            wandb.log({\"val_loss\": loss.item()})\n",
    "\n",
    "        val_cos_sim /= len(val_dataset)\n",
    "        val_acc /= len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_dataloader):.4f}, Val Loss: {val_loss/len(val_dataloader):.4f}, Val Cos Sim: {val_cos_sim:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        wandb.log({\"val_cos_sim\": val_cos_sim, \"val_acc\": val_acc})\n",
    "\n",
    "    wandb.finish()\n",
    "    return model\n",
    "\n",
    "fine_tune_model(model, train_texts, val_texts, accumulate_grad_batches=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6875b78b-869e-4907-8cc2-14460125b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存并上传\n",
    "model.save_pretrained(\"./myModel/\")\n",
    "# model.push_to_hub(\"my_awesome_peft_model\")\n",
    "from huggingface_hub import push_to_hub\n",
    "\n",
    "# 推送模型到 Hugging Face Hub\n",
    "push_to_hub(model, \"path/to/save/model\", \"your-username/your-model-repo-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43de23c-1898-4f83-9d6c-4cf62b74b2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81031de423f241b0976e4a254b4b43d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6704f32b984a42ba66cdd7bce3ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/154k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Autumn/biobert-cosmetic-ingredients-similarity/commit/b8140e5bedfa42586af5baef80346969d739601c', commit_message='Upload model', commit_description='', oid='b8140e5bedfa42586af5baef80346969d739601c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import push_to_hub\n",
    "model.push_to_hub(\"Autumn/biobert-cosmetic-ingredients-similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae071d5d-f3b0-49eb-ab28-7e03d59d8c02",
   "metadata": {},
   "source": [
    "## 2.3 验证集对比原模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0574bcec-6d17-41ff-921b-16714a7d64f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a87412-0ed1-4586-a0fc-935b9b2bed26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(32070) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32071) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32073) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32074) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Original Model:   0%|                                | 0/34 [00:00<?, ?batch/s]python(32075) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32077) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32078) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32079) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32080) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32083) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:   3%|▏       | 1/34 [00:16<09:13, 16.76s/batch, Val Loss=0.483]python(32085) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32086) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32087) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32088) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:   6%|▍       | 2/34 [00:27<07:08, 13.38s/batch, Val Loss=0.573]python(32089) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32090) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32091) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:   9%|▋       | 3/34 [00:36<05:50, 11.31s/batch, Val Loss=0.529]python(32092) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32093) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32094) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32095) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  12%|▉       | 4/34 [00:47<05:34, 11.16s/batch, Val Loss=0.503]python(32097) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32098) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32099) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32100) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  15%|█▎       | 5/34 [00:58<05:23, 11.16s/batch, Val Loss=0.51]python(32102) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32103) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32105) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32106) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32107) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  18%|█▍      | 6/34 [01:12<05:41, 12.20s/batch, Val Loss=0.492]python(32108) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32109) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32110) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32111) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32113) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  21%|█▋      | 7/34 [01:26<05:45, 12.79s/batch, Val Loss=0.501]python(32114) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32115) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32116) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32117) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32121) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32123) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32124) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32129) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32130) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32131) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32132) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32133) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  24%|█▉      | 8/34 [02:02<08:40, 20.00s/batch, Val Loss=0.491]python(32134) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32136) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32137) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32138) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32139) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32140) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32142) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32145) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32150) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32151) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32152) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  26%|██      | 9/34 [02:31<09:32, 22.89s/batch, Val Loss=0.509]python(32153) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32155) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32156) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32157) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32158) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32159) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  29%|██▎     | 10/34 [02:50<08:37, 21.58s/batch, Val Loss=0.51]python(32160) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32161) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32162) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32163) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32164) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32165) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  32%|██▎    | 11/34 [03:07<07:44, 20.19s/batch, Val Loss=0.504]python(32167) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32168) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32170) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32171) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32172) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32175) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32176) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32178) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32179) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  35%|██▍    | 12/34 [03:33<08:04, 22.02s/batch, Val Loss=0.497]python(32180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32181) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32182) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32183) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32184) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  38%|██▋    | 13/34 [03:46<06:46, 19.34s/batch, Val Loss=0.496]python(32185) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32186) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32187) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32188) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32189) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32190) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32191) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32193) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  41%|██▉    | 14/34 [04:09<06:50, 20.52s/batch, Val Loss=0.495]python(32194) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32196) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32198) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32199) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32201) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32204) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32207) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32210) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32212) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32217) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  44%|███    | 15/34 [04:36<07:03, 22.31s/batch, Val Loss=0.488]python(32220) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32221) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32225) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32227) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32228) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32229) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  47%|███▎   | 16/34 [04:53<06:15, 20.84s/batch, Val Loss=0.494]python(32230) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32231) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32232) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32235) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32236) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  50%|███▌   | 17/34 [05:06<05:13, 18.47s/batch, Val Loss=0.487]python(32239) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32240) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32241) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32242) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32247) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32248) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  53%|███▋   | 18/34 [05:34<05:39, 21.20s/batch, Val Loss=0.485]python(32249) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32250) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32251) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32252) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32253) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32254) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32255) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  56%|███▉   | 19/34 [05:53<05:08, 20.60s/batch, Val Loss=0.489]python(32256) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32257) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32260) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32261) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32262) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32263) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  59%|████▋   | 20/34 [06:11<04:37, 19.85s/batch, Val Loss=0.49]python(32265) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32266) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32267) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32268) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32269) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32270) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32271) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32272) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  62%|████▎  | 21/34 [06:32<04:20, 20.05s/batch, Val Loss=0.491]python(32273) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32274) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32275) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32276) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32280) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32288) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32290) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  65%|█████▏  | 22/34 [06:52<04:01, 20.10s/batch, Val Loss=0.49]python(32294) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32296) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32302) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32306) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  68%|████▋  | 23/34 [07:01<03:05, 16.90s/batch, Val Loss=0.488]python(32308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32309) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32310) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  71%|████▉  | 24/34 [07:11<02:26, 14.64s/batch, Val Loss=0.489]python(32312) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32315) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32316) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32317) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32318) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  74%|█████▏ | 25/34 [07:23<02:04, 13.87s/batch, Val Loss=0.483]python(32319) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32320) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32321) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  76%|█████▎ | 26/34 [07:30<01:35, 11.90s/batch, Val Loss=0.482]python(32322) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32323) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  79%|█████▌ | 27/34 [07:36<01:10, 10.07s/batch, Val Loss=0.479]python(32324) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32325) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  82%|██████▌ | 28/34 [07:42<00:53,  8.92s/batch, Val Loss=0.48]python(32326) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32327) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32328) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  85%|█████▉ | 29/34 [07:49<00:41,  8.37s/batch, Val Loss=0.478]python(32329) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32330) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  88%|███████ | 30/34 [07:56<00:31,  7.82s/batch, Val Loss=0.48]python(32331) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32332) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32333) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32334) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32337) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32341) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  91%|██████▍| 31/34 [08:14<00:32, 10.86s/batch, Val Loss=0.483]python(32343) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32346) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32349) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32350) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32354) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32357) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32358) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  94%|██████▌| 32/34 [08:35<00:27, 13.93s/batch, Val Loss=0.483]python(32362) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32364) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32365) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32368) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32369) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32370) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model:  97%|██████▊| 33/34 [08:49<00:14, 14.15s/batch, Val Loss=0.486]python(32372) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32374) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32377) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32378) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32379) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32380) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32382) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32383) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32388) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(32390) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Evaluating Original Model: 100%|███████| 34/34 [09:17<00:00, 16.40s/batch, Val Loss=0.483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: Val Loss: 0.4834, Val Cos Sim: 263.5462, Val Acc: 87.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4834236730547512, 263.5462100139718, 87.97003745318352)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_origin = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model_origin = model_origin.to(device)\n",
    "print(model_origin)\n",
    "\n",
    "def evaluate_original_model(model_origin, val_texts, tokenizer, device):\n",
    "    \"\"\"评估原始模型在验证集上的效果\"\"\"\n",
    "    val_dataset = TextPairDataset(val_texts)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "    model_origin.eval()\n",
    "    val_loss = 0\n",
    "    val_cos_sim = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        val_progress_bar = tqdm(val_dataloader, desc=\"Evaluating Original Model\", unit=\"batch\")\n",
    "        for sentences_a, sentences_b in val_progress_bar:\n",
    "            encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            sentence_embeddings_a = model_origin(**encoded_input_a).last_hidden_state\n",
    "            sentence_embeddings_b = model_origin(**encoded_input_b).last_hidden_state\n",
    "            cos_sim = torch.cosine_similarity(sentence_embeddings_a, sentence_embeddings_b, dim=-1)\n",
    "            val_cos_sim += cos_sim.sum().item()\n",
    "            val_acc += (cos_sim >= 0.7).sum().item()\n",
    "\n",
    "            loss = torch.mean(1 - cos_sim)\n",
    "            val_loss += loss.item()\n",
    "            val_progress_bar.set_postfix({\"Val Loss\": val_loss / (val_progress_bar.n + 1)})\n",
    "\n",
    "    val_cos_sim /= len(val_dataset)\n",
    "    val_acc /= len(val_dataset)\n",
    "    print(f\"Original Model: Val Loss: {val_loss/len(val_dataloader):.4f}, Val Cos Sim: {val_cos_sim:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return val_loss/len(val_dataloader), val_cos_sim, val_acc\n",
    "\n",
    "# 在新的 Jupyter Notebook 中调用评估函数\n",
    "evaluate_original_model(model_origin, val_texts, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a62c76-2027-4569-b825-d2eeea052725",
   "metadata": {},
   "source": [
    "## 2.4. 下游任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34660e97-c45c-41f1-8518-ba463ddf6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务1. 计算固定每个成分间的相似度，根据相似度给出固定长度的link（之后做Neo4J 固定布局的图可视化，可能是圆形的结构？）\n",
    "\n",
    "# 任务2. 使用模型讲用户输入的自然语言进行embedding，找到最匹配他的成分语言，给出对应相似成分的预测（）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a5f9d-3d39-4d91-b8e9-bacc8743ac1d",
   "metadata": {},
   "source": [
    "#### 000 测试 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7890fc91-ba0f-4d30-96cc-623e72147922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "base_model.model.embeddings.word_embeddings.weight torch.Size([28996, 768])\n",
      "base_model.model.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "base_model.model.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "base_model.model.embeddings.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.embeddings.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.0.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.0.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.0.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.0.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.1.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.1.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.1.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.1.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.2.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.2.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.2.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.2.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.3.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.3.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.3.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.3.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.4.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.4.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.4.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.4.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.5.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.5.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.5.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.5.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.6.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.6.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.6.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.6.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.7.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.7.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.7.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.7.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.8.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.8.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.8.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.8.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.9.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.9.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.9.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.9.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.10.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.10.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.10.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.10.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.self.query.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.11.attention.self.query.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.self.query.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.11.attention.self.query.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.self.value.base_layer.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.11.attention.self.value.base_layer.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.self.value.lora_A.default.weight torch.Size([1, 768])\n",
      "base_model.model.encoder.layer.11.attention.self.value.lora_B.default.weight torch.Size([768, 1])\n",
      "base_model.model.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "base_model.model.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "base_model.model.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "base_model.model.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "base_model.model.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "base_model.model.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "base_model.model.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "base_model.model.pooler.dense.weight torch.Size([768, 768])\n",
      "base_model.model.pooler.dense.bias torch.Size([768])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 打印一个样本输入和输出的形状\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sample_text1, sample_text2 \u001b[38;5;241m=\u001b[39m train_texts[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample text1 shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_text1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample text2 shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_text2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m outputs1 \u001b[38;5;241m=\u001b[39m model(sample_text1)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# 打印一个样本输入和输出的形状\n",
    "sample_text1, sample_text2 = train_texts[0]\n",
    "print(f\"Sample text1 shape: {sample_text1.shape}\")\n",
    "print(f\"Sample text2 shape: {sample_text2.shape}\")\n",
    "\n",
    "outputs1 = model(sample_text1)\n",
    "outputs2 = model(sample_text2)\n",
    "print(f\"Model output1 shape: {outputs1.shape}\")\n",
    "print(f\"Model output2 shape: {outputs2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a69801e2-0a33-4208-9593-55c3bc606094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Model output1 shape: torch.Size([2, 512, 768])\n",
      "Model output2 shape: torch.Size([2, 512, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8280, -0.9024,  0.1995,  ..., -0.3143, -0.1624,  0.9020],\n",
       "         [ 0.9236, -0.4079, -0.0779,  ...,  0.4532, -0.5143,  0.3333],\n",
       "         [ 0.5195, -0.7559,  0.2349,  ...,  0.6782, -0.2863,  0.1701],\n",
       "         ...,\n",
       "         [ 0.4462, -0.9198, -0.6070,  ...,  0.3245,  0.4255,  1.0480],\n",
       "         [ 0.8107, -1.1048, -0.1713,  ...,  0.8964,  0.2540,  0.7170],\n",
       "         [ 0.0654, -0.7277,  0.3448,  ...,  0.4545, -0.1554,  1.1606]],\n",
       "\n",
       "        [[-1.3981, -0.0845,  0.1867,  ..., -0.8284,  0.6307,  0.5061],\n",
       "         [-0.5423,  0.0880,  0.7182,  ..., -0.1797,  0.0468,  1.1006],\n",
       "         [-0.9348,  0.1268,  0.0450,  ...,  0.1030, -0.1996,  0.3013],\n",
       "         ...,\n",
       "         [-0.3146, -0.5538, -0.2006,  ..., -0.1527,  0.1480,  0.1084],\n",
       "         [-0.7362, -0.2626, -0.0669,  ..., -0.3275,  0.6866,  0.7149],\n",
       "         [-1.4319,  0.0186, -0.1068,  ..., -0.7160,  0.5469,  0.6057]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_a, sentences_b = train_texts[:2]\n",
    "print(len(sentences_a), len(sentences_b))\n",
    "encoded_input_a = tokenizer(sentences_a, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "encoded_input_b = tokenizer(sentences_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_a = model(**encoded_input_a)\n",
    "    sentence_embeddings_a = outputs_a.last_hidden_state\n",
    "    outputs_b = model(**encoded_input_b)\n",
    "    sentence_embeddings_b = outputs_b.last_hidden_state\n",
    "\n",
    "print(f\"Model output1 shape: {sentence_embeddings_a.shape}\")\n",
    "print(f\"Model output2 shape: {sentence_embeddings_b.shape}\")\n",
    "sentence_embeddings_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9253f-d3aa-48fd-9e8b-171512eb1726",
   "metadata": {},
   "source": [
    "# 3. 杂项\n",
    "## 3.1. gsarti/biobert-nli基本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd80cfb-8132-4240-938d-4b48474903f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "\n",
    "sentences_a = [\n",
    "    \"This is the first sentence OH MY GOD!\",\n",
    "    \"Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes.\",\n",
    "    \"This is the third sentence.\"\n",
    "]\n",
    "\n",
    "sentences_b = [\n",
    "    \"That is the first sentence!\",\n",
    "    \"Last year,studies have not shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes. Recent studies have shown that the dysregulation of the PI3K/AKT/mTOR signaling pathway plays a crucial role in the development and progression of various diseases, including cancer, neurodegenerative disorders, and cardiovascular diseases, making it an attractive target for therapeutic interventions. However, the complexity and interconnectedness of this pathway pose challenges for the design of effective targeted therapies. Therefore, further research is needed to unravel the intricate mechanisms underlying the dysregulation of this pathway and to develop novel therapeutic strategies that can selectively modulate its components to achieve optimal clinical outcomes.\",\n",
    "    \"This is the fifth sentence, OH MY GOD.\"\n",
    "]\n",
    "\n",
    "encoded_input_a = tokenizer(sentences_a, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "encoded_input_b = tokenizer(sentences_b, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_a = model(**encoded_input_a)\n",
    "    sentence_embeddings_a = outputs_a.last_hidden_state\n",
    "    outputs_b = model(**encoded_input_b)\n",
    "    sentence_embeddings_b = outputs_b.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b984b5f-b1b8-46ff-880b-e99933fab1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512, 768]), torch.Size([3, 512, 768]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_a.shape, sentence_embeddings_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a68bd19-6d41-4565-a0f0-a8e60c602de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.8171e-01, -4.0024e-01,  5.6180e-01,  ...,  7.3958e-01,\n",
       "           8.0139e-01,  1.1355e+00],\n",
       "         [-9.8806e-02, -2.4155e-01, -1.1370e-01,  ...,  1.2160e+00,\n",
       "           5.5963e-01,  1.3576e+00],\n",
       "         [ 3.4458e-01, -3.8337e-01, -9.2877e-02,  ...,  1.1780e+00,\n",
       "           5.2670e-01,  1.2173e+00],\n",
       "         ...,\n",
       "         [ 2.3720e-01, -6.5993e-01,  3.5448e-01,  ...,  1.0120e+00,\n",
       "           1.2228e+00,  1.4106e+00],\n",
       "         [ 2.2951e-01, -6.8032e-01,  2.9290e-01,  ...,  1.0224e+00,\n",
       "           1.3131e+00,  1.4847e+00],\n",
       "         [-1.8171e-01, -4.0024e-01,  5.6180e-01,  ...,  7.3958e-01,\n",
       "           8.0139e-01,  1.1355e+00]],\n",
       "\n",
       "        [[-4.7639e-01,  8.0986e-01, -7.7809e-01,  ..., -1.0216e+00,\n",
       "           4.5359e-01, -3.7967e-02],\n",
       "         [-5.1692e-01,  1.0944e+00, -2.5708e-01,  ..., -6.3311e-01,\n",
       "           3.4273e-01,  3.5301e-01],\n",
       "         [-7.6363e-01,  1.2261e+00, -1.9415e-01,  ..., -5.1874e-01,\n",
       "           6.3227e-02,  1.9215e-02],\n",
       "         ...,\n",
       "         [-3.5907e-02,  4.3222e-01, -9.5388e-01,  ..., -2.9576e-01,\n",
       "           5.1554e-01, -4.6712e-02],\n",
       "         [-1.3043e-01,  7.6062e-01, -7.6037e-01,  ..., -3.7209e-01,\n",
       "           2.7816e-02,  5.4056e-02],\n",
       "         [-4.7639e-01,  8.0986e-01, -7.7809e-01,  ..., -1.0216e+00,\n",
       "           4.5359e-01, -3.7966e-02]],\n",
       "\n",
       "        [[ 3.1909e-01, -5.9792e-04,  2.1877e-01,  ..., -5.3575e-01,\n",
       "           7.7707e-01,  6.9641e-01],\n",
       "         [ 2.4653e-01,  5.1219e-01,  4.0490e-02,  ..., -5.0139e-01,\n",
       "          -3.7997e-01,  7.2422e-01],\n",
       "         [ 5.7193e-01,  1.1103e-01,  7.8313e-02,  ..., -2.4169e-01,\n",
       "          -1.1511e-01,  7.9642e-01],\n",
       "         ...,\n",
       "         [ 3.9329e-01,  1.2635e-01,  2.9189e-02,  ..., -8.4377e-01,\n",
       "           3.5979e-01,  1.0487e+00],\n",
       "         [ 4.3678e-01,  2.2638e-01,  6.5403e-02,  ..., -8.5341e-01,\n",
       "           2.9760e-01,  9.5593e-01],\n",
       "         [ 3.1909e-01, -5.9795e-04,  2.1877e-01,  ..., -5.3575e-01,\n",
       "           7.7707e-01,  6.9641e-01]]]), pooler_output=tensor([[-0.1014, -0.1589,  0.9263,  ...,  0.9773,  0.0537,  0.9964],\n",
       "        [ 0.0593,  0.2094,  0.1944,  ..., -0.7561,  0.7152,  0.9118],\n",
       "        [ 0.3417,  0.4293,  0.8300,  ...,  0.8417,  0.2742,  0.9969]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1bad5e-0fc9-475a-8cf0-27b2d9c05e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6890)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义相似度指标函数\n",
    "def sim_metric(output1, output2):\n",
    "    return F.cosine_similarity(output1, output2, dim=-1).mean()\n",
    "\n",
    "sim_metric(sentence_embeddings_a, sentence_embeddings_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618563f-44e5-44df-8484-2d5cfc4e004a",
   "metadata": {},
   "source": [
    "### 大模型的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edeb884-977c-4a33-8bc1-d8ca7d2fcc91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.config: BertConfig {\n",
      "  \"_name_or_path\": \"gsarti/biobert-nli\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "model.base_model: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "model.encoder: BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "\n",
    "# Check model attributes\n",
    "# print(dir(model))\n",
    "\n",
    "# 打印更多模型属性\n",
    "print(f\"model.config: {model.config}\")\n",
    "print(f\"model.base_model: {model.base_model}\")\n",
    "print(f\"model.encoder: {model.encoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30ca2a-d582-49fe-b00d-91c07449a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict', 'input_shape', 'batch_size', 'seq_length', 'device', 'past_key_values_length', 'buffered_token_type_ids', 'buffered_token_type_ids_expanded', 'extended_attention_mask', 'encoder_batch_size', 'encoder_sequence_length', '_', 'encoder_hidden_shape', 'encoder_extended_attention_mask', 'embedding_output', 'encoder_outputs', 'sequence_output', 'pooled_output')\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "\n",
    "# 打印模型可接受的参数\n",
    "print(model.forward.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb11b4-a33e-46ab-a941-2695d222daa3",
   "metadata": {},
   "source": [
    "# 3.2. LoRA参数\n",
    "### 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55905ff9-a8e0-4ddc-9771-1f66dbac54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# 设置LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5bf00bb-eac0-40cf-9d67-d0227b3f2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskType.SEQ_CLS\n",
      "TaskType.SEQ_2_SEQ_LM\n",
      "TaskType.CAUSAL_LM\n",
      "TaskType.TOKEN_CLS\n",
      "TaskType.QUESTION_ANS\n",
      "TaskType.FEATURE_EXTRACTION\n"
     ]
    }
   ],
   "source": [
    "for taskType in TaskType: \n",
    "    print(taskType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e3532-c049-4bcc-a3e0-c7487ddda3aa",
   "metadata": {},
   "source": [
    "- TaskType.SEQ_CLS: 序列分类任务,即给定一个输入序列,预测整个序列的类别标签。例如情感分析、主题分类等。\n",
    "- TaskType.***SEQ_2_SEQ_LM***: 序列到序列的语言模型任务,即给定一个输入序列,生成一个输出序列。例如机器翻译、摘要生成等。\n",
    "- TaskType.CAUSAL_LM: 是因果语言模型任务,即给定前一个词,预测下一个词。这种模型通常用于生成任务,如文本续写、对话生成等。\n",
    "- TaskType.TOKEN_CLS: token分类任务,即给定一个输入序列,对序列中的每个token进行分类。例如命名实体识别、词性标注等。\n",
    "- TaskType.QUESTION_ANS: 问答任务,即给定一个问题和相关的背景文本,预测问题的答案。\n",
    "- TaskType.FEATURE_EXTRACTION: 这不是一个具体的任务类型,而是用于表示语言模型可以作为特征提取器使用的通用类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77f14c-a43c-45c1-a266-25b54e293ea5",
   "metadata": {},
   "source": [
    "### 对比大模型引入LoRA前后需要训练参数数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c605e50-2539-4167-af32-feb5d4a7e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at gsarti/biobert-nli and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始大模型的可训练参数数量: 108311810\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 检查是否可以使用 MPS 设备\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# 计算可训练参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'原始大模型的可训练参数数量: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fc3e826-cbcd-4f36-b49b-d803901afbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "引入LoRA后大模型的可训练参数数量为: 296450\n"
     ]
    }
   ],
   "source": [
    "# 将A、B矩阵插入大模型\n",
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "\n",
    "# 计算可训练参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'引入LoRA后大模型的可训练参数数量为: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3f4fb-7c94-4d37-8c22-58fc236849d7",
   "metadata": {},
   "source": [
    "## 3.3 Wandb测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c1cda-b9b0-4b6c-9d82-6e780a5f661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"autumndyer/BioBERT_LoRA/<run_id>\")\n",
    "run.config[\"key\"] = updated_value\n",
    "run.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b45f5d4-620e-4c25-96ed-a35e6dc6fb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mautumndyer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173bb7f13aa472c9db7ae1e8a1b5ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168125921782728, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/euan/JupyterDoc/wandb/run-20240422_224217-u1r4g374</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/autumndyer/BioBERT_LoRA_similarity/runs/u1r4g374' target=\"_blank\">dandy-pond-3</a></strong> to <a href='https://wandb.ai/autumndyer/BioBERT_LoRA_similarity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/autumndyer/BioBERT_LoRA_similarity' target=\"_blank\">https://wandb.ai/autumndyer/BioBERT_LoRA_similarity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/autumndyer/BioBERT_LoRA_similarity/runs/u1r4g374' target=\"_blank\">https://wandb.ai/autumndyer/BioBERT_LoRA_similarity/runs/u1r4g374</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/autumndyer/BioBERT_LoRA_similarity/runs/u1r4g374?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x117944f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"BioBERT_LoRA_similarity\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    # \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"BioBERT\",\n",
    "    # \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# # simulate training\n",
    "# epochs = 10\n",
    "# offset = random.random() / 5\n",
    "# for epoch in range(2, epochs):\n",
    "#     acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "#     loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f89b6-14cc-4890-96f8-9d1200a5b2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
